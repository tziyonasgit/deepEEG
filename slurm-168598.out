/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/home/chntzi001/deepEEG/EEGPT/downstream_tueg/utils.py:505: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
Namespace(batch_size=16, epochs=25, update_freq=100, save_ckpt_freq=0, robust_test=None, model='EEGPT', qkv_bias=False, rel_pos_bias=False, abs_pos_emb=True, layer_scale_init_value=0.1, input_size=200, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.0005, layer_decay=0.65, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, smoothing=0.1, reprob=0.25, remode='pixel', recount=1, resplit=False, finetune='/home/chntzi001/deepEEG/EEGPT/downstream_tueg/Checkpoints/eegpt_mcae_58chs_4s_large4E.ckpt', model_key='model|module|state_dict', model_prefix='', model_filter_name='gzp', init_scale=0.001, use_mean_pooling=True, disable_weight_decay_on_rel_pos_bias=False, nb_classes=0, output_dir='./checkpoints/finetune_quero_eegpt/fold_1', log_dir='./log/finetune_quero_eegpt/fold_1', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', enable_deepspeed=False, dataset='QUERO', fold=1, kfoldcrossval=True, distributed=False)
5069 1431
5069 1431
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fb19e44ba00>
Patch size = 200
Load ckpt from /home/chntzi001/deepEEG/EEGPT/downstream_tueg/Checkpoints/eegpt_mcae_58chs_4s_large4E.ckpt
Weights of EEGPTClassifier not initialized from pretrained model: ['chan_conv.0.weight', 'chan_conv.0.bias', 'chan_conv.1.weight', 'chan_conv.1.bias', 'chan_conv.1.running_mean', 'chan_conv.1.running_var', 'chan_conv.3.weight', 'chan_conv.3.bias', 'chan_conv.4.weight', 'chan_conv.4.bias', 'chan_conv.4.running_mean', 'chan_conv.4.running_var', 'reconstructor.cls_token', 'fc_norm.weight', 'fc_norm.bias', 'head.1.weight', 'head.1.bias']
Weights from pretrained model not used in EEGPTClassifier: ['encoder.summary_token', 'encoder.patch_embed.proj.weight', 'encoder.patch_embed.proj.bias', 'encoder.chan_embed.weight', 'encoder.blocks.0.norm1.weight', 'encoder.blocks.0.norm1.bias', 'encoder.blocks.0.attn.qkv.weight', 'encoder.blocks.0.attn.qkv.bias', 'encoder.blocks.0.attn.proj.weight', 'encoder.blocks.0.attn.proj.bias', 'encoder.blocks.0.norm2.weight', 'encoder.blocks.0.norm2.bias', 'encoder.blocks.0.mlp.fc1.weight', 'encoder.blocks.0.mlp.fc1.bias', 'encoder.blocks.0.mlp.fc2.weight', 'encoder.blocks.0.mlp.fc2.bias', 'encoder.blocks.1.norm1.weight', 'encoder.blocks.1.norm1.bias', 'encoder.blocks.1.attn.qkv.weight', 'encoder.blocks.1.attn.qkv.bias', 'encoder.blocks.1.attn.proj.weight', 'encoder.blocks.1.attn.proj.bias', 'encoder.blocks.1.norm2.weight', 'encoder.blocks.1.norm2.bias', 'encoder.blocks.1.mlp.fc1.weight', 'encoder.blocks.1.mlp.fc1.bias', 'encoder.blocks.1.mlp.fc2.weight', 'encoder.blocks.1.mlp.fc2.bias', 'encoder.blocks.2.norm1.weight', 'encoder.blocks.2.norm1.bias', 'encoder.blocks.2.attn.qkv.weight', 'encoder.blocks.2.attn.qkv.bias', 'encoder.blocks.2.attn.proj.weight', 'encoder.blocks.2.attn.proj.bias', 'encoder.blocks.2.norm2.weight', 'encoder.blocks.2.norm2.bias', 'encoder.blocks.2.mlp.fc1.weight', 'encoder.blocks.2.mlp.fc1.bias', 'encoder.blocks.2.mlp.fc2.weight', 'encoder.blocks.2.mlp.fc2.bias', 'encoder.blocks.3.norm1.weight', 'encoder.blocks.3.norm1.bias', 'encoder.blocks.3.attn.qkv.weight', 'encoder.blocks.3.attn.qkv.bias', 'encoder.blocks.3.attn.proj.weight', 'encoder.blocks.3.attn.proj.bias', 'encoder.blocks.3.norm2.weight', 'encoder.blocks.3.norm2.bias', 'encoder.blocks.3.mlp.fc1.weight', 'encoder.blocks.3.mlp.fc1.bias', 'encoder.blocks.3.mlp.fc2.weight', 'encoder.blocks.3.mlp.fc2.bias', 'encoder.blocks.4.norm1.weight', 'encoder.blocks.4.norm1.bias', 'encoder.blocks.4.attn.qkv.weight', 'encoder.blocks.4.attn.qkv.bias', 'encoder.blocks.4.attn.proj.weight', 'encoder.blocks.4.attn.proj.bias', 'encoder.blocks.4.norm2.weight', 'encoder.blocks.4.norm2.bias', 'encoder.blocks.4.mlp.fc1.weight', 'encoder.blocks.4.mlp.fc1.bias', 'encoder.blocks.4.mlp.fc2.weight', 'encoder.blocks.4.mlp.fc2.bias', 'encoder.blocks.5.norm1.weight', 'encoder.blocks.5.norm1.bias', 'encoder.blocks.5.attn.qkv.weight', 'encoder.blocks.5.attn.qkv.bias', 'encoder.blocks.5.attn.proj.weight', 'encoder.blocks.5.attn.proj.bias', 'encoder.blocks.5.norm2.weight', 'encoder.blocks.5.norm2.bias', 'encoder.blocks.5.mlp.fc1.weight', 'encoder.blocks.5.mlp.fc1.bias', 'encoder.blocks.5.mlp.fc2.weight', 'encoder.blocks.5.mlp.fc2.bias', 'encoder.blocks.6.norm1.weight', 'encoder.blocks.6.norm1.bias', 'encoder.blocks.6.attn.qkv.weight', 'encoder.blocks.6.attn.qkv.bias', 'encoder.blocks.6.attn.proj.weight', 'encoder.blocks.6.attn.proj.bias', 'encoder.blocks.6.norm2.weight', 'encoder.blocks.6.norm2.bias', 'encoder.blocks.6.mlp.fc1.weight', 'encoder.blocks.6.mlp.fc1.bias', 'encoder.blocks.6.mlp.fc2.weight', 'encoder.blocks.6.mlp.fc2.bias', 'encoder.blocks.7.norm1.weight', 'encoder.blocks.7.norm1.bias', 'encoder.blocks.7.attn.qkv.weight', 'encoder.blocks.7.attn.qkv.bias', 'encoder.blocks.7.attn.proj.weight', 'encoder.blocks.7.attn.proj.bias', 'encoder.blocks.7.norm2.weight', 'encoder.blocks.7.norm2.bias', 'encoder.blocks.7.mlp.fc1.weight', 'encoder.blocks.7.mlp.fc1.bias', 'encoder.blocks.7.mlp.fc2.weight', 'encoder.blocks.7.mlp.fc2.bias', 'encoder.norm.weight', 'encoder.norm.bias', 'predictor.mask_token', 'predictor.predictor_embed.weight', 'predictor.predictor_embed.bias', 'predictor.time_embed.freqs', 'predictor.predictor_blocks.0.norm1.weight', 'predictor.predictor_blocks.0.norm1.bias', 'predictor.predictor_blocks.0.attn.qkv.weight', 'predictor.predictor_blocks.0.attn.qkv.bias', 'predictor.predictor_blocks.0.attn.proj.weight', 'predictor.predictor_blocks.0.attn.proj.bias', 'predictor.predictor_blocks.0.norm2.weight', 'predictor.predictor_blocks.0.norm2.bias', 'predictor.predictor_blocks.0.mlp.fc1.weight', 'predictor.predictor_blocks.0.mlp.fc1.bias', 'predictor.predictor_blocks.0.mlp.fc2.weight', 'predictor.predictor_blocks.0.mlp.fc2.bias', 'predictor.predictor_blocks.1.norm1.weight', 'predictor.predictor_blocks.1.norm1.bias', 'predictor.predictor_blocks.1.attn.qkv.weight', 'predictor.predictor_blocks.1.attn.qkv.bias', 'predictor.predictor_blocks.1.attn.proj.weight', 'predictor.predictor_blocks.1.attn.proj.bias', 'predictor.predictor_blocks.1.norm2.weight', 'predictor.predictor_blocks.1.norm2.bias', 'predictor.predictor_blocks.1.mlp.fc1.weight', 'predictor.predictor_blocks.1.mlp.fc1.bias', 'predictor.predictor_blocks.1.mlp.fc2.weight', 'predictor.predictor_blocks.1.mlp.fc2.bias', 'predictor.predictor_blocks.2.norm1.weight', 'predictor.predictor_blocks.2.norm1.bias', 'predictor.predictor_blocks.2.attn.qkv.weight', 'predictor.predictor_blocks.2.attn.qkv.bias', 'predictor.predictor_blocks.2.attn.proj.weight', 'predictor.predictor_blocks.2.attn.proj.bias', 'predictor.predictor_blocks.2.norm2.weight', 'predictor.predictor_blocks.2.norm2.bias', 'predictor.predictor_blocks.2.mlp.fc1.weight', 'predictor.predictor_blocks.2.mlp.fc1.bias', 'predictor.predictor_blocks.2.mlp.fc2.weight', 'predictor.predictor_blocks.2.mlp.fc2.bias', 'predictor.predictor_blocks.3.norm1.weight', 'predictor.predictor_blocks.3.norm1.bias', 'predictor.predictor_blocks.3.attn.qkv.weight', 'predictor.predictor_blocks.3.attn.qkv.bias', 'predictor.predictor_blocks.3.attn.proj.weight', 'predictor.predictor_blocks.3.attn.proj.bias', 'predictor.predictor_blocks.3.norm2.weight', 'predictor.predictor_blocks.3.norm2.bias', 'predictor.predictor_blocks.3.mlp.fc1.weight', 'predictor.predictor_blocks.3.mlp.fc1.bias', 'predictor.predictor_blocks.3.mlp.fc2.weight', 'predictor.predictor_blocks.3.mlp.fc2.bias', 'predictor.predictor_blocks.4.norm1.weight', 'predictor.predictor_blocks.4.norm1.bias', 'predictor.predictor_blocks.4.attn.qkv.weight', 'predictor.predictor_blocks.4.attn.qkv.bias', 'predictor.predictor_blocks.4.attn.proj.weight', 'predictor.predictor_blocks.4.attn.proj.bias', 'predictor.predictor_blocks.4.norm2.weight', 'predictor.predictor_blocks.4.norm2.bias', 'predictor.predictor_blocks.4.mlp.fc1.weight', 'predictor.predictor_blocks.4.mlp.fc1.bias', 'predictor.predictor_blocks.4.mlp.fc2.weight', 'predictor.predictor_blocks.4.mlp.fc2.bias', 'predictor.predictor_blocks.5.norm1.weight', 'predictor.predictor_blocks.5.norm1.bias', 'predictor.predictor_blocks.5.attn.qkv.weight', 'predictor.predictor_blocks.5.attn.qkv.bias', 'predictor.predictor_blocks.5.attn.proj.weight', 'predictor.predictor_blocks.5.attn.proj.bias', 'predictor.predictor_blocks.5.norm2.weight', 'predictor.predictor_blocks.5.norm2.bias', 'predictor.predictor_blocks.5.mlp.fc1.weight', 'predictor.predictor_blocks.5.mlp.fc1.bias', 'predictor.predictor_blocks.5.mlp.fc2.weight', 'predictor.predictor_blocks.5.mlp.fc2.bias', 'predictor.predictor_blocks.6.norm1.weight', 'predictor.predictor_blocks.6.norm1.bias', 'predictor.predictor_blocks.6.attn.qkv.weight', 'predictor.predictor_blocks.6.attn.qkv.bias', 'predictor.predictor_blocks.6.attn.proj.weight', 'predictor.predictor_blocks.6.attn.proj.bias', 'predictor.predictor_blocks.6.norm2.weight', 'predictor.predictor_blocks.6.norm2.bias', 'predictor.predictor_blocks.6.mlp.fc1.weight', 'predictor.predictor_blocks.6.mlp.fc1.bias', 'predictor.predictor_blocks.6.mlp.fc2.weight', 'predictor.predictor_blocks.6.mlp.fc2.bias', 'predictor.predictor_blocks.7.norm1.weight', 'predictor.predictor_blocks.7.norm1.bias', 'predictor.predictor_blocks.7.attn.qkv.weight', 'predictor.predictor_blocks.7.attn.qkv.bias', 'predictor.predictor_blocks.7.attn.proj.weight', 'predictor.predictor_blocks.7.attn.proj.bias', 'predictor.predictor_blocks.7.norm2.weight', 'predictor.predictor_blocks.7.norm2.bias', 'predictor.predictor_blocks.7.mlp.fc1.weight', 'predictor.predictor_blocks.7.mlp.fc1.bias', 'predictor.predictor_blocks.7.mlp.fc2.weight', 'predictor.predictor_blocks.7.mlp.fc2.bias', 'predictor.predictor_norm.weight', 'predictor.predictor_norm.bias', 'predictor.predictor_proj.weight', 'predictor.predictor_proj.bias']
Model = EEGPTClassifier(
  (chan_conv): Sequential(
    (0): Conv2dWithConstraint(19, 20, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): GELU(approximate='none')
    (3): Conv2d(20, 20, kernel_size=(1, 15), stride=(1, 1), groups=20)
    (4): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): GELU(approximate='none')
    (6): Dropout(p=0.25, inplace=False)
  )
  (target_encoder): EEGTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(1, 512, kernel_size=(1, 64), stride=(1, 64))
    )
    (chan_embed): Embedding(62, 512)
    (blocks): ModuleList(
      (0-7): 8 x Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  )
  (reconstructor): EEGTransformerReconstructor(
    (reconstructor_embed): Linear(in_features=512, out_features=512, bias=True)
    (time_embed): RotaryEmbedding()
    (chan_embed): Embedding(62, 512)
    (reconstructor_blocks): ModuleList(
      (0-7): 8 x Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (reconstructor_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    (reconstructor_proj): Linear(in_features=512, out_features=64, bias=True)
  )
  (norm): Identity()
  (fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (head): Sequential(
    (0): Dropout(p=0.5, inplace=False)
    (1): LinearWithConstraint(in_features=63488, out_features=1, bias=True)
  )
)
number of params: 50900833
LR = 0.00050000
Batch size = 1600
Update frequent = 100
Number of training examples = 5069
Number of training training per epoch = 3
Assigned values = [0.0006599743590836596, 0.0010153451678210146, 0.0015620694889554071, 0.002403183829162165, 0.003697205891018715, 0.005688009063105715, 0.008750783174008792, 0.013462743344628911, 0.02071191283789063, 0.03186448128906251, 0.049022278906250015, 0.07541889062500001, 0.11602906250000002, 0.17850625000000003, 0.274625, 0.42250000000000004, 0.65, 1.0]
Skip weight decay name marked in model: {'reconstructor.pos_embed', 'target_encoder.summary_token', 'target_encoder.chan_embed', 'reconstructor.cls_token', 'reconstructor.chan_embed', 'reconstructor.time_embed'}
Param groups = {
  "layer_17_decay": {
    "weight_decay": 0.05,
    "params": [
      "chan_conv.0.weight",
      "chan_conv.3.weight",
      "target_encoder.patch_embed.proj.weight",
      "target_encoder.chan_embed.weight",
      "target_encoder.blocks.0.attn.qkv.weight",
      "target_encoder.blocks.0.attn.proj.weight",
      "target_encoder.blocks.0.mlp.fc1.weight",
      "target_encoder.blocks.0.mlp.fc2.weight",
      "target_encoder.blocks.1.attn.qkv.weight",
      "target_encoder.blocks.1.attn.proj.weight",
      "target_encoder.blocks.1.mlp.fc1.weight",
      "target_encoder.blocks.1.mlp.fc2.weight",
      "target_encoder.blocks.2.attn.qkv.weight",
      "target_encoder.blocks.2.attn.proj.weight",
      "target_encoder.blocks.2.mlp.fc1.weight",
      "target_encoder.blocks.2.mlp.fc2.weight",
      "target_encoder.blocks.3.attn.qkv.weight",
      "target_encoder.blocks.3.attn.proj.weight",
      "target_encoder.blocks.3.mlp.fc1.weight",
      "target_encoder.blocks.3.mlp.fc2.weight",
      "target_encoder.blocks.4.attn.qkv.weight",
      "target_encoder.blocks.4.attn.proj.weight",
      "target_encoder.blocks.4.mlp.fc1.weight",
      "target_encoder.blocks.4.mlp.fc2.weight",
      "target_encoder.blocks.5.attn.qkv.weight",
      "target_encoder.blocks.5.attn.proj.weight",
      "target_encoder.blocks.5.mlp.fc1.weight",
      "target_encoder.blocks.5.mlp.fc2.weight",
      "target_encoder.blocks.6.attn.qkv.weight",
      "target_encoder.blocks.6.attn.proj.weight",
      "target_encoder.blocks.6.mlp.fc1.weight",
      "target_encoder.blocks.6.mlp.fc2.weight",
      "target_encoder.blocks.7.attn.qkv.weight",
      "target_encoder.blocks.7.attn.proj.weight",
      "target_encoder.blocks.7.mlp.fc1.weight",
      "target_encoder.blocks.7.mlp.fc2.weight",
      "reconstructor.mask_token",
      "reconstructor.reconstructor_embed.weight",
      "reconstructor.chan_embed.weight",
      "reconstructor.reconstructor_blocks.0.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.0.attn.proj.weight",
      "reconstructor.reconstructor_blocks.0.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.0.mlp.fc2.weight",
      "reconstructor.reconstructor_blocks.1.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.1.attn.proj.weight",
      "reconstructor.reconstructor_blocks.1.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.1.mlp.fc2.weight",
      "reconstructor.reconstructor_blocks.2.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.2.attn.proj.weight",
      "reconstructor.reconstructor_blocks.2.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.2.mlp.fc2.weight",
      "reconstructor.reconstructor_blocks.3.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.3.attn.proj.weight",
      "reconstructor.reconstructor_blocks.3.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.3.mlp.fc2.weight",
      "reconstructor.reconstructor_blocks.4.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.4.attn.proj.weight",
      "reconstructor.reconstructor_blocks.4.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.4.mlp.fc2.weight",
      "reconstructor.reconstructor_blocks.5.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.5.attn.proj.weight",
      "reconstructor.reconstructor_blocks.5.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.5.mlp.fc2.weight",
      "reconstructor.reconstructor_blocks.6.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.6.attn.proj.weight",
      "reconstructor.reconstructor_blocks.6.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.6.mlp.fc2.weight",
      "reconstructor.reconstructor_blocks.7.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.7.attn.proj.weight",
      "reconstructor.reconstructor_blocks.7.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.7.mlp.fc2.weight",
      "reconstructor.reconstructor_proj.weight",
      "head.1.weight"
    ],
    "lr_scale": 1.0
  },
  "layer_17_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "chan_conv.0.bias",
      "chan_conv.1.weight",
      "chan_conv.1.bias",
      "chan_conv.3.bias",
      "chan_conv.4.weight",
      "chan_conv.4.bias",
      "target_encoder.summary_token",
      "target_encoder.patch_embed.proj.bias",
      "target_encoder.blocks.0.norm1.weight",
      "target_encoder.blocks.0.norm1.bias",
      "target_encoder.blocks.0.attn.qkv.bias",
      "target_encoder.blocks.0.attn.proj.bias",
      "target_encoder.blocks.0.norm2.weight",
      "target_encoder.blocks.0.norm2.bias",
      "target_encoder.blocks.0.mlp.fc1.bias",
      "target_encoder.blocks.0.mlp.fc2.bias",
      "target_encoder.blocks.1.norm1.weight",
      "target_encoder.blocks.1.norm1.bias",
      "target_encoder.blocks.1.attn.qkv.bias",
      "target_encoder.blocks.1.attn.proj.bias",
      "target_encoder.blocks.1.norm2.weight",
      "target_encoder.blocks.1.norm2.bias",
      "target_encoder.blocks.1.mlp.fc1.bias",
      "target_encoder.blocks.1.mlp.fc2.bias",
      "target_encoder.blocks.2.norm1.weight",
      "target_encoder.blocks.2.norm1.bias",
      "target_encoder.blocks.2.attn.qkv.bias",
      "target_encoder.blocks.2.attn.proj.bias",
      "target_encoder.blocks.2.norm2.weight",
      "target_encoder.blocks.2.norm2.bias",
      "target_encoder.blocks.2.mlp.fc1.bias",
      "target_encoder.blocks.2.mlp.fc2.bias",
      "target_encoder.blocks.3.norm1.weight",
      "target_encoder.blocks.3.norm1.bias",
      "target_encoder.blocks.3.attn.qkv.bias",
      "target_encoder.blocks.3.attn.proj.bias",
      "target_encoder.blocks.3.norm2.weight",
      "target_encoder.blocks.3.norm2.bias",
      "target_encoder.blocks.3.mlp.fc1.bias",
      "target_encoder.blocks.3.mlp.fc2.bias",
      "target_encoder.blocks.4.norm1.weight",
      "target_encoder.blocks.4.norm1.bias",
      "target_encoder.blocks.4.attn.qkv.bias",
      "target_encoder.blocks.4.attn.proj.bias",
      "target_encoder.blocks.4.norm2.weight",
      "target_encoder.blocks.4.norm2.bias",
      "target_encoder.blocks.4.mlp.fc1.bias",
      "target_encoder.blocks.4.mlp.fc2.bias",
      "target_encoder.blocks.5.norm1.weight",
      "target_encoder.blocks.5.norm1.bias",
      "target_encoder.blocks.5.attn.qkv.bias",
      "target_encoder.blocks.5.attn.proj.bias",
      "target_encoder.blocks.5.norm2.weight",
      "target_encoder.blocks.5.norm2.bias",
      "target_encoder.blocks.5.mlp.fc1.bias",
      "target_encoder.blocks.5.mlp.fc2.bias",
      "target_encoder.blocks.6.norm1.weight",
      "target_encoder.blocks.6.norm1.bias",
      "target_encoder.blocks.6.attn.qkv.bias",
      "target_encoder.blocks.6.attn.proj.bias",
      "target_encoder.blocks.6.norm2.weight",
      "target_encoder.blocks.6.norm2.bias",
      "target_encoder.blocks.6.mlp.fc1.bias",
      "target_encoder.blocks.6.mlp.fc2.bias",
      "target_encoder.blocks.7.norm1.weight",
      "target_encoder.blocks.7.norm1.bias",
      "target_encoder.blocks.7.attn.qkv.bias",
      "target_encoder.blocks.7.attn.proj.bias",
      "target_encoder.blocks.7.norm2.weight",
      "target_encoder.blocks.7.norm2.bias",
      "target_encoder.blocks.7.mlp.fc1.bias",
      "target_encoder.blocks.7.mlp.fc2.bias",
      "target_encoder.norm.weight",
      "target_encoder.norm.bias",
      "reconstructor.cls_token",
      "reconstructor.reconstructor_embed.bias",
      "reconstructor.reconstructor_blocks.0.norm1.weight",
      "reconstructor.reconstructor_blocks.0.norm1.bias",
      "reconstructor.reconstructor_blocks.0.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.0.attn.proj.bias",
      "reconstructor.reconstructor_blocks.0.norm2.weight",
      "reconstructor.reconstructor_blocks.0.norm2.bias",
      "reconstructor.reconstructor_blocks.0.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.0.mlp.fc2.bias",
      "reconstructor.reconstructor_blocks.1.norm1.weight",
      "reconstructor.reconstructor_blocks.1.norm1.bias",
      "reconstructor.reconstructor_blocks.1.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.1.attn.proj.bias",
      "reconstructor.reconstructor_blocks.1.norm2.weight",
      "reconstructor.reconstructor_blocks.1.norm2.bias",
      "reconstructor.reconstructor_blocks.1.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.1.mlp.fc2.bias",
      "reconstructor.reconstructor_blocks.2.norm1.weight",
      "reconstructor.reconstructor_blocks.2.norm1.bias",
      "reconstructor.reconstructor_blocks.2.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.2.attn.proj.bias",
      "reconstructor.reconstructor_blocks.2.norm2.weight",
      "reconstructor.reconstructor_blocks.2.norm2.bias",
      "reconstructor.reconstructor_blocks.2.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.2.mlp.fc2.bias",
      "reconstructor.reconstructor_blocks.3.norm1.weight",
      "reconstructor.reconstructor_blocks.3.norm1.bias",
      "reconstructor.reconstructor_blocks.3.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.3.attn.proj.bias",
      "reconstructor.reconstructor_blocks.3.norm2.weight",
      "reconstructor.reconstructor_blocks.3.norm2.bias",
      "reconstructor.reconstructor_blocks.3.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.3.mlp.fc2.bias",
      "reconstructor.reconstructor_blocks.4.norm1.weight",
      "reconstructor.reconstructor_blocks.4.norm1.bias",
      "reconstructor.reconstructor_blocks.4.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.4.attn.proj.bias",
      "reconstructor.reconstructor_blocks.4.norm2.weight",
      "reconstructor.reconstructor_blocks.4.norm2.bias",
      "reconstructor.reconstructor_blocks.4.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.4.mlp.fc2.bias",
      "reconstructor.reconstructor_blocks.5.norm1.weight",
      "reconstructor.reconstructor_blocks.5.norm1.bias",
      "reconstructor.reconstructor_blocks.5.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.5.attn.proj.bias",
      "reconstructor.reconstructor_blocks.5.norm2.weight",
      "reconstructor.reconstructor_blocks.5.norm2.bias",
      "reconstructor.reconstructor_blocks.5.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.5.mlp.fc2.bias",
      "reconstructor.reconstructor_blocks.6.norm1.weight",
      "reconstructor.reconstructor_blocks.6.norm1.bias",
      "reconstructor.reconstructor_blocks.6.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.6.attn.proj.bias",
      "reconstructor.reconstructor_blocks.6.norm2.weight",
      "reconstructor.reconstructor_blocks.6.norm2.bias",
      "reconstructor.reconstructor_blocks.6.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.6.mlp.fc2.bias",
      "reconstructor.reconstructor_blocks.7.norm1.weight",
      "reconstructor.reconstructor_blocks.7.norm1.bias",
      "reconstructor.reconstructor_blocks.7.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.7.attn.proj.bias",
      "reconstructor.reconstructor_blocks.7.norm2.weight",
      "reconstructor.reconstructor_blocks.7.norm2.bias",
      "reconstructor.reconstructor_blocks.7.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.7.mlp.fc2.bias",
      "reconstructor.reconstructor_norm.weight",
      "reconstructor.reconstructor_norm.bias",
      "reconstructor.reconstructor_proj.bias",
      "fc_norm.weight",
      "fc_norm.bias",
      "head.1.bias"
    ],
    "lr_scale": 1.0
  }
}
Optimizer config: {'lr': 0.0005, 'weight_decay': 0.0, 'eps': 1e-08}
Use step level LR scheduler!
Set warmup steps = 15
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = BCEWithLogitsLoss()
Auto resume checkpoint: 
Start training for 25 epochs
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [0]  [  0/316]  eta: 0:04:02  lr: 0.000000  min_lr: 0.000000  loss: 0.9645 (0.9645)  class_acc: 0.3750 (0.3750)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7682  data: 0.1934  max mem: 2109
Epoch: [0]  [ 10/316]  eta: 0:00:30  lr: 0.000000  min_lr: 0.000000  loss: 0.9406 (0.8994)  class_acc: 0.3750 (0.4034)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0986  data: 0.0178  max mem: 2210
Epoch: [0]  [ 20/316]  eta: 0:00:19  lr: 0.000000  min_lr: 0.000000  loss: 0.9009 (0.9077)  class_acc: 0.4375 (0.4286)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0309  data: 0.0002  max mem: 2210
Epoch: [0]  [ 30/316]  eta: 0:00:15  lr: 0.000000  min_lr: 0.000000  loss: 0.9044 (0.9174)  class_acc: 0.4375 (0.4274)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0305  data: 0.0001  max mem: 2210
Epoch: [0]  [ 40/316]  eta: 0:00:13  lr: 0.000000  min_lr: 0.000000  loss: 0.9044 (0.9057)  class_acc: 0.4375 (0.4345)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0307  data: 0.0002  max mem: 2210
Epoch: [0]  [ 50/316]  eta: 0:00:12  lr: 0.000000  min_lr: 0.000000  loss: 0.9097 (0.9083)  class_acc: 0.3750 (0.4240)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0307  data: 0.0002  max mem: 2210
Epoch: [0]  [ 60/316]  eta: 0:00:10  lr: 0.000000  min_lr: 0.000000  loss: 0.8542 (0.9051)  class_acc: 0.4375 (0.4334)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0305  data: 0.0001  max mem: 2210
Epoch: [0]  [ 70/316]  eta: 0:00:10  lr: 0.000000  min_lr: 0.000000  loss: 0.8509 (0.8939)  class_acc: 0.4375 (0.4401)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0307  data: 0.0001  max mem: 2210
Epoch: [0]  [ 80/316]  eta: 0:00:09  lr: 0.000000  min_lr: 0.000000  loss: 0.8571 (0.8896)  class_acc: 0.4375 (0.4414)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0307  data: 0.0001  max mem: 2210
Epoch: [0]  [ 90/316]  eta: 0:00:08  lr: 0.000000  min_lr: 0.000000  loss: 0.8212 (0.8828)  class_acc: 0.4375 (0.4444)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0302  data: 0.0001  max mem: 2210
Epoch: [0]  [100/316]  eta: 0:00:08  lr: 0.000036  min_lr: 0.000036  loss: 0.8522 (0.8885)  class_acc: 0.3750 (0.4344)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 71.7395 (71.7395)  time: 0.0319  data: 0.0001  max mem: 2301
Epoch: [0]  [110/316]  eta: 0:00:07  lr: 0.000036  min_lr: 0.000036  loss: 0.8955 (0.8922)  class_acc: 0.3750 (0.4313)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 71.7395 (71.7395)  time: 0.0320  data: 0.0001  max mem: 2401
Epoch: [0]  [120/316]  eta: 0:00:07  lr: 0.000036  min_lr: 0.000036  loss: 0.9738 (0.8982)  class_acc: 0.4375 (0.4318)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 71.7395 (71.7395)  time: 0.0307  data: 0.0001  max mem: 2401
Epoch: [0]  [130/316]  eta: 0:00:06  lr: 0.000036  min_lr: 0.000036  loss: 0.9738 (0.9000)  class_acc: 0.4375 (0.4289)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 71.7395 (71.7395)  time: 0.0306  data: 0.0001  max mem: 2401
Epoch: [0]  [140/316]  eta: 0:00:06  lr: 0.000036  min_lr: 0.000036  loss: 0.9053 (0.8988)  class_acc: 0.3750 (0.4273)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 71.7395 (71.7395)  time: 0.0304  data: 0.0001  max mem: 2401
Epoch: [0]  [150/316]  eta: 0:00:05  lr: 0.000036  min_lr: 0.000036  loss: 0.9278 (0.9024)  class_acc: 0.3750 (0.4251)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 71.7395 (71.7395)  time: 0.0305  data: 0.0001  max mem: 2401
Epoch: [0]  [160/316]  eta: 0:00:05  lr: 0.000036  min_lr: 0.000036  loss: 0.8749 (0.8995)  class_acc: 0.4375 (0.4266)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 71.7395 (71.7395)  time: 0.0309  data: 0.0001  max mem: 2401
Epoch: [0]  [170/316]  eta: 0:00:05  lr: 0.000036  min_lr: 0.000036  loss: 0.8684 (0.8989)  class_acc: 0.4375 (0.4280)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 71.7395 (71.7395)  time: 0.0310  data: 0.0001  max mem: 2401
Epoch: [0]  [180/316]  eta: 0:00:04  lr: 0.000036  min_lr: 0.000036  loss: 0.8660 (0.8948)  class_acc: 0.4375 (0.4306)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 71.7395 (71.7395)  time: 0.0305  data: 0.0001  max mem: 2401
Epoch: [0]  [190/316]  eta: 0:00:04  lr: 0.000036  min_lr: 0.000036  loss: 0.8710 (0.8956)  class_acc: 0.4375 (0.4296)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 71.7395 (71.7395)  time: 0.0304  data: 0.0001  max mem: 2401
Epoch: [0]  [200/316]  eta: 0:00:03  lr: 0.000071  min_lr: 0.000071  loss: 0.9076 (0.8984)  class_acc: 0.4375 (0.4291)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 70.7378 (71.2387)  time: 0.0307  data: 0.0001  max mem: 2401
Epoch: [0]  [210/316]  eta: 0:00:03  lr: 0.000071  min_lr: 0.000071  loss: 0.9318 (0.9006)  class_acc: 0.5000 (0.4378)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 70.7378 (71.2387)  time: 0.0307  data: 0.0001  max mem: 2401
Epoch: [0]  [220/316]  eta: 0:00:03  lr: 0.000071  min_lr: 0.000071  loss: 0.9417 (0.9057)  class_acc: 0.6250 (0.4454)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 70.7378 (71.2387)  time: 0.0306  data: 0.0001  max mem: 2401
Epoch: [0]  [230/316]  eta: 0:00:02  lr: 0.000071  min_lr: 0.000071  loss: 0.9417 (0.9130)  class_acc: 0.6250 (0.4508)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 70.7378 (71.2387)  time: 0.0305  data: 0.0001  max mem: 2401
Epoch: [0]  [240/316]  eta: 0:00:02  lr: 0.000071  min_lr: 0.000071  loss: 0.9359 (0.9164)  class_acc: 0.6250 (0.4567)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 70.7378 (71.2387)  time: 0.0304  data: 0.0001  max mem: 2401
Epoch: [0]  [250/316]  eta: 0:00:02  lr: 0.000071  min_lr: 0.000071  loss: 0.8570 (0.9199)  class_acc: 0.6250 (0.4624)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 70.7378 (71.2387)  time: 0.0305  data: 0.0001  max mem: 2401
Epoch: [0]  [260/316]  eta: 0:00:01  lr: 0.000071  min_lr: 0.000071  loss: 0.8620 (0.9176)  class_acc: 0.6250 (0.4693)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 70.7378 (71.2387)  time: 0.0301  data: 0.0001  max mem: 2401
Epoch: [0]  [270/316]  eta: 0:00:01  lr: 0.000071  min_lr: 0.000071  loss: 0.9090 (0.9194)  class_acc: 0.6250 (0.4739)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 70.7378 (71.2387)  time: 0.0299  data: 0.0001  max mem: 2401
Epoch: [0]  [280/316]  eta: 0:00:01  lr: 0.000071  min_lr: 0.000071  loss: 0.9927 (0.9217)  class_acc: 0.5625 (0.4778)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 70.7378 (71.2387)  time: 0.0302  data: 0.0001  max mem: 2401
Epoch: [0]  [290/316]  eta: 0:00:00  lr: 0.000071  min_lr: 0.000071  loss: 0.9927 (0.9249)  class_acc: 0.5625 (0.4805)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 70.7378 (71.2387)  time: 0.0302  data: 0.0002  max mem: 2401
Epoch: [0]  [300/316]  eta: 0:00:00  lr: 0.000071  min_lr: 0.000071  loss: 1.0340 (0.9293)  class_acc: 0.5625 (0.4825)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 71.7395 (72.8055)  time: 0.0288  data: 0.0002  max mem: 2401
Epoch: [0]  [310/316]  eta: 0:00:00  lr: 0.000071  min_lr: 0.000071  loss: 1.0340 (0.9293)  class_acc: 0.5625 (0.4825)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 71.7395 (72.8055)  time: 0.0137  data: 0.0001  max mem: 2401
Epoch: [0]  [315/316]  eta: 0:00:00  lr: 0.000071  min_lr: 0.000071  loss: 1.0340 (0.9293)  class_acc: 0.5625 (0.4825)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 71.7395 (72.8055)  time: 0.0062  data: 0.0001  max mem: 2401
Epoch: [0] Total time: 0:00:10 (0.0318 s / it)
Averaged stats: lr: 0.000071  min_lr: 0.000071  loss: 1.0340 (0.9293)  class_acc: 0.5625 (0.4825)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 71.7395 (72.8055)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:19  loss: 0.0858 (0.0858)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.3327  data: 0.2279  max mem: 2401
Val:  [10/60]  eta: 0:00:02  loss: 0.9662 (0.7662)  accuracy: 0.5417 (0.3826)  balanced_accuracy: 0.5000 (0.3182)  pr_auc: 0.4864 (0.4265)  roc_auc: 0.3601 (0.3528)  time: 0.0430  data: 0.0208  max mem: 2401
Val:  [20/60]  eta: 0:00:01  loss: 0.8243 (0.8127)  accuracy: 0.6250 (0.5397)  balanced_accuracy: 0.5000 (0.4048)  pr_auc: 0.6441 (0.5606)  roc_auc: 0.3929 (0.3825)  time: 0.0142  data: 0.0001  max mem: 2401
Val:  [30/60]  eta: 0:00:00  loss: 0.9809 (0.8873)  accuracy: 0.6667 (0.5699)  balanced_accuracy: 0.5000 (0.4355)  pr_auc: 0.6988 (0.6035)  roc_auc: 0.4491 (0.4317)  time: 0.0144  data: 0.0001  max mem: 2401
Val:  [40/60]  eta: 0:00:00  loss: 1.0750 (0.9580)  accuracy: 0.6250 (0.5742)  balanced_accuracy: 0.5000 (0.4512)  pr_auc: 0.6985 (0.6229)  roc_auc: 0.5469 (0.4609)  time: 0.0144  data: 0.0001  max mem: 2401
Val:  [50/60]  eta: 0:00:00  loss: 1.2360 (1.0228)  accuracy: 0.5417 (0.5637)  balanced_accuracy: 0.5000 (0.4608)  pr_auc: 0.6978 (0.6375)  roc_auc: 0.6007 (0.5038)  time: 0.0142  data: 0.0001  max mem: 2401
Val:  [59/60]  eta: 0:00:00  loss: 1.3991 (1.1983)  accuracy: 0.5000 (0.5171)  balanced_accuracy: 0.5000 (0.4444)  pr_auc: 0.5325 (0.5746)  roc_auc: 0.5444 (0.4479)  time: 0.0154  data: 0.0001  max mem: 2401
Val: Total time: 0:00:01 (0.0221 s / it)
* loss 1.198
Accuracy of the network on the 1431 test EEG: 0.58%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [1]  [  0/316]  eta: 0:01:20  lr: 0.000107  min_lr: 0.000107  loss: 0.9865 (0.9865)  class_acc: 0.6875 (0.6875)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2539  data: 0.1907  max mem: 2401
Epoch: [1]  [ 10/316]  eta: 0:00:15  lr: 0.000107  min_lr: 0.000107  loss: 1.1699 (1.1754)  class_acc: 0.6250 (0.6477)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0508  data: 0.0175  max mem: 2401
Epoch: [1]  [ 20/316]  eta: 0:00:12  lr: 0.000107  min_lr: 0.000107  loss: 1.0841 (1.1284)  class_acc: 0.6875 (0.6637)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0299  data: 0.0001  max mem: 2401
Epoch: [1]  [ 30/316]  eta: 0:00:10  lr: 0.000107  min_lr: 0.000107  loss: 1.0634 (1.1348)  class_acc: 0.6875 (0.6593)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0294  data: 0.0001  max mem: 2401
Epoch: [1]  [ 40/316]  eta: 0:00:09  lr: 0.000107  min_lr: 0.000107  loss: 1.0842 (1.2092)  class_acc: 0.6250 (0.6372)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0294  data: 0.0001  max mem: 2401
Epoch: [1]  [ 50/316]  eta: 0:00:09  lr: 0.000107  min_lr: 0.000107  loss: 1.1719 (1.1922)  class_acc: 0.6250 (0.6409)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0298  data: 0.0001  max mem: 2401
Epoch: [1]  [ 60/316]  eta: 0:00:08  lr: 0.000107  min_lr: 0.000107  loss: 1.1619 (1.2131)  class_acc: 0.6250 (0.6342)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0299  data: 0.0001  max mem: 2401
Epoch: [1]  [ 70/316]  eta: 0:00:08  lr: 0.000107  min_lr: 0.000107  loss: 1.3890 (1.2469)  class_acc: 0.6250 (0.6259)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0304  data: 0.0001  max mem: 2401
Epoch: [1]  [ 80/316]  eta: 0:00:07  lr: 0.000107  min_lr: 0.000107  loss: 1.3137 (1.2512)  class_acc: 0.6250 (0.6227)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0309  data: 0.0001  max mem: 2401
Epoch: [1]  [ 90/316]  eta: 0:00:07  lr: 0.000107  min_lr: 0.000107  loss: 1.3041 (1.2689)  class_acc: 0.5625 (0.6154)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0305  data: 0.0001  max mem: 2401
Epoch: [1]  [100/316]  eta: 0:00:06  lr: 0.000143  min_lr: 0.000143  loss: 1.1494 (1.2442)  class_acc: 0.6250 (0.6219)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 95.7409 (95.7409)  time: 0.0304  data: 0.0001  max mem: 2401
Epoch: [1]  [110/316]  eta: 0:00:06  lr: 0.000143  min_lr: 0.000143  loss: 0.9025 (1.2035)  class_acc: 0.6250 (0.6199)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 95.7409 (95.7409)  time: 0.0300  data: 0.0002  max mem: 2403
Epoch: [1]  [120/316]  eta: 0:00:06  lr: 0.000143  min_lr: 0.000143  loss: 0.7782 (1.1681)  class_acc: 0.6250 (0.6198)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 95.7409 (95.7409)  time: 0.0297  data: 0.0001  max mem: 2403
Epoch: [1]  [130/316]  eta: 0:00:05  lr: 0.000143  min_lr: 0.000143  loss: 0.8556 (1.1527)  class_acc: 0.5625 (0.6145)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 95.7409 (95.7409)  time: 0.0304  data: 0.0001  max mem: 2403
Epoch: [1]  [140/316]  eta: 0:00:05  lr: 0.000143  min_lr: 0.000143  loss: 0.8388 (1.1304)  class_acc: 0.5625 (0.6139)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 95.7409 (95.7409)  time: 0.0308  data: 0.0001  max mem: 2403
Epoch: [1]  [150/316]  eta: 0:00:05  lr: 0.000143  min_lr: 0.000143  loss: 0.7769 (1.1043)  class_acc: 0.6250 (0.6180)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 95.7409 (95.7409)  time: 0.0307  data: 0.0001  max mem: 2403
Epoch: [1]  [160/316]  eta: 0:00:04  lr: 0.000143  min_lr: 0.000143  loss: 0.7458 (1.0843)  class_acc: 0.6250 (0.6200)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 95.7409 (95.7409)  time: 0.0311  data: 0.0001  max mem: 2403
Epoch: [1]  [170/316]  eta: 0:00:04  lr: 0.000143  min_lr: 0.000143  loss: 0.7458 (1.0660)  class_acc: 0.6250 (0.6224)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 95.7409 (95.7409)  time: 0.0313  data: 0.0001  max mem: 2403
Epoch: [1]  [180/316]  eta: 0:00:04  lr: 0.000143  min_lr: 0.000143  loss: 0.8034 (1.0582)  class_acc: 0.6250 (0.6191)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 95.7409 (95.7409)  time: 0.0309  data: 0.0001  max mem: 2403
Epoch: [1]  [190/316]  eta: 0:00:03  lr: 0.000143  min_lr: 0.000143  loss: 0.9003 (1.0488)  class_acc: 0.5625 (0.6175)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 95.7409 (95.7409)  time: 0.0304  data: 0.0001  max mem: 2403
Epoch: [1]  [200/316]  eta: 0:00:03  lr: 0.000179  min_lr: 0.000179  loss: 0.8793 (1.0452)  class_acc: 0.5625 (0.6129)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 53.4104 (74.5757)  time: 0.0302  data: 0.0001  max mem: 2403
Epoch: [1]  [210/316]  eta: 0:00:03  lr: 0.000179  min_lr: 0.000179  loss: 0.9998 (1.0543)  class_acc: 0.4375 (0.6025)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 53.4104 (74.5757)  time: 0.0312  data: 0.0002  max mem: 2403
Epoch: [1]  [220/316]  eta: 0:00:03  lr: 0.000179  min_lr: 0.000179  loss: 1.2095 (1.0656)  class_acc: 0.3750 (0.5922)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 53.4104 (74.5757)  time: 0.0327  data: 0.0002  max mem: 2403
Epoch: [1]  [230/316]  eta: 0:00:02  lr: 0.000179  min_lr: 0.000179  loss: 1.2704 (1.0749)  class_acc: 0.3750 (0.5847)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 53.4104 (74.5757)  time: 0.0333  data: 0.0002  max mem: 2403
Epoch: [1]  [240/316]  eta: 0:00:02  lr: 0.000179  min_lr: 0.000179  loss: 1.1824 (1.0794)  class_acc: 0.4375 (0.5778)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 53.4104 (74.5757)  time: 0.0329  data: 0.0002  max mem: 2403
Epoch: [1]  [250/316]  eta: 0:00:02  lr: 0.000179  min_lr: 0.000179  loss: 1.2373 (1.0846)  class_acc: 0.3750 (0.5695)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 53.4104 (74.5757)  time: 0.0318  data: 0.0002  max mem: 2403
Epoch: [1]  [260/316]  eta: 0:00:01  lr: 0.000179  min_lr: 0.000179  loss: 1.2462 (1.0923)  class_acc: 0.3125 (0.5613)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 53.4104 (74.5757)  time: 0.0321  data: 0.0002  max mem: 2403
Epoch: [1]  [270/316]  eta: 0:00:01  lr: 0.000179  min_lr: 0.000179  loss: 1.2462 (1.0998)  class_acc: 0.3750 (0.5551)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 53.4104 (74.5757)  time: 0.0323  data: 0.0002  max mem: 2403
Epoch: [1]  [280/316]  eta: 0:00:01  lr: 0.000179  min_lr: 0.000179  loss: 1.1928 (1.1021)  class_acc: 0.3750 (0.5505)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 53.4104 (74.5757)  time: 0.0310  data: 0.0001  max mem: 2403
Epoch: [1]  [290/316]  eta: 0:00:00  lr: 0.000179  min_lr: 0.000179  loss: 1.2018 (1.1055)  class_acc: 0.4375 (0.5462)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 53.4104 (74.5757)  time: 0.0310  data: 0.0002  max mem: 2403
Epoch: [1]  [300/316]  eta: 0:00:00  lr: 0.000179  min_lr: 0.000179  loss: 1.1149 (1.1026)  class_acc: 0.4375 (0.5437)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 95.7409 (89.8396)  time: 0.0298  data: 0.0002  max mem: 2403
Epoch: [1]  [310/316]  eta: 0:00:00  lr: 0.000179  min_lr: 0.000179  loss: 1.1149 (1.1026)  class_acc: 0.4375 (0.5437)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 95.7409 (89.8396)  time: 0.0140  data: 0.0001  max mem: 2403
Epoch: [1]  [315/316]  eta: 0:00:00  lr: 0.000179  min_lr: 0.000179  loss: 1.1149 (1.1026)  class_acc: 0.4375 (0.5437)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 95.7409 (89.8396)  time: 0.0062  data: 0.0001  max mem: 2403
Epoch: [1] Total time: 0:00:09 (0.0305 s / it)
Averaged stats: lr: 0.000179  min_lr: 0.000179  loss: 1.1149 (1.1026)  class_acc: 0.4375 (0.5437)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 95.7409 (89.8396)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:16  loss: 1.1530 (1.1530)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2826  data: 0.2248  max mem: 2403
Val:  [10/60]  eta: 0:00:01  loss: 0.9201 (0.9143)  accuracy: 0.3750 (0.2614)  balanced_accuracy: 0.4778 (0.3181)  pr_auc: 0.5193 (0.4006)  roc_auc: 0.4071 (0.3256)  time: 0.0389  data: 0.0206  max mem: 2403
Val:  [20/60]  eta: 0:00:01  loss: 0.8804 (0.8855)  accuracy: 0.3333 (0.3016)  balanced_accuracy: 0.5111 (0.4215)  pr_auc: 0.6692 (0.5969)  roc_auc: 0.5259 (0.4596)  time: 0.0147  data: 0.0002  max mem: 2403
Val:  [30/60]  eta: 0:00:00  loss: 0.8283 (0.8683)  accuracy: 0.3750 (0.3306)  balanced_accuracy: 0.5312 (0.4524)  pr_auc: 0.7935 (0.6447)  roc_auc: 0.6000 (0.4960)  time: 0.0150  data: 0.0002  max mem: 2403
Val:  [40/60]  eta: 0:00:00  loss: 0.8367 (0.8674)  accuracy: 0.3750 (0.3415)  balanced_accuracy: 0.5000 (0.4510)  pr_auc: 0.6814 (0.6250)  roc_auc: 0.4922 (0.4723)  time: 0.0151  data: 0.0002  max mem: 2403
Val:  [50/60]  eta: 0:00:00  loss: 0.8459 (0.8666)  accuracy: 0.4167 (0.3587)  balanced_accuracy: 0.4357 (0.4505)  pr_auc: 0.4897 (0.5898)  roc_auc: 0.3706 (0.4376)  time: 0.0144  data: 0.0001  max mem: 2403
Val:  [59/60]  eta: 0:00:00  loss: 0.8139 (0.8278)  accuracy: 0.4167 (0.3683)  balanced_accuracy: 0.4231 (0.4331)  pr_auc: 0.4241 (0.5393)  roc_auc: 0.3287 (0.4155)  time: 0.0135  data: 0.0001  max mem: 2403
Val: Total time: 0:00:01 (0.0215 s / it)
* loss 0.828
Accuracy of the network on the 1431 test EEG: 0.42%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [2]  [  0/316]  eta: 0:01:24  lr: 0.000214  min_lr: 0.000214  loss: 0.8053 (0.8053)  class_acc: 0.5000 (0.5000)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2684  data: 0.2222  max mem: 2403
Epoch: [2]  [ 10/316]  eta: 0:00:16  lr: 0.000214  min_lr: 0.000214  loss: 0.9650 (0.9827)  class_acc: 0.3750 (0.4148)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0529  data: 0.0203  max mem: 2403
Epoch: [2]  [ 20/316]  eta: 0:00:12  lr: 0.000214  min_lr: 0.000214  loss: 0.9650 (0.9751)  class_acc: 0.3750 (0.4196)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0312  data: 0.0001  max mem: 2403
Epoch: [2]  [ 30/316]  eta: 0:00:11  lr: 0.000214  min_lr: 0.000214  loss: 0.9766 (0.9877)  class_acc: 0.4375 (0.4153)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0309  data: 0.0001  max mem: 2403
Epoch: [2]  [ 40/316]  eta: 0:00:10  lr: 0.000214  min_lr: 0.000214  loss: 0.9782 (0.9581)  class_acc: 0.3750 (0.4268)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0311  data: 0.0001  max mem: 2403
Epoch: [2]  [ 50/316]  eta: 0:00:09  lr: 0.000214  min_lr: 0.000214  loss: 0.9187 (0.9516)  class_acc: 0.3750 (0.4265)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0313  data: 0.0001  max mem: 2403
Epoch: [2]  [ 60/316]  eta: 0:00:09  lr: 0.000214  min_lr: 0.000214  loss: 0.9187 (0.9432)  class_acc: 0.3750 (0.4273)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0317  data: 0.0001  max mem: 2403
Epoch: [2]  [ 70/316]  eta: 0:00:08  lr: 0.000214  min_lr: 0.000214  loss: 0.8651 (0.9319)  class_acc: 0.3750 (0.4331)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0321  data: 0.0001  max mem: 2403
Epoch: [2]  [ 80/316]  eta: 0:00:08  lr: 0.000214  min_lr: 0.000214  loss: 0.8601 (0.9291)  class_acc: 0.4375 (0.4352)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0323  data: 0.0001  max mem: 2403
Epoch: [2]  [ 90/316]  eta: 0:00:07  lr: 0.000214  min_lr: 0.000214  loss: 0.8785 (0.9195)  class_acc: 0.5000 (0.4437)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0325  data: 0.0001  max mem: 2403
Epoch: [2]  [100/316]  eta: 0:00:07  lr: 0.000250  min_lr: 0.000250  loss: 0.8905 (0.9236)  class_acc: 0.4375 (0.4431)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 86.1238 (86.1238)  time: 0.0329  data: 0.0002  max mem: 2403
Epoch: [2]  [110/316]  eta: 0:00:07  lr: 0.000250  min_lr: 0.000250  loss: 0.9896 (0.9320)  class_acc: 0.5000 (0.4578)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 86.1238 (86.1238)  time: 0.0328  data: 0.0002  max mem: 2403
Epoch: [2]  [120/316]  eta: 0:00:06  lr: 0.000250  min_lr: 0.000250  loss: 0.9514 (0.9342)  class_acc: 0.6875 (0.4747)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 86.1238 (86.1238)  time: 0.0323  data: 0.0001  max mem: 2403
Epoch: [2]  [130/316]  eta: 0:00:06  lr: 0.000250  min_lr: 0.000250  loss: 1.0406 (0.9453)  class_acc: 0.6250 (0.4819)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 86.1238 (86.1238)  time: 0.0325  data: 0.0001  max mem: 2403
Epoch: [2]  [140/316]  eta: 0:00:05  lr: 0.000250  min_lr: 0.000250  loss: 1.0307 (0.9479)  class_acc: 0.5625 (0.4907)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 86.1238 (86.1238)  time: 0.0325  data: 0.0001  max mem: 2403
Epoch: [2]  [150/316]  eta: 0:00:05  lr: 0.000250  min_lr: 0.000250  loss: 0.8969 (0.9409)  class_acc: 0.6250 (0.5037)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 86.1238 (86.1238)  time: 0.0325  data: 0.0001  max mem: 2403
Epoch: [2]  [160/316]  eta: 0:00:05  lr: 0.000250  min_lr: 0.000250  loss: 0.9002 (0.9435)  class_acc: 0.6250 (0.5109)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 86.1238 (86.1238)  time: 0.0326  data: 0.0001  max mem: 2403
Epoch: [2]  [170/316]  eta: 0:00:04  lr: 0.000250  min_lr: 0.000250  loss: 0.9244 (0.9420)  class_acc: 0.6250 (0.5205)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 86.1238 (86.1238)  time: 0.0325  data: 0.0001  max mem: 2403
Epoch: [2]  [180/316]  eta: 0:00:04  lr: 0.000250  min_lr: 0.000250  loss: 0.9653 (0.9475)  class_acc: 0.6250 (0.5231)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 86.1238 (86.1238)  time: 0.0323  data: 0.0001  max mem: 2403
Epoch: [2]  [190/316]  eta: 0:00:04  lr: 0.000250  min_lr: 0.000250  loss: 1.0868 (0.9502)  class_acc: 0.5625 (0.5272)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 86.1238 (86.1238)  time: 0.0322  data: 0.0001  max mem: 2403
Epoch: [2]  [200/316]  eta: 0:00:03  lr: 0.000286  min_lr: 0.000286  loss: 1.0728 (0.9605)  class_acc: 0.5625 (0.5271)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 82.0553 (84.0896)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [2]  [210/316]  eta: 0:00:03  lr: 0.000286  min_lr: 0.000286  loss: 1.1197 (0.9691)  class_acc: 0.5625 (0.5317)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 82.0553 (84.0896)  time: 0.0323  data: 0.0001  max mem: 2403
Epoch: [2]  [220/316]  eta: 0:00:03  lr: 0.000286  min_lr: 0.000286  loss: 1.0417 (0.9785)  class_acc: 0.6250 (0.5348)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 82.0553 (84.0896)  time: 0.0307  data: 0.0001  max mem: 2403
Epoch: [2]  [230/316]  eta: 0:00:02  lr: 0.000286  min_lr: 0.000286  loss: 1.0578 (0.9910)  class_acc: 0.6250 (0.5368)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 82.0553 (84.0896)  time: 0.0302  data: 0.0001  max mem: 2403
Epoch: [2]  [240/316]  eta: 0:00:02  lr: 0.000286  min_lr: 0.000286  loss: 1.1191 (0.9962)  class_acc: 0.6250 (0.5402)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 82.0553 (84.0896)  time: 0.0303  data: 0.0001  max mem: 2403
Epoch: [2]  [250/316]  eta: 0:00:02  lr: 0.000286  min_lr: 0.000286  loss: 1.0413 (1.0057)  class_acc: 0.6250 (0.5433)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 82.0553 (84.0896)  time: 0.0305  data: 0.0001  max mem: 2403
Epoch: [2]  [260/316]  eta: 0:00:01  lr: 0.000286  min_lr: 0.000286  loss: 1.0413 (1.0065)  class_acc: 0.6250 (0.5474)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 82.0553 (84.0896)  time: 0.0305  data: 0.0001  max mem: 2403
Epoch: [2]  [270/316]  eta: 0:00:01  lr: 0.000286  min_lr: 0.000286  loss: 1.1092 (1.0119)  class_acc: 0.6250 (0.5496)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 82.0553 (84.0896)  time: 0.0304  data: 0.0001  max mem: 2403
Epoch: [2]  [280/316]  eta: 0:00:01  lr: 0.000286  min_lr: 0.000286  loss: 1.1525 (1.0159)  class_acc: 0.5625 (0.5509)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 82.0553 (84.0896)  time: 0.0305  data: 0.0001  max mem: 2403
Epoch: [2]  [290/316]  eta: 0:00:00  lr: 0.000286  min_lr: 0.000286  loss: 1.2813 (1.0247)  class_acc: 0.5625 (0.5518)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 82.0553 (84.0896)  time: 0.0305  data: 0.0001  max mem: 2403
Epoch: [2]  [300/316]  eta: 0:00:00  lr: 0.000286  min_lr: 0.000286  loss: 1.2173 (1.0307)  class_acc: 0.5625 (0.5519)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 86.1238 (88.4341)  time: 0.0293  data: 0.0001  max mem: 2403
Epoch: [2]  [310/316]  eta: 0:00:00  lr: 0.000286  min_lr: 0.000286  loss: 1.2173 (1.0307)  class_acc: 0.5625 (0.5519)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 86.1238 (88.4341)  time: 0.0141  data: 0.0001  max mem: 2403
Epoch: [2]  [315/316]  eta: 0:00:00  lr: 0.000286  min_lr: 0.000286  loss: 1.2173 (1.0307)  class_acc: 0.5625 (0.5519)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 86.1238 (88.4341)  time: 0.0061  data: 0.0000  max mem: 2403
Epoch: [2] Total time: 0:00:09 (0.0313 s / it)
Averaged stats: lr: 0.000286  min_lr: 0.000286  loss: 1.2173 (1.0307)  class_acc: 0.5625 (0.5519)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 86.1238 (88.4341)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:15  loss: 0.5078 (0.5078)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2614  data: 0.2477  max mem: 2403
Val:  [10/60]  eta: 0:00:01  loss: 0.6172 (0.6021)  accuracy: 0.5000 (0.3674)  balanced_accuracy: 0.4615 (0.3150)  pr_auc: 0.4615 (0.3989)  roc_auc: 0.4143 (0.3178)  time: 0.0385  data: 0.0234  max mem: 2403
Val:  [20/60]  eta: 0:00:01  loss: 0.6004 (0.6078)  accuracy: 0.5833 (0.5238)  balanced_accuracy: 0.4786 (0.4067)  pr_auc: 0.6675 (0.5800)  roc_auc: 0.4741 (0.4260)  time: 0.0155  data: 0.0006  max mem: 2403
Val:  [30/60]  eta: 0:00:00  loss: 0.6214 (0.6199)  accuracy: 0.6667 (0.5578)  balanced_accuracy: 0.5000 (0.4404)  pr_auc: 0.7633 (0.6325)  roc_auc: 0.5463 (0.4766)  time: 0.0149  data: 0.0002  max mem: 2403
Val:  [40/60]  eta: 0:00:00  loss: 0.6769 (0.6440)  accuracy: 0.5833 (0.5620)  balanced_accuracy: 0.5000 (0.4529)  pr_auc: 0.6574 (0.6205)  roc_auc: 0.5156 (0.4645)  time: 0.0148  data: 0.0002  max mem: 2403
Val:  [50/60]  eta: 0:00:00  loss: 0.7456 (0.6674)  accuracy: 0.5417 (0.5531)  balanced_accuracy: 0.5000 (0.4617)  pr_auc: 0.5603 (0.6017)  roc_auc: 0.4545 (0.4544)  time: 0.0143  data: 0.0001  max mem: 2403
Val:  [59/60]  eta: 0:00:00  loss: 0.8027 (0.7059)  accuracy: 0.5000 (0.5031)  balanced_accuracy: 0.4685 (0.4379)  pr_auc: 0.4656 (0.5487)  roc_auc: 0.3576 (0.4179)  time: 0.0134  data: 0.0001  max mem: 2403
Val: Total time: 0:00:01 (0.0209 s / it)
* loss 0.706
Accuracy of the network on the 1431 test EEG: 0.57%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [3]  [  0/316]  eta: 0:01:29  lr: 0.000321  min_lr: 0.000321  loss: 0.7003 (0.7003)  class_acc: 0.6875 (0.6875)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2820  data: 0.2428  max mem: 2403
Epoch: [3]  [ 10/316]  eta: 0:00:16  lr: 0.000321  min_lr: 0.000321  loss: 0.6568 (0.6606)  class_acc: 0.6250 (0.6193)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0541  data: 0.0222  max mem: 2403
Epoch: [3]  [ 20/316]  eta: 0:00:12  lr: 0.000321  min_lr: 0.000321  loss: 0.6418 (0.6525)  class_acc: 0.6250 (0.6399)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0319  data: 0.0001  max mem: 2403
Epoch: [3]  [ 30/316]  eta: 0:00:11  lr: 0.000321  min_lr: 0.000321  loss: 0.6423 (0.6558)  class_acc: 0.6250 (0.6250)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0327  data: 0.0001  max mem: 2403
Epoch: [3]  [ 40/316]  eta: 0:00:10  lr: 0.000321  min_lr: 0.000321  loss: 0.6863 (0.6720)  class_acc: 0.5625 (0.6189)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0331  data: 0.0001  max mem: 2403
Epoch: [3]  [ 50/316]  eta: 0:00:09  lr: 0.000321  min_lr: 0.000321  loss: 0.6423 (0.6560)  class_acc: 0.6250 (0.6262)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0333  data: 0.0001  max mem: 2403
Epoch: [3]  [ 60/316]  eta: 0:00:09  lr: 0.000321  min_lr: 0.000321  loss: 0.6057 (0.6654)  class_acc: 0.6250 (0.6270)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [3]  [ 70/316]  eta: 0:00:08  lr: 0.000321  min_lr: 0.000321  loss: 0.7190 (0.6752)  class_acc: 0.5625 (0.6162)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [3]  [ 80/316]  eta: 0:00:08  lr: 0.000321  min_lr: 0.000321  loss: 0.7190 (0.6833)  class_acc: 0.5625 (0.6111)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0327  data: 0.0001  max mem: 2403
Epoch: [3]  [ 90/316]  eta: 0:00:08  lr: 0.000321  min_lr: 0.000321  loss: 0.7248 (0.6909)  class_acc: 0.5625 (0.6023)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0333  data: 0.0001  max mem: 2403
Epoch: [3]  [100/316]  eta: 0:00:07  lr: 0.000357  min_lr: 0.000357  loss: 0.6710 (0.6901)  class_acc: 0.6250 (0.6040)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2138 (5.2138)  time: 0.0338  data: 0.0001  max mem: 2403
Epoch: [3]  [110/316]  eta: 0:00:07  lr: 0.000357  min_lr: 0.000357  loss: 0.8809 (0.7270)  class_acc: 0.5000 (0.5884)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2138 (5.2138)  time: 0.0340  data: 0.0001  max mem: 2403
Epoch: [3]  [120/316]  eta: 0:00:06  lr: 0.000357  min_lr: 0.000357  loss: 1.1175 (0.7732)  class_acc: 0.3750 (0.5692)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2138 (5.2138)  time: 0.0336  data: 0.0001  max mem: 2403
Epoch: [3]  [130/316]  eta: 0:00:06  lr: 0.000357  min_lr: 0.000357  loss: 1.1028 (0.7960)  class_acc: 0.4375 (0.5611)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2138 (5.2138)  time: 0.0331  data: 0.0001  max mem: 2403
Epoch: [3]  [140/316]  eta: 0:00:06  lr: 0.000357  min_lr: 0.000357  loss: 1.0052 (0.8180)  class_acc: 0.4375 (0.5496)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2138 (5.2138)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [3]  [150/316]  eta: 0:00:05  lr: 0.000357  min_lr: 0.000357  loss: 1.0987 (0.8439)  class_acc: 0.3750 (0.5364)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2138 (5.2138)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [3]  [160/316]  eta: 0:00:05  lr: 0.000357  min_lr: 0.000357  loss: 1.1232 (0.8626)  class_acc: 0.3750 (0.5264)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2138 (5.2138)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [3]  [170/316]  eta: 0:00:05  lr: 0.000357  min_lr: 0.000357  loss: 1.1416 (0.8820)  class_acc: 0.3125 (0.5161)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2138 (5.2138)  time: 0.0330  data: 0.0001  max mem: 2403
Epoch: [3]  [180/316]  eta: 0:00:04  lr: 0.000357  min_lr: 0.000357  loss: 1.1262 (0.8930)  class_acc: 0.3750 (0.5114)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2138 (5.2138)  time: 0.0330  data: 0.0001  max mem: 2403
Epoch: [3]  [190/316]  eta: 0:00:04  lr: 0.000357  min_lr: 0.000357  loss: 1.0160 (0.8999)  class_acc: 0.4375 (0.5062)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2138 (5.2138)  time: 0.0332  data: 0.0001  max mem: 2403
Epoch: [3]  [200/316]  eta: 0:00:03  lr: 0.000393  min_lr: 0.000393  loss: 0.9729 (0.9028)  class_acc: 0.4375 (0.5059)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2138 (60.7685)  time: 0.0333  data: 0.0001  max mem: 2403
Epoch: [3]  [210/316]  eta: 0:00:03  lr: 0.000393  min_lr: 0.000393  loss: 0.7931 (0.8974)  class_acc: 0.5000 (0.5062)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2138 (60.7685)  time: 0.0325  data: 0.0001  max mem: 2403
Epoch: [3]  [220/316]  eta: 0:00:03  lr: 0.000393  min_lr: 0.000393  loss: 0.7681 (0.8919)  class_acc: 0.4375 (0.5034)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2138 (60.7685)  time: 0.0317  data: 0.0001  max mem: 2403
Epoch: [3]  [230/316]  eta: 0:00:02  lr: 0.000393  min_lr: 0.000393  loss: 0.7784 (0.8879)  class_acc: 0.4375 (0.5032)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2138 (60.7685)  time: 0.0318  data: 0.0001  max mem: 2403
Epoch: [3]  [240/316]  eta: 0:00:02  lr: 0.000393  min_lr: 0.000393  loss: 0.7406 (0.8805)  class_acc: 0.5625 (0.5057)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2138 (60.7685)  time: 0.0320  data: 0.0001  max mem: 2403
Epoch: [3]  [250/316]  eta: 0:00:02  lr: 0.000393  min_lr: 0.000393  loss: 0.7137 (0.8782)  class_acc: 0.5000 (0.5037)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2138 (60.7685)  time: 0.0323  data: 0.0001  max mem: 2403
Epoch: [3]  [260/316]  eta: 0:00:01  lr: 0.000393  min_lr: 0.000393  loss: 0.7951 (0.8750)  class_acc: 0.5000 (0.5026)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2138 (60.7685)  time: 0.0322  data: 0.0001  max mem: 2403
Epoch: [3]  [270/316]  eta: 0:00:01  lr: 0.000393  min_lr: 0.000393  loss: 0.7532 (0.8713)  class_acc: 0.5000 (0.5005)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2138 (60.7685)  time: 0.0322  data: 0.0001  max mem: 2403
Epoch: [3]  [280/316]  eta: 0:00:01  lr: 0.000393  min_lr: 0.000393  loss: 0.7338 (0.8654)  class_acc: 0.5000 (0.5027)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2138 (60.7685)  time: 0.0323  data: 0.0001  max mem: 2403
Epoch: [3]  [290/316]  eta: 0:00:00  lr: 0.000393  min_lr: 0.000393  loss: 0.7444 (0.8624)  class_acc: 0.5000 (0.5015)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2138 (60.7685)  time: 0.0317  data: 0.0001  max mem: 2403
Epoch: [3]  [300/316]  eta: 0:00:00  lr: 0.000393  min_lr: 0.000393  loss: 0.7458 (0.8577)  class_acc: 0.5000 (0.5027)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 46.0669 (55.8680)  time: 0.0294  data: 0.0001  max mem: 2403
Epoch: [3]  [310/316]  eta: 0:00:00  lr: 0.000393  min_lr: 0.000393  loss: 0.7458 (0.8577)  class_acc: 0.5000 (0.5027)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 46.0669 (55.8680)  time: 0.0137  data: 0.0001  max mem: 2403
Epoch: [3]  [315/316]  eta: 0:00:00  lr: 0.000393  min_lr: 0.000393  loss: 0.7458 (0.8577)  class_acc: 0.5000 (0.5027)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 46.0669 (55.8680)  time: 0.0061  data: 0.0000  max mem: 2403
Epoch: [3] Total time: 0:00:10 (0.0322 s / it)
Averaged stats: lr: 0.000393  min_lr: 0.000393  loss: 0.7458 (0.8577)  class_acc: 0.5000 (0.5027)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 46.0669 (55.8680)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:15  loss: 0.1507 (0.1507)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2526  data: 0.2273  max mem: 2403
Val:  [10/60]  eta: 0:00:01  loss: 0.7930 (0.6425)  accuracy: 0.5417 (0.3826)  balanced_accuracy: 0.5000 (0.3182)  pr_auc: 0.4499 (0.4004)  roc_auc: 0.4000 (0.3228)  time: 0.0388  data: 0.0209  max mem: 2403
Val:  [20/60]  eta: 0:00:01  loss: 0.6423 (0.6733)  accuracy: 0.6250 (0.5397)  balanced_accuracy: 0.5000 (0.4048)  pr_auc: 0.6498 (0.5707)  roc_auc: 0.4357 (0.4063)  time: 0.0161  data: 0.0002  max mem: 2403
Val:  [30/60]  eta: 0:00:00  loss: 0.7892 (0.7262)  accuracy: 0.6667 (0.5699)  balanced_accuracy: 0.5000 (0.4355)  pr_auc: 0.7500 (0.6266)  roc_auc: 0.5078 (0.4668)  time: 0.0148  data: 0.0002  max mem: 2403
Val:  [40/60]  eta: 0:00:00  loss: 0.8901 (0.7874)  accuracy: 0.6250 (0.5742)  balanced_accuracy: 0.5000 (0.4512)  pr_auc: 0.6905 (0.6247)  roc_auc: 0.5259 (0.4653)  time: 0.0148  data: 0.0002  max mem: 2403
Val:  [50/60]  eta: 0:00:00  loss: 1.0346 (0.8524)  accuracy: 0.5417 (0.5637)  balanced_accuracy: 0.5000 (0.4608)  pr_auc: 0.6045 (0.6159)  roc_auc: 0.4931 (0.4691)  time: 0.0143  data: 0.0001  max mem: 2403
Val:  [59/60]  eta: 0:00:00  loss: 1.2242 (0.9825)  accuracy: 0.5000 (0.5171)  balanced_accuracy: 0.5000 (0.4444)  pr_auc: 0.4482 (0.5569)  roc_auc: 0.3047 (0.4213)  time: 0.0134  data: 0.0001  max mem: 2403
Val: Total time: 0:00:01 (0.0209 s / it)
* loss 0.983
Accuracy of the network on the 1431 test EEG: 0.58%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [4]  [  0/316]  eta: 0:01:10  lr: 0.000429  min_lr: 0.000429  loss: 0.8199 (0.8199)  class_acc: 0.6875 (0.6875)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2235  data: 0.1864  max mem: 2403
Epoch: [4]  [ 10/316]  eta: 0:00:16  lr: 0.000429  min_lr: 0.000429  loss: 0.8684 (0.8533)  class_acc: 0.6250 (0.6477)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0529  data: 0.0201  max mem: 2403
Epoch: [4]  [ 20/316]  eta: 0:00:12  lr: 0.000429  min_lr: 0.000429  loss: 0.7827 (0.8046)  class_acc: 0.6875 (0.6637)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0343  data: 0.0018  max mem: 2403
Epoch: [4]  [ 30/316]  eta: 0:00:11  lr: 0.000429  min_lr: 0.000429  loss: 0.7827 (0.8077)  class_acc: 0.6875 (0.6633)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [4]  [ 40/316]  eta: 0:00:10  lr: 0.000429  min_lr: 0.000429  loss: 0.8855 (0.8686)  class_acc: 0.6250 (0.6433)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0325  data: 0.0001  max mem: 2403
Epoch: [4]  [ 50/316]  eta: 0:00:09  lr: 0.000429  min_lr: 0.000429  loss: 0.9262 (0.8621)  class_acc: 0.6250 (0.6458)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0318  data: 0.0001  max mem: 2403
Epoch: [4]  [ 60/316]  eta: 0:00:09  lr: 0.000429  min_lr: 0.000429  loss: 0.9050 (0.8847)  class_acc: 0.6250 (0.6383)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0318  data: 0.0001  max mem: 2403
Epoch: [4]  [ 70/316]  eta: 0:00:08  lr: 0.000429  min_lr: 0.000429  loss: 0.9737 (0.9014)  class_acc: 0.6250 (0.6285)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0317  data: 0.0001  max mem: 2403
Epoch: [4]  [ 80/316]  eta: 0:00:08  lr: 0.000429  min_lr: 0.000429  loss: 0.9493 (0.9076)  class_acc: 0.6250 (0.6250)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0313  data: 0.0001  max mem: 2403
Epoch: [4]  [ 90/316]  eta: 0:00:07  lr: 0.000429  min_lr: 0.000429  loss: 0.9399 (0.9205)  class_acc: 0.5625 (0.6174)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0312  data: 0.0001  max mem: 2403
Epoch: [4]  [100/316]  eta: 0:00:07  lr: 0.000464  min_lr: 0.000464  loss: 0.9114 (0.9131)  class_acc: 0.6250 (0.6231)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (74.3719)  time: 0.0318  data: 0.0001  max mem: 2403
Epoch: [4]  [110/316]  eta: 0:00:06  lr: 0.000464  min_lr: 0.000464  loss: 0.8925 (0.9185)  class_acc: 0.6875 (0.6216)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (74.3719)  time: 0.0317  data: 0.0001  max mem: 2403
Epoch: [4]  [120/316]  eta: 0:00:06  lr: 0.000464  min_lr: 0.000464  loss: 0.8718 (0.9080)  class_acc: 0.6875 (0.6255)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (74.3719)  time: 0.0311  data: 0.0001  max mem: 2403
Epoch: [4]  [130/316]  eta: 0:00:06  lr: 0.000464  min_lr: 0.000464  loss: 0.9850 (0.9189)  class_acc: 0.5625 (0.6202)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (74.3719)  time: 0.0313  data: 0.0001  max mem: 2403
Epoch: [4]  [140/316]  eta: 0:00:05  lr: 0.000464  min_lr: 0.000464  loss: 0.9850 (0.9218)  class_acc: 0.5625 (0.6201)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (74.3719)  time: 0.0316  data: 0.0001  max mem: 2403
Epoch: [4]  [150/316]  eta: 0:00:05  lr: 0.000464  min_lr: 0.000464  loss: 0.7890 (0.9125)  class_acc: 0.6875 (0.6250)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (74.3719)  time: 0.0315  data: 0.0001  max mem: 2403
Epoch: [4]  [160/316]  eta: 0:00:05  lr: 0.000464  min_lr: 0.000464  loss: 0.8597 (0.9167)  class_acc: 0.6250 (0.6242)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (74.3719)  time: 0.0318  data: 0.0001  max mem: 2403
Epoch: [4]  [170/316]  eta: 0:00:04  lr: 0.000464  min_lr: 0.000464  loss: 0.8675 (0.9090)  class_acc: 0.6250 (0.6272)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (74.3719)  time: 0.0316  data: 0.0001  max mem: 2403
Epoch: [4]  [180/316]  eta: 0:00:04  lr: 0.000464  min_lr: 0.000464  loss: 0.9090 (0.9169)  class_acc: 0.6250 (0.6240)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (74.3719)  time: 0.0314  data: 0.0001  max mem: 2403
Epoch: [4]  [190/316]  eta: 0:00:04  lr: 0.000464  min_lr: 0.000464  loss: 1.0435 (0.9187)  class_acc: 0.5625 (0.6224)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (74.3719)  time: 0.0322  data: 0.0001  max mem: 2403
Epoch: [4]  [200/316]  eta: 0:00:03  lr: 0.000500  min_lr: 0.000500  loss: 1.0383 (0.9244)  class_acc: 0.5625 (0.6188)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (76.4353)  time: 0.0332  data: 0.0001  max mem: 2403
Epoch: [4]  [210/316]  eta: 0:00:03  lr: 0.000500  min_lr: 0.000500  loss: 0.8205 (0.9149)  class_acc: 0.5625 (0.6158)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (76.4353)  time: 0.0332  data: 0.0001  max mem: 2403
Epoch: [4]  [220/316]  eta: 0:00:03  lr: 0.000500  min_lr: 0.000500  loss: 0.7499 (0.9075)  class_acc: 0.5000 (0.6109)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (76.4353)  time: 0.0324  data: 0.0001  max mem: 2403
Epoch: [4]  [230/316]  eta: 0:00:02  lr: 0.000500  min_lr: 0.000500  loss: 0.7529 (0.9029)  class_acc: 0.5000 (0.6055)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (76.4353)  time: 0.0319  data: 0.0001  max mem: 2403
Epoch: [4]  [240/316]  eta: 0:00:02  lr: 0.000500  min_lr: 0.000500  loss: 0.7261 (0.8942)  class_acc: 0.5000 (0.6053)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (76.4353)  time: 0.0320  data: 0.0001  max mem: 2403
Epoch: [4]  [250/316]  eta: 0:00:02  lr: 0.000500  min_lr: 0.000500  loss: 0.6655 (0.8869)  class_acc: 0.5625 (0.6038)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (76.4353)  time: 0.0321  data: 0.0001  max mem: 2403
Epoch: [4]  [260/316]  eta: 0:00:01  lr: 0.000500  min_lr: 0.000500  loss: 0.6823 (0.8790)  class_acc: 0.5625 (0.6032)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (76.4353)  time: 0.0321  data: 0.0001  max mem: 2403
Epoch: [4]  [270/316]  eta: 0:00:01  lr: 0.000500  min_lr: 0.000500  loss: 0.6645 (0.8731)  class_acc: 0.5625 (0.6019)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (76.4353)  time: 0.0319  data: 0.0001  max mem: 2403
Epoch: [4]  [280/316]  eta: 0:00:01  lr: 0.000500  min_lr: 0.000500  loss: 0.7067 (0.8683)  class_acc: 0.5625 (0.5992)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (76.4353)  time: 0.0321  data: 0.0001  max mem: 2403
Epoch: [4]  [290/316]  eta: 0:00:00  lr: 0.000500  min_lr: 0.000500  loss: 0.6973 (0.8614)  class_acc: 0.5625 (0.5999)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (76.4353)  time: 0.0323  data: 0.0001  max mem: 2403
Epoch: [4]  [300/316]  eta: 0:00:00  lr: 0.000500  min_lr: 0.000500  loss: 0.6885 (0.8578)  class_acc: 0.5625 (0.5985)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (54.8288)  time: 0.0300  data: 0.0001  max mem: 2403
Epoch: [4]  [310/316]  eta: 0:00:00  lr: 0.000500  min_lr: 0.000500  loss: 0.6885 (0.8578)  class_acc: 0.5625 (0.5985)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (54.8288)  time: 0.0140  data: 0.0001  max mem: 2403
Epoch: [4]  [315/316]  eta: 0:00:00  lr: 0.000500  min_lr: 0.000500  loss: 0.6885 (0.8578)  class_acc: 0.5625 (0.5985)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (54.8288)  time: 0.0061  data: 0.0000  max mem: 2403
Epoch: [4] Total time: 0:00:09 (0.0315 s / it)
Averaged stats: lr: 0.000500  min_lr: 0.000500  loss: 0.6885 (0.8578)  class_acc: 0.5625 (0.5985)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 74.3719 (54.8288)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:14  loss: 1.3756 (1.3756)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2398  data: 0.2235  max mem: 2403
Val:  [10/60]  eta: 0:00:01  loss: 1.0363 (1.0361)  accuracy: 0.3333 (0.2538)  balanced_accuracy: 0.5000 (0.3182)  pr_auc: 0.5026 (0.3965)  roc_auc: 0.3857 (0.3236)  time: 0.0375  data: 0.0213  max mem: 2403
Val:  [20/60]  eta: 0:00:01  loss: 0.9900 (1.0098)  accuracy: 0.2917 (0.2738)  balanced_accuracy: 0.5000 (0.4074)  pr_auc: 0.6845 (0.5844)  roc_auc: 0.5071 (0.4373)  time: 0.0159  data: 0.0006  max mem: 2403
Val:  [30/60]  eta: 0:00:00  loss: 0.9267 (0.9788)  accuracy: 0.3333 (0.3065)  balanced_accuracy: 0.5000 (0.4394)  pr_auc: 0.7863 (0.6396)  roc_auc: 0.5778 (0.4876)  time: 0.0149  data: 0.0002  max mem: 2403
Val:  [40/60]  eta: 0:00:00  loss: 0.9230 (0.9675)  accuracy: 0.3750 (0.3313)  balanced_accuracy: 0.5000 (0.4526)  pr_auc: 0.6389 (0.6228)  roc_auc: 0.4825 (0.4622)  time: 0.0146  data: 0.0002  max mem: 2403
Val:  [50/60]  eta: 0:00:00  loss: 0.8983 (0.9573)  accuracy: 0.4583 (0.3578)  balanced_accuracy: 0.5000 (0.4595)  pr_auc: 0.4793 (0.5855)  roc_auc: 0.3333 (0.4251)  time: 0.0140  data: 0.0001  max mem: 2403
Val:  [59/60]  eta: 0:00:00  loss: 0.8483 (0.9020)  accuracy: 0.5000 (0.3697)  balanced_accuracy: 0.5000 (0.4421)  pr_auc: 0.4102 (0.5347)  roc_auc: 0.2587 (0.4019)  time: 0.0135  data: 0.0001  max mem: 2403
Val: Total time: 0:00:01 (0.0210 s / it)
* loss 0.902
Accuracy of the network on the 1431 test EEG: 0.42%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [5]  [  0/316]  eta: 0:01:18  lr: 0.000500  min_lr: 0.000500  loss: 0.7524 (0.7524)  class_acc: 0.5000 (0.5000)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2490  data: 0.2027  max mem: 2403
Epoch: [5]  [ 10/316]  eta: 0:00:15  lr: 0.000500  min_lr: 0.000500  loss: 0.9936 (0.9986)  class_acc: 0.4375 (0.4091)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0504  data: 0.0185  max mem: 2403
Epoch: [5]  [ 20/316]  eta: 0:00:12  lr: 0.000500  min_lr: 0.000500  loss: 1.0129 (0.9995)  class_acc: 0.3750 (0.4048)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0303  data: 0.0001  max mem: 2403
Epoch: [5]  [ 30/316]  eta: 0:00:10  lr: 0.000500  min_lr: 0.000500  loss: 1.0491 (1.0045)  class_acc: 0.3750 (0.3972)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0297  data: 0.0001  max mem: 2403
Epoch: [5]  [ 40/316]  eta: 0:00:09  lr: 0.000500  min_lr: 0.000500  loss: 0.9101 (0.9765)  class_acc: 0.4375 (0.4162)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0295  data: 0.0001  max mem: 2403
Epoch: [5]  [ 50/316]  eta: 0:00:09  lr: 0.000500  min_lr: 0.000500  loss: 0.9099 (0.9765)  class_acc: 0.4375 (0.4142)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0295  data: 0.0001  max mem: 2403
Epoch: [5]  [ 60/316]  eta: 0:00:08  lr: 0.000500  min_lr: 0.000500  loss: 0.9970 (0.9884)  class_acc: 0.3750 (0.4098)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0295  data: 0.0001  max mem: 2403
Epoch: [5]  [ 70/316]  eta: 0:00:08  lr: 0.000500  min_lr: 0.000500  loss: 0.9610 (0.9790)  class_acc: 0.3750 (0.4155)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0304  data: 0.0001  max mem: 2403
Epoch: [5]  [ 80/316]  eta: 0:00:07  lr: 0.000500  min_lr: 0.000500  loss: 0.9063 (0.9731)  class_acc: 0.3750 (0.4159)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0312  data: 0.0001  max mem: 2403
Epoch: [5]  [ 90/316]  eta: 0:00:07  lr: 0.000500  min_lr: 0.000500  loss: 0.9097 (0.9678)  class_acc: 0.4375 (0.4231)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0306  data: 0.0001  max mem: 2403
Epoch: [5]  [100/316]  eta: 0:00:06  lr: 0.000500  min_lr: 0.000500  loss: 0.9097 (0.9662)  class_acc: 0.4375 (0.4276)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 100.4403 (100.4403)  time: 0.0301  data: 0.0001  max mem: 2403
Epoch: [5]  [110/316]  eta: 0:00:06  lr: 0.000500  min_lr: 0.000500  loss: 0.7565 (0.9428)  class_acc: 0.5625 (0.4398)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 100.4403 (100.4403)  time: 0.0303  data: 0.0001  max mem: 2403
Epoch: [5]  [120/316]  eta: 0:00:06  lr: 0.000500  min_lr: 0.000500  loss: 0.7358 (0.9266)  class_acc: 0.5000 (0.4442)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 100.4403 (100.4403)  time: 0.0305  data: 0.0001  max mem: 2403
Epoch: [5]  [130/316]  eta: 0:00:05  lr: 0.000500  min_lr: 0.000500  loss: 0.7416 (0.9124)  class_acc: 0.5000 (0.4518)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 100.4403 (100.4403)  time: 0.0307  data: 0.0001  max mem: 2403
Epoch: [5]  [140/316]  eta: 0:00:05  lr: 0.000500  min_lr: 0.000500  loss: 0.7242 (0.8989)  class_acc: 0.5000 (0.4605)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 100.4403 (100.4403)  time: 0.0311  data: 0.0001  max mem: 2403
Epoch: [5]  [150/316]  eta: 0:00:05  lr: 0.000500  min_lr: 0.000500  loss: 0.7494 (0.8911)  class_acc: 0.5000 (0.4623)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 100.4403 (100.4403)  time: 0.0309  data: 0.0001  max mem: 2403
Epoch: [5]  [160/316]  eta: 0:00:04  lr: 0.000500  min_lr: 0.000500  loss: 0.7498 (0.8813)  class_acc: 0.5000 (0.4662)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 100.4403 (100.4403)  time: 0.0305  data: 0.0001  max mem: 2403
Epoch: [5]  [170/316]  eta: 0:00:04  lr: 0.000500  min_lr: 0.000500  loss: 0.7096 (0.8701)  class_acc: 0.5625 (0.4719)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 100.4403 (100.4403)  time: 0.0303  data: 0.0001  max mem: 2403
Epoch: [5]  [180/316]  eta: 0:00:04  lr: 0.000500  min_lr: 0.000500  loss: 0.6909 (0.8597)  class_acc: 0.5625 (0.4776)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 100.4403 (100.4403)  time: 0.0303  data: 0.0001  max mem: 2403
Epoch: [5]  [190/316]  eta: 0:00:03  lr: 0.000500  min_lr: 0.000500  loss: 0.7077 (0.8528)  class_acc: 0.5625 (0.4800)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 100.4403 (100.4403)  time: 0.0304  data: 0.0001  max mem: 2403
Epoch: [5]  [200/316]  eta: 0:00:03  lr: 0.000499  min_lr: 0.000499  loss: 0.7113 (0.8458)  class_acc: 0.5625 (0.4851)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.6844 (67.0624)  time: 0.0308  data: 0.0001  max mem: 2403
Epoch: [5]  [210/316]  eta: 0:00:03  lr: 0.000499  min_lr: 0.000499  loss: 0.7806 (0.8435)  class_acc: 0.6250 (0.4911)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.6844 (67.0624)  time: 0.0309  data: 0.0001  max mem: 2403
Epoch: [5]  [220/316]  eta: 0:00:03  lr: 0.000499  min_lr: 0.000499  loss: 0.8536 (0.8445)  class_acc: 0.6250 (0.4966)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.6844 (67.0624)  time: 0.0307  data: 0.0001  max mem: 2403
Epoch: [5]  [230/316]  eta: 0:00:02  lr: 0.000499  min_lr: 0.000499  loss: 0.8710 (0.8470)  class_acc: 0.6250 (0.5003)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.6844 (67.0624)  time: 0.0308  data: 0.0001  max mem: 2403
Epoch: [5]  [240/316]  eta: 0:00:02  lr: 0.000499  min_lr: 0.000499  loss: 0.8394 (0.8456)  class_acc: 0.6250 (0.5047)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.6844 (67.0624)  time: 0.0307  data: 0.0001  max mem: 2403
Epoch: [5]  [250/316]  eta: 0:00:02  lr: 0.000499  min_lr: 0.000499  loss: 0.8005 (0.8429)  class_acc: 0.6250 (0.5090)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.6844 (67.0624)  time: 0.0303  data: 0.0001  max mem: 2403
Epoch: [5]  [260/316]  eta: 0:00:01  lr: 0.000499  min_lr: 0.000499  loss: 0.7408 (0.8377)  class_acc: 0.6250 (0.5148)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.6844 (67.0624)  time: 0.0300  data: 0.0001  max mem: 2403
Epoch: [5]  [270/316]  eta: 0:00:01  lr: 0.000499  min_lr: 0.000499  loss: 0.7628 (0.8371)  class_acc: 0.6250 (0.5194)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.6844 (67.0624)  time: 0.0302  data: 0.0001  max mem: 2403
Epoch: [5]  [280/316]  eta: 0:00:01  lr: 0.000499  min_lr: 0.000499  loss: 0.8510 (0.8360)  class_acc: 0.6250 (0.5222)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.6844 (67.0624)  time: 0.0303  data: 0.0001  max mem: 2403
Epoch: [5]  [290/316]  eta: 0:00:00  lr: 0.000499  min_lr: 0.000499  loss: 0.8903 (0.8379)  class_acc: 0.5625 (0.5247)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.6844 (67.0624)  time: 0.0301  data: 0.0001  max mem: 2403
Epoch: [5]  [300/316]  eta: 0:00:00  lr: 0.000499  min_lr: 0.000499  loss: 0.9075 (0.8387)  class_acc: 0.5625 (0.5258)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 62.0385 (65.3877)  time: 0.0286  data: 0.0001  max mem: 2403
Epoch: [5]  [310/316]  eta: 0:00:00  lr: 0.000499  min_lr: 0.000499  loss: 0.9075 (0.8387)  class_acc: 0.5625 (0.5258)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 62.0385 (65.3877)  time: 0.0136  data: 0.0001  max mem: 2403
Epoch: [5]  [315/316]  eta: 0:00:00  lr: 0.000499  min_lr: 0.000499  loss: 0.9075 (0.8387)  class_acc: 0.5625 (0.5258)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 62.0385 (65.3877)  time: 0.0061  data: 0.0000  max mem: 2403
Epoch: [5] Total time: 0:00:09 (0.0300 s / it)
Averaged stats: lr: 0.000499  min_lr: 0.000499  loss: 0.9075 (0.8387)  class_acc: 0.5625 (0.5258)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 62.0385 (65.3877)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:12  loss: 0.2197 (0.2197)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2100  data: 0.1881  max mem: 2403
Val:  [10/60]  eta: 0:00:01  loss: 0.7492 (0.6115)  accuracy: 0.5417 (0.3826)  balanced_accuracy: 0.5000 (0.3182)  pr_auc: 0.4689 (0.3963)  roc_auc: 0.3714 (0.3218)  time: 0.0376  data: 0.0225  max mem: 2403
Val:  [20/60]  eta: 0:00:01  loss: 0.6261 (0.6431)  accuracy: 0.6250 (0.5417)  balanced_accuracy: 0.5000 (0.4077)  pr_auc: 0.6538 (0.5567)  roc_auc: 0.4306 (0.3896)  time: 0.0172  data: 0.0030  max mem: 2403
Val:  [30/60]  eta: 0:00:00  loss: 0.7219 (0.6860)  accuracy: 0.6667 (0.5712)  balanced_accuracy: 0.5000 (0.4375)  pr_auc: 0.7353 (0.6142)  roc_auc: 0.4954 (0.4496)  time: 0.0141  data: 0.0001  max mem: 2403
Val:  [40/60]  eta: 0:00:00  loss: 0.8138 (0.7352)  accuracy: 0.6250 (0.5752)  balanced_accuracy: 0.5000 (0.4527)  pr_auc: 0.6790 (0.6199)  roc_auc: 0.5245 (0.4600)  time: 0.0141  data: 0.0001  max mem: 2403
Val:  [50/60]  eta: 0:00:00  loss: 0.9342 (0.7919)  accuracy: 0.5417 (0.5645)  balanced_accuracy: 0.5000 (0.4620)  pr_auc: 0.6044 (0.6109)  roc_auc: 0.4792 (0.4636)  time: 0.0140  data: 0.0001  max mem: 2403
Val:  [59/60]  eta: 0:00:00  loss: 1.1518 (0.9018)  accuracy: 0.5000 (0.5178)  balanced_accuracy: 0.5000 (0.4455)  pr_auc: 0.4697 (0.5531)  roc_auc: 0.3056 (0.4166)  time: 0.0135  data: 0.0001  max mem: 2403
Val: Total time: 0:00:01 (0.0202 s / it)
* loss 0.902
Accuracy of the network on the 1431 test EEG: 0.58%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [6]  [  0/316]  eta: 0:01:23  lr: 0.000497  min_lr: 0.000497  loss: 0.6140 (0.6140)  class_acc: 0.6875 (0.6875)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2643  data: 0.2150  max mem: 2403
Epoch: [6]  [ 10/316]  eta: 0:00:15  lr: 0.000497  min_lr: 0.000497  loss: 0.8300 (0.7420)  class_acc: 0.6250 (0.6591)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0513  data: 0.0197  max mem: 2403
Epoch: [6]  [ 20/316]  eta: 0:00:12  lr: 0.000497  min_lr: 0.000497  loss: 0.7778 (0.7525)  class_acc: 0.6875 (0.6756)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0300  data: 0.0001  max mem: 2403
Epoch: [6]  [ 30/316]  eta: 0:00:10  lr: 0.000497  min_lr: 0.000497  loss: 0.7375 (0.7573)  class_acc: 0.6875 (0.6794)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0304  data: 0.0001  max mem: 2403
Epoch: [6]  [ 40/316]  eta: 0:00:09  lr: 0.000497  min_lr: 0.000497  loss: 0.8318 (0.7813)  class_acc: 0.6875 (0.6631)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0310  data: 0.0001  max mem: 2403
Epoch: [6]  [ 50/316]  eta: 0:00:09  lr: 0.000497  min_lr: 0.000497  loss: 0.8385 (0.7826)  class_acc: 0.6250 (0.6630)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0312  data: 0.0001  max mem: 2403
Epoch: [6]  [ 60/316]  eta: 0:00:08  lr: 0.000497  min_lr: 0.000497  loss: 0.7164 (0.7957)  class_acc: 0.6250 (0.6527)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0312  data: 0.0001  max mem: 2403
Epoch: [6]  [ 70/316]  eta: 0:00:08  lr: 0.000497  min_lr: 0.000497  loss: 0.9218 (0.8142)  class_acc: 0.6250 (0.6408)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0313  data: 0.0001  max mem: 2403
Epoch: [6]  [ 80/316]  eta: 0:00:07  lr: 0.000497  min_lr: 0.000497  loss: 0.8375 (0.8176)  class_acc: 0.6250 (0.6373)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0313  data: 0.0001  max mem: 2403
Epoch: [6]  [ 90/316]  eta: 0:00:07  lr: 0.000497  min_lr: 0.000497  loss: 0.9234 (0.8329)  class_acc: 0.5625 (0.6291)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0313  data: 0.0001  max mem: 2403
Epoch: [6]  [100/316]  eta: 0:00:07  lr: 0.000495  min_lr: 0.000495  loss: 0.7571 (0.8151)  class_acc: 0.6250 (0.6368)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 63.0434 (63.0434)  time: 0.0316  data: 0.0002  max mem: 2403
Epoch: [6]  [110/316]  eta: 0:00:06  lr: 0.000495  min_lr: 0.000495  loss: 0.6717 (0.8051)  class_acc: 0.6250 (0.6312)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 63.0434 (63.0434)  time: 0.0318  data: 0.0001  max mem: 2403
Epoch: [6]  [120/316]  eta: 0:00:06  lr: 0.000495  min_lr: 0.000495  loss: 0.6659 (0.7904)  class_acc: 0.6250 (0.6343)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 63.0434 (63.0434)  time: 0.0317  data: 0.0001  max mem: 2403
Epoch: [6]  [130/316]  eta: 0:00:06  lr: 0.000495  min_lr: 0.000495  loss: 0.6754 (0.7855)  class_acc: 0.6250 (0.6293)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 63.0434 (63.0434)  time: 0.0316  data: 0.0001  max mem: 2403
Epoch: [6]  [140/316]  eta: 0:00:05  lr: 0.000495  min_lr: 0.000495  loss: 0.7056 (0.7779)  class_acc: 0.6250 (0.6272)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 63.0434 (63.0434)  time: 0.0316  data: 0.0001  max mem: 2403
Epoch: [6]  [150/316]  eta: 0:00:05  lr: 0.000495  min_lr: 0.000495  loss: 0.6697 (0.7689)  class_acc: 0.6250 (0.6283)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 63.0434 (63.0434)  time: 0.0318  data: 0.0001  max mem: 2403
Epoch: [6]  [160/316]  eta: 0:00:05  lr: 0.000495  min_lr: 0.000495  loss: 0.6174 (0.7635)  class_acc: 0.6250 (0.6277)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 63.0434 (63.0434)  time: 0.0320  data: 0.0001  max mem: 2403
Epoch: [6]  [170/316]  eta: 0:00:04  lr: 0.000495  min_lr: 0.000495  loss: 0.6167 (0.7561)  class_acc: 0.6250 (0.6283)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 63.0434 (63.0434)  time: 0.0320  data: 0.0001  max mem: 2403
Epoch: [6]  [180/316]  eta: 0:00:04  lr: 0.000495  min_lr: 0.000495  loss: 0.7087 (0.7537)  class_acc: 0.5625 (0.6240)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 63.0434 (63.0434)  time: 0.0317  data: 0.0001  max mem: 2403
Epoch: [6]  [190/316]  eta: 0:00:04  lr: 0.000495  min_lr: 0.000495  loss: 0.6947 (0.7496)  class_acc: 0.5000 (0.6224)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 63.0434 (63.0434)  time: 0.0317  data: 0.0001  max mem: 2403
Epoch: [6]  [200/316]  eta: 0:00:03  lr: 0.000491  min_lr: 0.000491  loss: 0.6826 (0.7493)  class_acc: 0.5625 (0.6197)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7971 (37.4202)  time: 0.0321  data: 0.0002  max mem: 2403
Epoch: [6]  [210/316]  eta: 0:00:03  lr: 0.000491  min_lr: 0.000491  loss: 0.6752 (0.7465)  class_acc: 0.5625 (0.6185)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7971 (37.4202)  time: 0.0316  data: 0.0002  max mem: 2403
Epoch: [6]  [220/316]  eta: 0:00:03  lr: 0.000491  min_lr: 0.000491  loss: 0.7160 (0.7466)  class_acc: 0.6250 (0.6160)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7971 (37.4202)  time: 0.0308  data: 0.0001  max mem: 2403
Epoch: [6]  [230/316]  eta: 0:00:02  lr: 0.000491  min_lr: 0.000491  loss: 0.7282 (0.7456)  class_acc: 0.6250 (0.6144)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7971 (37.4202)  time: 0.0306  data: 0.0001  max mem: 2403
Epoch: [6]  [240/316]  eta: 0:00:02  lr: 0.000491  min_lr: 0.000491  loss: 0.7249 (0.7453)  class_acc: 0.5625 (0.6105)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7971 (37.4202)  time: 0.0303  data: 0.0001  max mem: 2403
Epoch: [6]  [250/316]  eta: 0:00:02  lr: 0.000491  min_lr: 0.000491  loss: 0.7118 (0.7458)  class_acc: 0.5000 (0.6073)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7971 (37.4202)  time: 0.0300  data: 0.0002  max mem: 2403
Epoch: [6]  [260/316]  eta: 0:00:01  lr: 0.000491  min_lr: 0.000491  loss: 0.7820 (0.7483)  class_acc: 0.5000 (0.6011)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7971 (37.4202)  time: 0.0299  data: 0.0002  max mem: 2403
Epoch: [6]  [270/316]  eta: 0:00:01  lr: 0.000491  min_lr: 0.000491  loss: 0.7880 (0.7506)  class_acc: 0.5000 (0.5964)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7971 (37.4202)  time: 0.0301  data: 0.0002  max mem: 2403
Epoch: [6]  [280/316]  eta: 0:00:01  lr: 0.000491  min_lr: 0.000491  loss: 0.7825 (0.7520)  class_acc: 0.5000 (0.5927)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7971 (37.4202)  time: 0.0306  data: 0.0001  max mem: 2403
Epoch: [6]  [290/316]  eta: 0:00:00  lr: 0.000491  min_lr: 0.000491  loss: 0.7825 (0.7548)  class_acc: 0.5000 (0.5885)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7971 (37.4202)  time: 0.0307  data: 0.0001  max mem: 2403
Epoch: [6]  [300/316]  eta: 0:00:00  lr: 0.000491  min_lr: 0.000491  loss: 0.7544 (0.7535)  class_acc: 0.5000 (0.5875)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 56.1354 (43.6586)  time: 0.0291  data: 0.0001  max mem: 2403
Epoch: [6]  [310/316]  eta: 0:00:00  lr: 0.000491  min_lr: 0.000491  loss: 0.7544 (0.7535)  class_acc: 0.5000 (0.5875)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 56.1354 (43.6586)  time: 0.0138  data: 0.0001  max mem: 2403
Epoch: [6]  [315/316]  eta: 0:00:00  lr: 0.000491  min_lr: 0.000491  loss: 0.7544 (0.7535)  class_acc: 0.5000 (0.5875)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 56.1354 (43.6586)  time: 0.0062  data: 0.0001  max mem: 2403
Epoch: [6] Total time: 0:00:09 (0.0307 s / it)
Averaged stats: lr: 0.000491  min_lr: 0.000491  loss: 0.7544 (0.7535)  class_acc: 0.5000 (0.5875)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 56.1354 (43.6586)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:13  loss: 1.0555 (1.0555)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2320  data: 0.2180  max mem: 2403
Val:  [10/60]  eta: 0:00:01  loss: 0.8140 (0.8356)  accuracy: 0.3333 (0.2576)  balanced_accuracy: 0.3889 (0.3028)  pr_auc: 0.4778 (0.3789)  roc_auc: 0.3857 (0.3025)  time: 0.0368  data: 0.0206  max mem: 2403
Val:  [20/60]  eta: 0:00:01  loss: 0.8140 (0.8292)  accuracy: 0.3333 (0.3155)  balanced_accuracy: 0.4526 (0.4035)  pr_auc: 0.6227 (0.5500)  roc_auc: 0.4167 (0.3815)  time: 0.0158  data: 0.0005  max mem: 2403
Val:  [30/60]  eta: 0:00:00  loss: 0.7848 (0.8064)  accuracy: 0.4167 (0.3562)  balanced_accuracy: 0.5000 (0.4447)  pr_auc: 0.7554 (0.6055)  roc_auc: 0.4922 (0.4379)  time: 0.0143  data: 0.0001  max mem: 2403
Val:  [40/60]  eta: 0:00:00  loss: 0.7675 (0.7994)  accuracy: 0.4583 (0.3811)  balanced_accuracy: 0.5111 (0.4622)  pr_auc: 0.6526 (0.6092)  roc_auc: 0.4889 (0.4414)  time: 0.0143  data: 0.0001  max mem: 2403
Val:  [50/60]  eta: 0:00:00  loss: 0.7522 (0.7947)  accuracy: 0.4583 (0.3971)  balanced_accuracy: 0.5000 (0.4654)  pr_auc: 0.5771 (0.5903)  roc_auc: 0.4444 (0.4345)  time: 0.0141  data: 0.0001  max mem: 2403
Val:  [59/60]  eta: 0:00:00  loss: 0.7255 (0.7729)  accuracy: 0.4167 (0.3948)  balanced_accuracy: 0.4565 (0.4422)  pr_auc: 0.4150 (0.5375)  roc_auc: 0.3264 (0.4033)  time: 0.0135  data: 0.0001  max mem: 2403
Val: Total time: 0:00:01 (0.0205 s / it)
* loss 0.773
Accuracy of the network on the 1431 test EEG: 0.45%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [7]  [  0/316]  eta: 0:01:21  lr: 0.000488  min_lr: 0.000488  loss: 0.4982 (0.4982)  class_acc: 0.8125 (0.8125)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2593  data: 0.2213  max mem: 2403
Epoch: [7]  [ 10/316]  eta: 0:00:16  lr: 0.000488  min_lr: 0.000488  loss: 0.7389 (0.7122)  class_acc: 0.6250 (0.5795)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0545  data: 0.0202  max mem: 2403
Epoch: [7]  [ 20/316]  eta: 0:00:13  lr: 0.000488  min_lr: 0.000488  loss: 0.7389 (0.7372)  class_acc: 0.5625 (0.5417)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0340  data: 0.0001  max mem: 2403
Epoch: [7]  [ 30/316]  eta: 0:00:11  lr: 0.000488  min_lr: 0.000488  loss: 0.7582 (0.7570)  class_acc: 0.4375 (0.5222)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0332  data: 0.0001  max mem: 2403
Epoch: [7]  [ 40/316]  eta: 0:00:10  lr: 0.000488  min_lr: 0.000488  loss: 0.7516 (0.7446)  class_acc: 0.5000 (0.5320)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0323  data: 0.0001  max mem: 2403
Epoch: [7]  [ 50/316]  eta: 0:00:09  lr: 0.000488  min_lr: 0.000488  loss: 0.7078 (0.7428)  class_acc: 0.5625 (0.5355)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0321  data: 0.0001  max mem: 2403
Epoch: [7]  [ 60/316]  eta: 0:00:09  lr: 0.000488  min_lr: 0.000488  loss: 0.7392 (0.7483)  class_acc: 0.5625 (0.5277)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0321  data: 0.0001  max mem: 2403
Epoch: [7]  [ 70/316]  eta: 0:00:08  lr: 0.000488  min_lr: 0.000488  loss: 0.7656 (0.7506)  class_acc: 0.5000 (0.5290)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0321  data: 0.0001  max mem: 2403
Epoch: [7]  [ 80/316]  eta: 0:00:08  lr: 0.000488  min_lr: 0.000488  loss: 0.7637 (0.7513)  class_acc: 0.5000 (0.5247)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0322  data: 0.0001  max mem: 2403
Epoch: [7]  [ 90/316]  eta: 0:00:07  lr: 0.000488  min_lr: 0.000488  loss: 0.7523 (0.7479)  class_acc: 0.5000 (0.5309)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0320  data: 0.0001  max mem: 2403
Epoch: [7]  [100/316]  eta: 0:00:07  lr: 0.000483  min_lr: 0.000483  loss: 0.7474 (0.7465)  class_acc: 0.5000 (0.5328)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 59.0809 (59.0809)  time: 0.0322  data: 0.0001  max mem: 2403
Epoch: [7]  [110/316]  eta: 0:00:07  lr: 0.000483  min_lr: 0.000483  loss: 0.7474 (0.7408)  class_acc: 0.5625 (0.5400)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 59.0809 (59.0809)  time: 0.0322  data: 0.0001  max mem: 2403
Epoch: [7]  [120/316]  eta: 0:00:06  lr: 0.000483  min_lr: 0.000483  loss: 0.6472 (0.7303)  class_acc: 0.6250 (0.5517)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 59.0809 (59.0809)  time: 0.0320  data: 0.0001  max mem: 2403
Epoch: [7]  [130/316]  eta: 0:00:06  lr: 0.000483  min_lr: 0.000483  loss: 0.6571 (0.7292)  class_acc: 0.6250 (0.5539)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 59.0809 (59.0809)  time: 0.0323  data: 0.0001  max mem: 2403
Epoch: [7]  [140/316]  eta: 0:00:05  lr: 0.000483  min_lr: 0.000483  loss: 0.6443 (0.7224)  class_acc: 0.6250 (0.5607)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 59.0809 (59.0809)  time: 0.0325  data: 0.0001  max mem: 2403
Epoch: [7]  [150/316]  eta: 0:00:05  lr: 0.000483  min_lr: 0.000483  loss: 0.6275 (0.7182)  class_acc: 0.6250 (0.5666)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 59.0809 (59.0809)  time: 0.0322  data: 0.0001  max mem: 2403
Epoch: [7]  [160/316]  eta: 0:00:05  lr: 0.000483  min_lr: 0.000483  loss: 0.6347 (0.7141)  class_acc: 0.6250 (0.5707)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 59.0809 (59.0809)  time: 0.0321  data: 0.0001  max mem: 2403
Epoch: [7]  [170/316]  eta: 0:00:04  lr: 0.000483  min_lr: 0.000483  loss: 0.6633 (0.7101)  class_acc: 0.6250 (0.5731)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 59.0809 (59.0809)  time: 0.0318  data: 0.0001  max mem: 2403
Epoch: [7]  [180/316]  eta: 0:00:04  lr: 0.000483  min_lr: 0.000483  loss: 0.6672 (0.7083)  class_acc: 0.6250 (0.5739)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 59.0809 (59.0809)  time: 0.0317  data: 0.0001  max mem: 2403
Epoch: [7]  [190/316]  eta: 0:00:04  lr: 0.000483  min_lr: 0.000483  loss: 0.6370 (0.7041)  class_acc: 0.5625 (0.5776)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 59.0809 (59.0809)  time: 0.0318  data: 0.0001  max mem: 2403
Epoch: [7]  [200/316]  eta: 0:00:03  lr: 0.000478  min_lr: 0.000478  loss: 0.6430 (0.7051)  class_acc: 0.5625 (0.5771)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6561 (32.3685)  time: 0.0321  data: 0.0001  max mem: 2403
Epoch: [7]  [210/316]  eta: 0:00:03  lr: 0.000478  min_lr: 0.000478  loss: 0.6792 (0.7057)  class_acc: 0.5625 (0.5785)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6561 (32.3685)  time: 0.0319  data: 0.0001  max mem: 2403
Epoch: [7]  [220/316]  eta: 0:00:03  lr: 0.000478  min_lr: 0.000478  loss: 0.7226 (0.7073)  class_acc: 0.6250 (0.5800)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6561 (32.3685)  time: 0.0315  data: 0.0001  max mem: 2403
Epoch: [7]  [230/316]  eta: 0:00:02  lr: 0.000478  min_lr: 0.000478  loss: 0.7226 (0.7100)  class_acc: 0.6250 (0.5801)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6561 (32.3685)  time: 0.0315  data: 0.0001  max mem: 2403
Epoch: [7]  [240/316]  eta: 0:00:02  lr: 0.000478  min_lr: 0.000478  loss: 0.7180 (0.7104)  class_acc: 0.6250 (0.5827)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6561 (32.3685)  time: 0.0306  data: 0.0001  max mem: 2403
Epoch: [7]  [250/316]  eta: 0:00:02  lr: 0.000478  min_lr: 0.000478  loss: 0.6529 (0.7125)  class_acc: 0.6875 (0.5844)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6561 (32.3685)  time: 0.0299  data: 0.0001  max mem: 2403
Epoch: [7]  [260/316]  eta: 0:00:01  lr: 0.000478  min_lr: 0.000478  loss: 0.6880 (0.7118)  class_acc: 0.6250 (0.5869)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6561 (32.3685)  time: 0.0297  data: 0.0001  max mem: 2403
Epoch: [7]  [270/316]  eta: 0:00:01  lr: 0.000478  min_lr: 0.000478  loss: 0.6987 (0.7133)  class_acc: 0.6250 (0.5876)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6561 (32.3685)  time: 0.0298  data: 0.0001  max mem: 2403
Epoch: [7]  [280/316]  eta: 0:00:01  lr: 0.000478  min_lr: 0.000478  loss: 0.7393 (0.7137)  class_acc: 0.5625 (0.5879)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6561 (32.3685)  time: 0.0298  data: 0.0001  max mem: 2403
Epoch: [7]  [290/316]  eta: 0:00:00  lr: 0.000478  min_lr: 0.000478  loss: 0.7393 (0.7143)  class_acc: 0.5625 (0.5874)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6561 (32.3685)  time: 0.0299  data: 0.0001  max mem: 2403
Epoch: [7]  [300/316]  eta: 0:00:00  lr: 0.000478  min_lr: 0.000478  loss: 0.6939 (0.7150)  class_acc: 0.5625 (0.5883)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 47.0461 (37.2610)  time: 0.0286  data: 0.0001  max mem: 2403
Epoch: [7]  [310/316]  eta: 0:00:00  lr: 0.000478  min_lr: 0.000478  loss: 0.6939 (0.7150)  class_acc: 0.5625 (0.5883)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 47.0461 (37.2610)  time: 0.0137  data: 0.0001  max mem: 2403
Epoch: [7]  [315/316]  eta: 0:00:00  lr: 0.000478  min_lr: 0.000478  loss: 0.6939 (0.7150)  class_acc: 0.5625 (0.5883)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 47.0461 (37.2610)  time: 0.0062  data: 0.0000  max mem: 2403
Epoch: [7] Total time: 0:00:09 (0.0313 s / it)
Averaged stats: lr: 0.000478  min_lr: 0.000478  loss: 0.6939 (0.7150)  class_acc: 0.5625 (0.5883)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 47.0461 (37.2610)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:14  loss: 0.2931 (0.2931)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2461  data: 0.2069  max mem: 2403
Val:  [10/60]  eta: 0:00:01  loss: 0.7082 (0.5914)  accuracy: 0.5417 (0.3864)  balanced_accuracy: 0.5000 (0.3212)  pr_auc: 0.4831 (0.3827)  roc_auc: 0.4074 (0.3118)  time: 0.0388  data: 0.0190  max mem: 2403
Val:  [20/60]  eta: 0:00:01  loss: 0.6101 (0.6162)  accuracy: 0.6250 (0.5456)  balanced_accuracy: 0.5000 (0.4123)  pr_auc: 0.6184 (0.5561)  roc_auc: 0.4259 (0.3927)  time: 0.0163  data: 0.0002  max mem: 2403
Val:  [30/60]  eta: 0:00:00  loss: 0.6621 (0.6494)  accuracy: 0.6667 (0.5739)  balanced_accuracy: 0.5000 (0.4406)  pr_auc: 0.7412 (0.6118)  roc_auc: 0.5000 (0.4501)  time: 0.0147  data: 0.0002  max mem: 2403
Val:  [40/60]  eta: 0:00:00  loss: 0.7695 (0.6945)  accuracy: 0.6250 (0.5752)  balanced_accuracy: 0.5000 (0.4536)  pr_auc: 0.6954 (0.6166)  roc_auc: 0.5037 (0.4551)  time: 0.0146  data: 0.0002  max mem: 2403
Val:  [50/60]  eta: 0:00:00  loss: 0.8684 (0.7520)  accuracy: 0.5417 (0.5637)  balanced_accuracy: 0.5000 (0.4619)  pr_auc: 0.5500 (0.5922)  roc_auc: 0.4286 (0.4379)  time: 0.0141  data: 0.0001  max mem: 2403
Val:  [59/60]  eta: 0:00:00  loss: 1.0959 (0.8428)  accuracy: 0.5000 (0.5171)  balanced_accuracy: 0.5000 (0.4454)  pr_auc: 0.4022 (0.5341)  roc_auc: 0.2500 (0.3960)  time: 0.0134  data: 0.0001  max mem: 2403
Val: Total time: 0:00:01 (0.0207 s / it)
* loss 0.843
Accuracy of the network on the 1431 test EEG: 0.58%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [8]  [  0/316]  eta: 0:02:16  lr: 0.000473  min_lr: 0.000473  loss: 0.6003 (0.6003)  class_acc: 0.6875 (0.6875)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4327  data: 0.3679  max mem: 2403
Epoch: [8]  [ 10/316]  eta: 0:00:21  lr: 0.000473  min_lr: 0.000473  loss: 0.6308 (0.6670)  class_acc: 0.6875 (0.6705)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0700  data: 0.0337  max mem: 2403
Epoch: [8]  [ 20/316]  eta: 0:00:15  lr: 0.000473  min_lr: 0.000473  loss: 0.6567 (0.6644)  class_acc: 0.6875 (0.6845)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0333  data: 0.0002  max mem: 2403
Epoch: [8]  [ 30/316]  eta: 0:00:13  lr: 0.000473  min_lr: 0.000473  loss: 0.6530 (0.6627)  class_acc: 0.6875 (0.6794)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [8]  [ 40/316]  eta: 0:00:11  lr: 0.000473  min_lr: 0.000473  loss: 0.6546 (0.6771)  class_acc: 0.6250 (0.6646)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0327  data: 0.0001  max mem: 2403
Epoch: [8]  [ 50/316]  eta: 0:00:10  lr: 0.000473  min_lr: 0.000473  loss: 0.6742 (0.6790)  class_acc: 0.6250 (0.6667)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [8]  [ 60/316]  eta: 0:00:10  lr: 0.000473  min_lr: 0.000473  loss: 0.6566 (0.6942)  class_acc: 0.6875 (0.6588)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0330  data: 0.0001  max mem: 2403
Epoch: [8]  [ 70/316]  eta: 0:00:09  lr: 0.000473  min_lr: 0.000473  loss: 0.7179 (0.7025)  class_acc: 0.6250 (0.6532)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [8]  [ 80/316]  eta: 0:00:08  lr: 0.000473  min_lr: 0.000473  loss: 0.7179 (0.7035)  class_acc: 0.6250 (0.6512)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0325  data: 0.0001  max mem: 2403
Epoch: [8]  [ 90/316]  eta: 0:00:08  lr: 0.000473  min_lr: 0.000473  loss: 0.7893 (0.7227)  class_acc: 0.6250 (0.6394)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0326  data: 0.0001  max mem: 2403
Epoch: [8]  [100/316]  eta: 0:00:07  lr: 0.000467  min_lr: 0.000467  loss: 0.6782 (0.7072)  class_acc: 0.6250 (0.6467)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 41.0845 (41.0845)  time: 0.0330  data: 0.0001  max mem: 2403
Epoch: [8]  [110/316]  eta: 0:00:07  lr: 0.000467  min_lr: 0.000467  loss: 0.6327 (0.7018)  class_acc: 0.6875 (0.6475)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 41.0845 (41.0845)  time: 0.0333  data: 0.0001  max mem: 2403
Epoch: [8]  [120/316]  eta: 0:00:07  lr: 0.000467  min_lr: 0.000467  loss: 0.6441 (0.6962)  class_acc: 0.6250 (0.6493)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 41.0845 (41.0845)  time: 0.0331  data: 0.0001  max mem: 2403
Epoch: [8]  [130/316]  eta: 0:00:06  lr: 0.000467  min_lr: 0.000467  loss: 0.6366 (0.6922)  class_acc: 0.6250 (0.6474)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 41.0845 (41.0845)  time: 0.0331  data: 0.0001  max mem: 2403
Epoch: [8]  [140/316]  eta: 0:00:06  lr: 0.000467  min_lr: 0.000467  loss: 0.6262 (0.6858)  class_acc: 0.6250 (0.6503)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 41.0845 (41.0845)  time: 0.0331  data: 0.0001  max mem: 2403
Epoch: [8]  [150/316]  eta: 0:00:05  lr: 0.000467  min_lr: 0.000467  loss: 0.5971 (0.6800)  class_acc: 0.6250 (0.6519)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 41.0845 (41.0845)  time: 0.0330  data: 0.0001  max mem: 2403
Epoch: [8]  [160/316]  eta: 0:00:05  lr: 0.000467  min_lr: 0.000467  loss: 0.5978 (0.6781)  class_acc: 0.6250 (0.6506)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 41.0845 (41.0845)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [8]  [170/316]  eta: 0:00:05  lr: 0.000467  min_lr: 0.000467  loss: 0.6509 (0.6745)  class_acc: 0.6250 (0.6520)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 41.0845 (41.0845)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [8]  [180/316]  eta: 0:00:04  lr: 0.000467  min_lr: 0.000467  loss: 0.6031 (0.6740)  class_acc: 0.6250 (0.6502)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 41.0845 (41.0845)  time: 0.0331  data: 0.0001  max mem: 2403
Epoch: [8]  [190/316]  eta: 0:00:04  lr: 0.000467  min_lr: 0.000467  loss: 0.6197 (0.6710)  class_acc: 0.6250 (0.6515)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 41.0845 (41.0845)  time: 0.0332  data: 0.0001  max mem: 2403
Epoch: [8]  [200/316]  eta: 0:00:04  lr: 0.000460  min_lr: 0.000460  loss: 0.6587 (0.6730)  class_acc: 0.6250 (0.6477)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4999 (24.7922)  time: 0.0333  data: 0.0001  max mem: 2403
Epoch: [8]  [210/316]  eta: 0:00:03  lr: 0.000460  min_lr: 0.000460  loss: 0.6756 (0.6730)  class_acc: 0.5625 (0.6451)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4999 (24.7922)  time: 0.0333  data: 0.0001  max mem: 2403
Epoch: [8]  [220/316]  eta: 0:00:03  lr: 0.000460  min_lr: 0.000460  loss: 0.7297 (0.6766)  class_acc: 0.5625 (0.6394)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4999 (24.7922)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [8]  [230/316]  eta: 0:00:02  lr: 0.000460  min_lr: 0.000460  loss: 0.7306 (0.6764)  class_acc: 0.5625 (0.6388)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4999 (24.7922)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [8]  [240/316]  eta: 0:00:02  lr: 0.000460  min_lr: 0.000460  loss: 0.6502 (0.6760)  class_acc: 0.6250 (0.6393)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4999 (24.7922)  time: 0.0328  data: 0.0002  max mem: 2403
Epoch: [8]  [250/316]  eta: 0:00:02  lr: 0.000460  min_lr: 0.000460  loss: 0.7182 (0.6773)  class_acc: 0.6250 (0.6355)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4999 (24.7922)  time: 0.0322  data: 0.0001  max mem: 2403
Epoch: [8]  [260/316]  eta: 0:00:01  lr: 0.000460  min_lr: 0.000460  loss: 0.7161 (0.6783)  class_acc: 0.5625 (0.6327)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4999 (24.7922)  time: 0.0322  data: 0.0001  max mem: 2403
Epoch: [8]  [270/316]  eta: 0:00:01  lr: 0.000460  min_lr: 0.000460  loss: 0.6596 (0.6785)  class_acc: 0.5625 (0.6310)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4999 (24.7922)  time: 0.0325  data: 0.0001  max mem: 2403
Epoch: [8]  [280/316]  eta: 0:00:01  lr: 0.000460  min_lr: 0.000460  loss: 0.6508 (0.6778)  class_acc: 0.5625 (0.6292)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4999 (24.7922)  time: 0.0324  data: 0.0001  max mem: 2403
Epoch: [8]  [290/316]  eta: 0:00:00  lr: 0.000460  min_lr: 0.000460  loss: 0.6508 (0.6796)  class_acc: 0.5625 (0.6259)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4999 (24.7922)  time: 0.0324  data: 0.0001  max mem: 2403
Epoch: [8]  [300/316]  eta: 0:00:00  lr: 0.000460  min_lr: 0.000460  loss: 0.6646 (0.6797)  class_acc: 0.5625 (0.6244)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 27.8316 (25.8053)  time: 0.0303  data: 0.0001  max mem: 2403
Epoch: [8]  [310/316]  eta: 0:00:00  lr: 0.000460  min_lr: 0.000460  loss: 0.6646 (0.6797)  class_acc: 0.5625 (0.6244)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 27.8316 (25.8053)  time: 0.0141  data: 0.0001  max mem: 2403
Epoch: [8]  [315/316]  eta: 0:00:00  lr: 0.000460  min_lr: 0.000460  loss: 0.6646 (0.6797)  class_acc: 0.5625 (0.6244)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 27.8316 (25.8053)  time: 0.0062  data: 0.0000  max mem: 2403
Epoch: [8] Total time: 0:00:10 (0.0329 s / it)
Averaged stats: lr: 0.000460  min_lr: 0.000460  loss: 0.6646 (0.6797)  class_acc: 0.5625 (0.6244)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 27.8316 (25.8053)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:13  loss: 0.8304 (0.8304)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2317  data: 0.2153  max mem: 2403
Val:  [10/60]  eta: 0:00:01  loss: 0.7104 (0.7247)  accuracy: 0.3750 (0.3144)  balanced_accuracy: 0.3667 (0.3197)  pr_auc: 0.4890 (0.3798)  roc_auc: 0.3778 (0.2991)  time: 0.0381  data: 0.0205  max mem: 2403
Val:  [20/60]  eta: 0:00:01  loss: 0.7212 (0.7206)  accuracy: 0.4583 (0.4048)  balanced_accuracy: 0.4722 (0.4003)  pr_auc: 0.6137 (0.5611)  roc_auc: 0.4571 (0.3979)  time: 0.0166  data: 0.0006  max mem: 2403
Val:  [30/60]  eta: 0:00:00  loss: 0.6992 (0.7109)  accuracy: 0.5417 (0.4503)  balanced_accuracy: 0.5105 (0.4456)  pr_auc: 0.7674 (0.6146)  roc_auc: 0.5078 (0.4500)  time: 0.0143  data: 0.0001  max mem: 2403
Val:  [40/60]  eta: 0:00:00  loss: 0.7074 (0.7224)  accuracy: 0.5000 (0.4533)  balanced_accuracy: 0.4895 (0.4485)  pr_auc: 0.6534 (0.6156)  roc_auc: 0.5037 (0.4475)  time: 0.0142  data: 0.0001  max mem: 2403
Val:  [50/60]  eta: 0:00:00  loss: 0.7728 (0.7436)  accuracy: 0.3750 (0.4314)  balanced_accuracy: 0.3750 (0.4277)  pr_auc: 0.5223 (0.5846)  roc_auc: 0.3852 (0.4192)  time: 0.0140  data: 0.0001  max mem: 2403
Val:  [59/60]  eta: 0:00:00  loss: 0.8130 (0.7491)  accuracy: 0.3333 (0.4074)  balanced_accuracy: 0.3000 (0.3993)  pr_auc: 0.3842 (0.5287)  roc_auc: 0.2500 (0.3847)  time: 0.0134  data: 0.0001  max mem: 2403
Val: Total time: 0:00:01 (0.0208 s / it)
* loss 0.749
Accuracy of the network on the 1431 test EEG: 0.47%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [9]  [  0/316]  eta: 0:01:17  lr: 0.000452  min_lr: 0.000452  loss: 0.6442 (0.6442)  class_acc: 0.6875 (0.6875)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2466  data: 0.1912  max mem: 2403
Epoch: [9]  [ 10/316]  eta: 0:00:16  lr: 0.000452  min_lr: 0.000452  loss: 0.6501 (0.7164)  class_acc: 0.5625 (0.5341)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0531  data: 0.0179  max mem: 2403
Epoch: [9]  [ 20/316]  eta: 0:00:12  lr: 0.000452  min_lr: 0.000452  loss: 0.6622 (0.7104)  class_acc: 0.5625 (0.5476)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0337  data: 0.0003  max mem: 2403
Epoch: [9]  [ 30/316]  eta: 0:00:11  lr: 0.000452  min_lr: 0.000452  loss: 0.6984 (0.7149)  class_acc: 0.5625 (0.5524)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0340  data: 0.0001  max mem: 2403
Epoch: [9]  [ 40/316]  eta: 0:00:10  lr: 0.000452  min_lr: 0.000452  loss: 0.7296 (0.7100)  class_acc: 0.5625 (0.5595)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0339  data: 0.0001  max mem: 2403
Epoch: [9]  [ 50/316]  eta: 0:00:10  lr: 0.000452  min_lr: 0.000452  loss: 0.6790 (0.7033)  class_acc: 0.5625 (0.5588)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0336  data: 0.0001  max mem: 2403
Epoch: [9]  [ 60/316]  eta: 0:00:09  lr: 0.000452  min_lr: 0.000452  loss: 0.6418 (0.6985)  class_acc: 0.5625 (0.5584)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0337  data: 0.0001  max mem: 2403
Epoch: [9]  [ 70/316]  eta: 0:00:09  lr: 0.000452  min_lr: 0.000452  loss: 0.6465 (0.7001)  class_acc: 0.5625 (0.5599)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0337  data: 0.0001  max mem: 2403
Epoch: [9]  [ 80/316]  eta: 0:00:08  lr: 0.000452  min_lr: 0.000452  loss: 0.6805 (0.6991)  class_acc: 0.5625 (0.5617)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0335  data: 0.0001  max mem: 2403
Epoch: [9]  [ 90/316]  eta: 0:00:08  lr: 0.000452  min_lr: 0.000452  loss: 0.6630 (0.6956)  class_acc: 0.5625 (0.5646)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0335  data: 0.0001  max mem: 2403
Epoch: [9]  [100/316]  eta: 0:00:07  lr: 0.000444  min_lr: 0.000444  loss: 0.6587 (0.6941)  class_acc: 0.5625 (0.5705)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 44.5450 (44.5450)  time: 0.0338  data: 0.0001  max mem: 2403
Epoch: [9]  [110/316]  eta: 0:00:07  lr: 0.000444  min_lr: 0.000444  loss: 0.6587 (0.6898)  class_acc: 0.5625 (0.5726)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 44.5450 (44.5450)  time: 0.0340  data: 0.0001  max mem: 2403
Epoch: [9]  [120/316]  eta: 0:00:06  lr: 0.000444  min_lr: 0.000444  loss: 0.5663 (0.6778)  class_acc: 0.6875 (0.5852)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 44.5450 (44.5450)  time: 0.0337  data: 0.0001  max mem: 2403
Epoch: [9]  [130/316]  eta: 0:00:06  lr: 0.000444  min_lr: 0.000444  loss: 0.5883 (0.6805)  class_acc: 0.6250 (0.5816)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 44.5450 (44.5450)  time: 0.0333  data: 0.0001  max mem: 2403
Epoch: [9]  [140/316]  eta: 0:00:06  lr: 0.000444  min_lr: 0.000444  loss: 0.6372 (0.6793)  class_acc: 0.5625 (0.5833)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 44.5450 (44.5450)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [9]  [150/316]  eta: 0:00:05  lr: 0.000444  min_lr: 0.000444  loss: 0.6624 (0.6810)  class_acc: 0.6250 (0.5853)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 44.5450 (44.5450)  time: 0.0336  data: 0.0001  max mem: 2403
Epoch: [9]  [160/316]  eta: 0:00:05  lr: 0.000444  min_lr: 0.000444  loss: 0.6560 (0.6755)  class_acc: 0.6250 (0.5897)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 44.5450 (44.5450)  time: 0.0332  data: 0.0001  max mem: 2403
Epoch: [9]  [170/316]  eta: 0:00:05  lr: 0.000444  min_lr: 0.000444  loss: 0.6042 (0.6723)  class_acc: 0.6875 (0.5961)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 44.5450 (44.5450)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [9]  [180/316]  eta: 0:00:04  lr: 0.000444  min_lr: 0.000444  loss: 0.6292 (0.6719)  class_acc: 0.6250 (0.5960)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 44.5450 (44.5450)  time: 0.0330  data: 0.0001  max mem: 2403
Epoch: [9]  [190/316]  eta: 0:00:04  lr: 0.000444  min_lr: 0.000444  loss: 0.6445 (0.6699)  class_acc: 0.5625 (0.5959)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 44.5450 (44.5450)  time: 0.0331  data: 0.0001  max mem: 2403
Epoch: [9]  [200/316]  eta: 0:00:04  lr: 0.000436  min_lr: 0.000436  loss: 0.6586 (0.6728)  class_acc: 0.5625 (0.5936)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 20.1072 (32.3261)  time: 0.0338  data: 0.0001  max mem: 2403
Epoch: [9]  [210/316]  eta: 0:00:03  lr: 0.000436  min_lr: 0.000436  loss: 0.6463 (0.6699)  class_acc: 0.5625 (0.5972)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 20.1072 (32.3261)  time: 0.0340  data: 0.0001  max mem: 2403
Epoch: [9]  [220/316]  eta: 0:00:03  lr: 0.000436  min_lr: 0.000436  loss: 0.5761 (0.6678)  class_acc: 0.6875 (0.6007)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 20.1072 (32.3261)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [9]  [230/316]  eta: 0:00:02  lr: 0.000436  min_lr: 0.000436  loss: 0.6668 (0.6696)  class_acc: 0.6250 (0.6006)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 20.1072 (32.3261)  time: 0.0324  data: 0.0001  max mem: 2403
Epoch: [9]  [240/316]  eta: 0:00:02  lr: 0.000436  min_lr: 0.000436  loss: 0.6482 (0.6666)  class_acc: 0.6250 (0.6050)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 20.1072 (32.3261)  time: 0.0325  data: 0.0001  max mem: 2403
Epoch: [9]  [250/316]  eta: 0:00:02  lr: 0.000436  min_lr: 0.000436  loss: 0.6578 (0.6703)  class_acc: 0.6250 (0.6046)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 20.1072 (32.3261)  time: 0.0324  data: 0.0001  max mem: 2403
Epoch: [9]  [260/316]  eta: 0:00:01  lr: 0.000436  min_lr: 0.000436  loss: 0.6578 (0.6687)  class_acc: 0.6250 (0.6063)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 20.1072 (32.3261)  time: 0.0326  data: 0.0001  max mem: 2403
Epoch: [9]  [270/316]  eta: 0:00:01  lr: 0.000436  min_lr: 0.000436  loss: 0.6194 (0.6682)  class_acc: 0.6875 (0.6079)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 20.1072 (32.3261)  time: 0.0326  data: 0.0001  max mem: 2403
Epoch: [9]  [280/316]  eta: 0:00:01  lr: 0.000436  min_lr: 0.000436  loss: 0.5945 (0.6657)  class_acc: 0.6875 (0.6112)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 20.1072 (32.3261)  time: 0.0324  data: 0.0001  max mem: 2403
Epoch: [9]  [290/316]  eta: 0:00:00  lr: 0.000436  min_lr: 0.000436  loss: 0.6181 (0.6659)  class_acc: 0.6875 (0.6119)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 20.1072 (32.3261)  time: 0.0327  data: 0.0001  max mem: 2403
Epoch: [9]  [300/316]  eta: 0:00:00  lr: 0.000436  min_lr: 0.000436  loss: 0.6593 (0.6662)  class_acc: 0.6250 (0.6121)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 22.7647 (29.1390)  time: 0.0307  data: 0.0001  max mem: 2403
Epoch: [9]  [310/316]  eta: 0:00:00  lr: 0.000436  min_lr: 0.000436  loss: 0.6593 (0.6662)  class_acc: 0.6250 (0.6121)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 22.7647 (29.1390)  time: 0.0141  data: 0.0001  max mem: 2403
Epoch: [9]  [315/316]  eta: 0:00:00  lr: 0.000436  min_lr: 0.000436  loss: 0.6593 (0.6662)  class_acc: 0.6250 (0.6121)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 22.7647 (29.1390)  time: 0.0062  data: 0.0000  max mem: 2403
Epoch: [9] Total time: 0:00:10 (0.0327 s / it)
Averaged stats: lr: 0.000436  min_lr: 0.000436  loss: 0.6593 (0.6662)  class_acc: 0.6250 (0.6121)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 22.7647 (29.1390)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:16  loss: 0.3565 (0.3565)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2730  data: 0.2329  max mem: 2403
Val:  [10/60]  eta: 0:00:02  loss: 0.6858 (0.6050)  accuracy: 0.5417 (0.3712)  balanced_accuracy: 0.4643 (0.3087)  pr_auc: 0.4695 (0.3759)  roc_auc: 0.4148 (0.2969)  time: 0.0402  data: 0.0229  max mem: 2403
Val:  [20/60]  eta: 0:00:01  loss: 0.6311 (0.6286)  accuracy: 0.6250 (0.5317)  balanced_accuracy: 0.5000 (0.4025)  pr_auc: 0.6066 (0.5398)  roc_auc: 0.4214 (0.3685)  time: 0.0157  data: 0.0010  max mem: 2403
Val:  [30/60]  eta: 0:00:00  loss: 0.6513 (0.6546)  accuracy: 0.6667 (0.5685)  balanced_accuracy: 0.5000 (0.4396)  pr_auc: 0.7114 (0.5975)  roc_auc: 0.4815 (0.4245)  time: 0.0149  data: 0.0002  max mem: 2403
Val:  [40/60]  eta: 0:00:00  loss: 0.7563 (0.6909)  accuracy: 0.6250 (0.5691)  balanced_accuracy: 0.5000 (0.4513)  pr_auc: 0.6647 (0.6088)  roc_auc: 0.5156 (0.4380)  time: 0.0154  data: 0.0002  max mem: 2403
Val:  [50/60]  eta: 0:00:00  loss: 0.8464 (0.7395)  accuracy: 0.5417 (0.5588)  balanced_accuracy: 0.5000 (0.4600)  pr_auc: 0.5687 (0.5901)  roc_auc: 0.4333 (0.4307)  time: 0.0146  data: 0.0001  max mem: 2403
Val:  [59/60]  eta: 0:00:00  loss: 1.0309 (0.8145)  accuracy: 0.5000 (0.5094)  balanced_accuracy: 0.5000 (0.4389)  pr_auc: 0.4150 (0.5345)  roc_auc: 0.2917 (0.3900)  time: 0.0135  data: 0.0001  max mem: 2403
Val: Total time: 0:00:01 (0.0214 s / it)
* loss 0.814
Accuracy of the network on the 1431 test EEG: 0.58%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [10]  [  0/316]  eta: 0:01:22  lr: 0.000427  min_lr: 0.000427  loss: 0.6116 (0.6116)  class_acc: 0.6875 (0.6875)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2617  data: 0.2171  max mem: 2403
Epoch: [10]  [ 10/316]  eta: 0:00:15  lr: 0.000427  min_lr: 0.000427  loss: 0.6116 (0.6595)  class_acc: 0.6875 (0.6534)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0516  data: 0.0204  max mem: 2403
Epoch: [10]  [ 20/316]  eta: 0:00:12  lr: 0.000427  min_lr: 0.000427  loss: 0.5663 (0.6216)  class_acc: 0.6875 (0.6875)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0305  data: 0.0004  max mem: 2403
Epoch: [10]  [ 30/316]  eta: 0:00:10  lr: 0.000427  min_lr: 0.000427  loss: 0.5763 (0.6080)  class_acc: 0.7500 (0.6976)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0303  data: 0.0001  max mem: 2403
Epoch: [10]  [ 40/316]  eta: 0:00:09  lr: 0.000427  min_lr: 0.000427  loss: 0.6194 (0.6266)  class_acc: 0.6875 (0.6875)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0304  data: 0.0001  max mem: 2403
Epoch: [10]  [ 50/316]  eta: 0:00:09  lr: 0.000427  min_lr: 0.000427  loss: 0.6194 (0.6165)  class_acc: 0.6875 (0.6887)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0306  data: 0.0001  max mem: 2403
Epoch: [10]  [ 60/316]  eta: 0:00:08  lr: 0.000427  min_lr: 0.000427  loss: 0.6029 (0.6309)  class_acc: 0.6875 (0.6803)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0306  data: 0.0001  max mem: 2403
Epoch: [10]  [ 70/316]  eta: 0:00:08  lr: 0.000427  min_lr: 0.000427  loss: 0.6650 (0.6473)  class_acc: 0.6250 (0.6681)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0304  data: 0.0001  max mem: 2403
Epoch: [10]  [ 80/316]  eta: 0:00:07  lr: 0.000427  min_lr: 0.000427  loss: 0.6605 (0.6459)  class_acc: 0.6250 (0.6651)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0303  data: 0.0001  max mem: 2403
Epoch: [10]  [ 90/316]  eta: 0:00:07  lr: 0.000427  min_lr: 0.000427  loss: 0.6155 (0.6544)  class_acc: 0.6250 (0.6593)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0305  data: 0.0001  max mem: 2403
Epoch: [10]  [100/316]  eta: 0:00:07  lr: 0.000417  min_lr: 0.000417  loss: 0.5849 (0.6438)  class_acc: 0.6875 (0.6658)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.6095 (30.6095)  time: 0.0305  data: 0.0001  max mem: 2403
Epoch: [10]  [110/316]  eta: 0:00:06  lr: 0.000417  min_lr: 0.000417  loss: 0.5706 (0.6397)  class_acc: 0.6875 (0.6672)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.6095 (30.6095)  time: 0.0305  data: 0.0001  max mem: 2403
Epoch: [10]  [120/316]  eta: 0:00:06  lr: 0.000417  min_lr: 0.000417  loss: 0.5976 (0.6356)  class_acc: 0.6250 (0.6710)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.6095 (30.6095)  time: 0.0302  data: 0.0001  max mem: 2403
Epoch: [10]  [130/316]  eta: 0:00:05  lr: 0.000417  min_lr: 0.000417  loss: 0.6170 (0.6377)  class_acc: 0.6875 (0.6684)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.6095 (30.6095)  time: 0.0301  data: 0.0001  max mem: 2403
Epoch: [10]  [140/316]  eta: 0:00:05  lr: 0.000417  min_lr: 0.000417  loss: 0.6481 (0.6417)  class_acc: 0.6875 (0.6693)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.6095 (30.6095)  time: 0.0301  data: 0.0001  max mem: 2403
Epoch: [10]  [150/316]  eta: 0:00:05  lr: 0.000417  min_lr: 0.000417  loss: 0.6234 (0.6390)  class_acc: 0.6875 (0.6697)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.6095 (30.6095)  time: 0.0299  data: 0.0001  max mem: 2403
Epoch: [10]  [160/316]  eta: 0:00:04  lr: 0.000417  min_lr: 0.000417  loss: 0.6132 (0.6364)  class_acc: 0.6250 (0.6693)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.6095 (30.6095)  time: 0.0298  data: 0.0001  max mem: 2403
Epoch: [10]  [170/316]  eta: 0:00:04  lr: 0.000417  min_lr: 0.000417  loss: 0.5687 (0.6318)  class_acc: 0.6875 (0.6732)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.6095 (30.6095)  time: 0.0300  data: 0.0001  max mem: 2403
Epoch: [10]  [180/316]  eta: 0:00:04  lr: 0.000417  min_lr: 0.000417  loss: 0.6139 (0.6350)  class_acc: 0.6875 (0.6702)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.6095 (30.6095)  time: 0.0302  data: 0.0001  max mem: 2403
Epoch: [10]  [190/316]  eta: 0:00:03  lr: 0.000417  min_lr: 0.000417  loss: 0.6320 (0.6353)  class_acc: 0.6250 (0.6675)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.6095 (30.6095)  time: 0.0300  data: 0.0001  max mem: 2403
Epoch: [10]  [200/316]  eta: 0:00:03  lr: 0.000408  min_lr: 0.000408  loss: 0.6401 (0.6375)  class_acc: 0.6250 (0.6642)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 23.0381 (26.8238)  time: 0.0301  data: 0.0001  max mem: 2403
Epoch: [10]  [210/316]  eta: 0:00:03  lr: 0.000408  min_lr: 0.000408  loss: 0.6538 (0.6378)  class_acc: 0.5625 (0.6617)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 23.0381 (26.8238)  time: 0.0300  data: 0.0001  max mem: 2403
Epoch: [10]  [220/316]  eta: 0:00:03  lr: 0.000408  min_lr: 0.000408  loss: 0.6398 (0.6386)  class_acc: 0.6250 (0.6618)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 23.0381 (26.8238)  time: 0.0300  data: 0.0001  max mem: 2403
Epoch: [10]  [230/316]  eta: 0:00:02  lr: 0.000408  min_lr: 0.000408  loss: 0.6249 (0.6395)  class_acc: 0.6250 (0.6602)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 23.0381 (26.8238)  time: 0.0304  data: 0.0001  max mem: 2403
Epoch: [10]  [240/316]  eta: 0:00:02  lr: 0.000408  min_lr: 0.000408  loss: 0.6180 (0.6385)  class_acc: 0.6875 (0.6598)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 23.0381 (26.8238)  time: 0.0303  data: 0.0001  max mem: 2403
Epoch: [10]  [250/316]  eta: 0:00:02  lr: 0.000408  min_lr: 0.000408  loss: 0.6180 (0.6392)  class_acc: 0.6250 (0.6581)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 23.0381 (26.8238)  time: 0.0303  data: 0.0001  max mem: 2403
Epoch: [10]  [260/316]  eta: 0:00:01  lr: 0.000408  min_lr: 0.000408  loss: 0.6293 (0.6395)  class_acc: 0.6250 (0.6568)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 23.0381 (26.8238)  time: 0.0305  data: 0.0001  max mem: 2403
Epoch: [10]  [270/316]  eta: 0:00:01  lr: 0.000408  min_lr: 0.000408  loss: 0.5962 (0.6364)  class_acc: 0.6875 (0.6594)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 23.0381 (26.8238)  time: 0.0304  data: 0.0001  max mem: 2403
Epoch: [10]  [280/316]  eta: 0:00:01  lr: 0.000408  min_lr: 0.000408  loss: 0.5593 (0.6340)  class_acc: 0.7500 (0.6615)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 23.0381 (26.8238)  time: 0.0304  data: 0.0001  max mem: 2403
Epoch: [10]  [290/316]  eta: 0:00:00  lr: 0.000408  min_lr: 0.000408  loss: 0.5705 (0.6345)  class_acc: 0.6875 (0.6600)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 23.0381 (26.8238)  time: 0.0302  data: 0.0001  max mem: 2403
Epoch: [10]  [300/316]  eta: 0:00:00  lr: 0.000408  min_lr: 0.000408  loss: 0.6289 (0.6342)  class_acc: 0.6250 (0.6602)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 23.0381 (19.5639)  time: 0.0286  data: 0.0001  max mem: 2403
Epoch: [10]  [310/316]  eta: 0:00:00  lr: 0.000408  min_lr: 0.000408  loss: 0.6289 (0.6342)  class_acc: 0.6250 (0.6602)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 23.0381 (19.5639)  time: 0.0137  data: 0.0001  max mem: 2403
Epoch: [10]  [315/316]  eta: 0:00:00  lr: 0.000408  min_lr: 0.000408  loss: 0.6289 (0.6342)  class_acc: 0.6250 (0.6602)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 23.0381 (19.5639)  time: 0.0061  data: 0.0000  max mem: 2403
Epoch: [10] Total time: 0:00:09 (0.0300 s / it)
Averaged stats: lr: 0.000408  min_lr: 0.000408  loss: 0.6289 (0.6342)  class_acc: 0.6250 (0.6602)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 23.0381 (19.5639)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:14  loss: 0.8214 (0.8214)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2477  data: 0.2298  max mem: 2403
Val:  [10/60]  eta: 0:00:01  loss: 0.7676 (0.7521)  accuracy: 0.3750 (0.2992)  balanced_accuracy: 0.3811 (0.2901)  pr_auc: 0.4603 (0.3767)  roc_auc: 0.3778 (0.2793)  time: 0.0397  data: 0.0235  max mem: 2403
Val:  [20/60]  eta: 0:00:01  loss: 0.7672 (0.7583)  accuracy: 0.4167 (0.3730)  balanced_accuracy: 0.4375 (0.3621)  pr_auc: 0.6038 (0.5317)  roc_auc: 0.4286 (0.3452)  time: 0.0171  data: 0.0015  max mem: 2403
Val:  [30/60]  eta: 0:00:00  loss: 0.7370 (0.7448)  accuracy: 0.5000 (0.4140)  balanced_accuracy: 0.4889 (0.4022)  pr_auc: 0.6737 (0.5836)  roc_auc: 0.4593 (0.3978)  time: 0.0154  data: 0.0002  max mem: 2403
Val:  [40/60]  eta: 0:00:00  loss: 0.7349 (0.7464)  accuracy: 0.5000 (0.4350)  balanced_accuracy: 0.4889 (0.4243)  pr_auc: 0.6448 (0.5972)  roc_auc: 0.4688 (0.4156)  time: 0.0152  data: 0.0002  max mem: 2403
Val:  [50/60]  eta: 0:00:00  loss: 0.7611 (0.7561)  accuracy: 0.4167 (0.4306)  balanced_accuracy: 0.4167 (0.4214)  pr_auc: 0.5903 (0.5827)  roc_auc: 0.3857 (0.4152)  time: 0.0145  data: 0.0001  max mem: 2403
Val:  [59/60]  eta: 0:00:00  loss: 0.7910 (0.7578)  accuracy: 0.3750 (0.4046)  balanced_accuracy: 0.3438 (0.3912)  pr_auc: 0.4524 (0.5293)  roc_auc: 0.3056 (0.3845)  time: 0.0135  data: 0.0001  max mem: 2403
Val: Total time: 0:00:01 (0.0212 s / it)
* loss 0.758
Accuracy of the network on the 1431 test EEG: 0.47%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [11]  [  0/316]  eta: 0:01:22  lr: 0.000397  min_lr: 0.000397  loss: 0.4081 (0.4081)  class_acc: 0.8750 (0.8750)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2612  data: 0.2052  max mem: 2403
Epoch: [11]  [ 10/316]  eta: 0:00:15  lr: 0.000397  min_lr: 0.000397  loss: 0.7098 (0.6451)  class_acc: 0.6250 (0.6250)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0522  data: 0.0190  max mem: 2403
Epoch: [11]  [ 20/316]  eta: 0:00:12  lr: 0.000397  min_lr: 0.000397  loss: 0.6440 (0.6500)  class_acc: 0.6250 (0.6190)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0309  data: 0.0003  max mem: 2403
Epoch: [11]  [ 30/316]  eta: 0:00:10  lr: 0.000397  min_lr: 0.000397  loss: 0.6270 (0.6368)  class_acc: 0.6250 (0.6270)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0306  data: 0.0001  max mem: 2403
Epoch: [11]  [ 40/316]  eta: 0:00:10  lr: 0.000397  min_lr: 0.000397  loss: 0.5728 (0.6290)  class_acc: 0.6250 (0.6326)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0306  data: 0.0001  max mem: 2403
Epoch: [11]  [ 50/316]  eta: 0:00:09  lr: 0.000397  min_lr: 0.000397  loss: 0.6476 (0.6362)  class_acc: 0.6250 (0.6238)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0306  data: 0.0001  max mem: 2403
Epoch: [11]  [ 60/316]  eta: 0:00:08  lr: 0.000397  min_lr: 0.000397  loss: 0.6476 (0.6373)  class_acc: 0.6250 (0.6199)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0308  data: 0.0001  max mem: 2403
Epoch: [11]  [ 70/316]  eta: 0:00:08  lr: 0.000397  min_lr: 0.000397  loss: 0.6258 (0.6360)  class_acc: 0.6250 (0.6188)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0309  data: 0.0001  max mem: 2403
Epoch: [11]  [ 80/316]  eta: 0:00:07  lr: 0.000397  min_lr: 0.000397  loss: 0.6252 (0.6362)  class_acc: 0.6250 (0.6219)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0310  data: 0.0001  max mem: 2403
Epoch: [11]  [ 90/316]  eta: 0:00:07  lr: 0.000397  min_lr: 0.000397  loss: 0.6544 (0.6372)  class_acc: 0.6250 (0.6216)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0313  data: 0.0001  max mem: 2403
Epoch: [11]  [100/316]  eta: 0:00:07  lr: 0.000386  min_lr: 0.000386  loss: 0.6539 (0.6391)  class_acc: 0.6250 (0.6219)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (29.3874)  time: 0.0316  data: 0.0001  max mem: 2403
Epoch: [11]  [110/316]  eta: 0:00:06  lr: 0.000386  min_lr: 0.000386  loss: 0.6207 (0.6446)  class_acc: 0.6250 (0.6182)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (29.3874)  time: 0.0321  data: 0.0001  max mem: 2403
Epoch: [11]  [120/316]  eta: 0:00:06  lr: 0.000386  min_lr: 0.000386  loss: 0.6344 (0.6450)  class_acc: 0.5625 (0.6173)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (29.3874)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [11]  [130/316]  eta: 0:00:06  lr: 0.000386  min_lr: 0.000386  loss: 0.6220 (0.6459)  class_acc: 0.6250 (0.6217)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (29.3874)  time: 0.0332  data: 0.0001  max mem: 2403
Epoch: [11]  [140/316]  eta: 0:00:05  lr: 0.000386  min_lr: 0.000386  loss: 0.6220 (0.6439)  class_acc: 0.6250 (0.6201)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (29.3874)  time: 0.0332  data: 0.0001  max mem: 2403
Epoch: [11]  [150/316]  eta: 0:00:05  lr: 0.000386  min_lr: 0.000386  loss: 0.6740 (0.6464)  class_acc: 0.5625 (0.6217)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (29.3874)  time: 0.0331  data: 0.0001  max mem: 2403
Epoch: [11]  [160/316]  eta: 0:00:05  lr: 0.000386  min_lr: 0.000386  loss: 0.6219 (0.6452)  class_acc: 0.5625 (0.6242)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (29.3874)  time: 0.0325  data: 0.0001  max mem: 2403
Epoch: [11]  [170/316]  eta: 0:00:04  lr: 0.000386  min_lr: 0.000386  loss: 0.6207 (0.6442)  class_acc: 0.6250 (0.6246)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (29.3874)  time: 0.0319  data: 0.0001  max mem: 2403
Epoch: [11]  [180/316]  eta: 0:00:04  lr: 0.000386  min_lr: 0.000386  loss: 0.6316 (0.6443)  class_acc: 0.6250 (0.6236)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (29.3874)  time: 0.0322  data: 0.0001  max mem: 2403
Epoch: [11]  [190/316]  eta: 0:00:04  lr: 0.000386  min_lr: 0.000386  loss: 0.6216 (0.6431)  class_acc: 0.6250 (0.6266)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (29.3874)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [11]  [200/316]  eta: 0:00:03  lr: 0.000375  min_lr: 0.000375  loss: 0.5855 (0.6411)  class_acc: 0.6875 (0.6303)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (29.4316)  time: 0.0331  data: 0.0001  max mem: 2403
Epoch: [11]  [210/316]  eta: 0:00:03  lr: 0.000375  min_lr: 0.000375  loss: 0.5600 (0.6383)  class_acc: 0.6875 (0.6336)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (29.4316)  time: 0.0332  data: 0.0001  max mem: 2403
Epoch: [11]  [220/316]  eta: 0:00:03  lr: 0.000375  min_lr: 0.000375  loss: 0.6021 (0.6376)  class_acc: 0.6875 (0.6377)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (29.4316)  time: 0.0332  data: 0.0001  max mem: 2403
Epoch: [11]  [230/316]  eta: 0:00:02  lr: 0.000375  min_lr: 0.000375  loss: 0.6475 (0.6395)  class_acc: 0.6875 (0.6358)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (29.4316)  time: 0.0330  data: 0.0001  max mem: 2403
Epoch: [11]  [240/316]  eta: 0:00:02  lr: 0.000375  min_lr: 0.000375  loss: 0.6154 (0.6388)  class_acc: 0.6875 (0.6369)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (29.4316)  time: 0.0326  data: 0.0001  max mem: 2403
Epoch: [11]  [250/316]  eta: 0:00:02  lr: 0.000375  min_lr: 0.000375  loss: 0.6154 (0.6383)  class_acc: 0.6875 (0.6375)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (29.4316)  time: 0.0324  data: 0.0001  max mem: 2403
Epoch: [11]  [260/316]  eta: 0:00:01  lr: 0.000375  min_lr: 0.000375  loss: 0.6380 (0.6388)  class_acc: 0.6250 (0.6363)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (29.4316)  time: 0.0326  data: 0.0001  max mem: 2403
Epoch: [11]  [270/316]  eta: 0:00:01  lr: 0.000375  min_lr: 0.000375  loss: 0.6302 (0.6374)  class_acc: 0.6250 (0.6377)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (29.4316)  time: 0.0326  data: 0.0001  max mem: 2403
Epoch: [11]  [280/316]  eta: 0:00:01  lr: 0.000375  min_lr: 0.000375  loss: 0.5414 (0.6331)  class_acc: 0.6875 (0.6421)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (29.4316)  time: 0.0322  data: 0.0001  max mem: 2403
Epoch: [11]  [290/316]  eta: 0:00:00  lr: 0.000375  min_lr: 0.000375  loss: 0.5515 (0.6328)  class_acc: 0.6875 (0.6413)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (29.4316)  time: 0.0320  data: 0.0001  max mem: 2403
Epoch: [11]  [300/316]  eta: 0:00:00  lr: 0.000375  min_lr: 0.000375  loss: 0.5959 (0.6316)  class_acc: 0.6250 (0.6429)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (21.2133)  time: 0.0302  data: 0.0001  max mem: 2403
Epoch: [11]  [310/316]  eta: 0:00:00  lr: 0.000375  min_lr: 0.000375  loss: 0.5959 (0.6316)  class_acc: 0.6250 (0.6429)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (21.2133)  time: 0.0142  data: 0.0001  max mem: 2403
Epoch: [11]  [315/316]  eta: 0:00:00  lr: 0.000375  min_lr: 0.000375  loss: 0.5959 (0.6316)  class_acc: 0.6250 (0.6429)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (21.2133)  time: 0.0062  data: 0.0000  max mem: 2403
Epoch: [11] Total time: 0:00:10 (0.0317 s / it)
Averaged stats: lr: 0.000375  min_lr: 0.000375  loss: 0.5959 (0.6316)  class_acc: 0.6250 (0.6429)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3874 (21.2133)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:18  loss: 0.4481 (0.4481)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.3079  data: 0.2946  max mem: 2403
Val:  [10/60]  eta: 0:00:02  loss: 0.6779 (0.6367)  accuracy: 0.4167 (0.3561)  balanced_accuracy: 0.4286 (0.3026)  pr_auc: 0.4542 (0.3702)  roc_auc: 0.3852 (0.2713)  time: 0.0413  data: 0.0270  max mem: 2403
Val:  [20/60]  eta: 0:00:01  loss: 0.6779 (0.6580)  accuracy: 0.5417 (0.5000)  balanced_accuracy: 0.4722 (0.3879)  pr_auc: 0.5939 (0.5280)  roc_auc: 0.4056 (0.3386)  time: 0.0146  data: 0.0002  max mem: 2403
Val:  [30/60]  eta: 0:00:00  loss: 0.6996 (0.6750)  accuracy: 0.6667 (0.5484)  balanced_accuracy: 0.5000 (0.4359)  pr_auc: 0.6814 (0.5787)  roc_auc: 0.4537 (0.3915)  time: 0.0146  data: 0.0002  max mem: 2403
Val:  [40/60]  eta: 0:00:00  loss: 0.7501 (0.6997)  accuracy: 0.5833 (0.5478)  balanced_accuracy: 0.5000 (0.4440)  pr_auc: 0.6502 (0.5953)  roc_auc: 0.4609 (0.4147)  time: 0.0146  data: 0.0002  max mem: 2403
Val:  [50/60]  eta: 0:00:00  loss: 0.8449 (0.7394)  accuracy: 0.5000 (0.5376)  balanced_accuracy: 0.4667 (0.4506)  pr_auc: 0.5961 (0.5818)  roc_auc: 0.3986 (0.4151)  time: 0.0143  data: 0.0001  max mem: 2403
Val:  [59/60]  eta: 0:00:00  loss: 0.9635 (0.7964)  accuracy: 0.3750 (0.4871)  balanced_accuracy: 0.4286 (0.4241)  pr_auc: 0.4503 (0.5270)  roc_auc: 0.2951 (0.3752)  time: 0.0134  data: 0.0001  max mem: 2403
Val: Total time: 0:00:01 (0.0212 s / it)
* loss 0.796
Accuracy of the network on the 1431 test EEG: 0.56%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [12]  [  0/316]  eta: 0:01:23  lr: 0.000364  min_lr: 0.000364  loss: 0.4400 (0.4400)  class_acc: 0.6875 (0.6875)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2631  data: 0.1990  max mem: 2403
Epoch: [12]  [ 10/316]  eta: 0:00:16  lr: 0.000364  min_lr: 0.000364  loss: 0.5395 (0.5275)  class_acc: 0.6875 (0.6989)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0546  data: 0.0182  max mem: 2403
Epoch: [12]  [ 20/316]  eta: 0:00:13  lr: 0.000364  min_lr: 0.000364  loss: 0.5514 (0.5265)  class_acc: 0.6875 (0.7173)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0337  data: 0.0001  max mem: 2403
Epoch: [12]  [ 30/316]  eta: 0:00:11  lr: 0.000364  min_lr: 0.000364  loss: 0.5428 (0.5292)  class_acc: 0.7500 (0.7077)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0335  data: 0.0001  max mem: 2403
Epoch: [12]  [ 40/316]  eta: 0:00:10  lr: 0.000364  min_lr: 0.000364  loss: 0.5395 (0.5367)  class_acc: 0.6875 (0.7012)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0335  data: 0.0001  max mem: 2403
Epoch: [12]  [ 50/316]  eta: 0:00:10  lr: 0.000364  min_lr: 0.000364  loss: 0.5635 (0.5409)  class_acc: 0.6875 (0.7034)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0336  data: 0.0001  max mem: 2403
Epoch: [12]  [ 60/316]  eta: 0:00:09  lr: 0.000364  min_lr: 0.000364  loss: 0.6312 (0.5588)  class_acc: 0.6875 (0.6957)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0337  data: 0.0001  max mem: 2403
Epoch: [12]  [ 70/316]  eta: 0:00:09  lr: 0.000364  min_lr: 0.000364  loss: 0.6886 (0.5755)  class_acc: 0.6250 (0.6866)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0337  data: 0.0001  max mem: 2403
Epoch: [12]  [ 80/316]  eta: 0:00:08  lr: 0.000364  min_lr: 0.000364  loss: 0.6025 (0.5761)  class_acc: 0.6875 (0.6852)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0336  data: 0.0001  max mem: 2403
Epoch: [12]  [ 90/316]  eta: 0:00:08  lr: 0.000364  min_lr: 0.000364  loss: 0.5925 (0.5883)  class_acc: 0.6250 (0.6786)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0335  data: 0.0001  max mem: 2403
Epoch: [12]  [100/316]  eta: 0:00:07  lr: 0.000352  min_lr: 0.000352  loss: 0.5694 (0.5865)  class_acc: 0.6875 (0.6807)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (19.0829)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [12]  [110/316]  eta: 0:00:07  lr: 0.000352  min_lr: 0.000352  loss: 0.5563 (0.5893)  class_acc: 0.6875 (0.6768)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (19.0829)  time: 0.0331  data: 0.0001  max mem: 2403
Epoch: [12]  [120/316]  eta: 0:00:06  lr: 0.000352  min_lr: 0.000352  loss: 0.5846 (0.5868)  class_acc: 0.6875 (0.6792)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (19.0829)  time: 0.0332  data: 0.0001  max mem: 2403
Epoch: [12]  [130/316]  eta: 0:00:06  lr: 0.000352  min_lr: 0.000352  loss: 0.6377 (0.5961)  class_acc: 0.6250 (0.6751)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (19.0829)  time: 0.0338  data: 0.0001  max mem: 2403
Epoch: [12]  [140/316]  eta: 0:00:06  lr: 0.000352  min_lr: 0.000352  loss: 0.6073 (0.5927)  class_acc: 0.6875 (0.6791)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (19.0829)  time: 0.0338  data: 0.0001  max mem: 2403
Epoch: [12]  [150/316]  eta: 0:00:05  lr: 0.000352  min_lr: 0.000352  loss: 0.5020 (0.5867)  class_acc: 0.7500 (0.6842)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (19.0829)  time: 0.0335  data: 0.0001  max mem: 2403
Epoch: [12]  [160/316]  eta: 0:00:05  lr: 0.000352  min_lr: 0.000352  loss: 0.5082 (0.5866)  class_acc: 0.7500 (0.6840)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (19.0829)  time: 0.0330  data: 0.0001  max mem: 2403
Epoch: [12]  [170/316]  eta: 0:00:05  lr: 0.000352  min_lr: 0.000352  loss: 0.5599 (0.5832)  class_acc: 0.6875 (0.6864)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (19.0829)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [12]  [180/316]  eta: 0:00:04  lr: 0.000352  min_lr: 0.000352  loss: 0.6013 (0.5895)  class_acc: 0.6250 (0.6809)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (19.0829)  time: 0.0333  data: 0.0001  max mem: 2403
Epoch: [12]  [190/316]  eta: 0:00:04  lr: 0.000352  min_lr: 0.000352  loss: 0.6013 (0.5883)  class_acc: 0.6250 (0.6810)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (19.0829)  time: 0.0333  data: 0.0001  max mem: 2403
Epoch: [12]  [200/316]  eta: 0:00:04  lr: 0.000340  min_lr: 0.000340  loss: 0.6012 (0.5914)  class_acc: 0.6875 (0.6791)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (20.2471)  time: 0.0335  data: 0.0001  max mem: 2403
Epoch: [12]  [210/316]  eta: 0:00:03  lr: 0.000340  min_lr: 0.000340  loss: 0.5873 (0.5890)  class_acc: 0.6875 (0.6807)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (20.2471)  time: 0.0336  data: 0.0001  max mem: 2403
Epoch: [12]  [220/316]  eta: 0:00:03  lr: 0.000340  min_lr: 0.000340  loss: 0.5194 (0.5884)  class_acc: 0.6875 (0.6821)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (20.2471)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [12]  [230/316]  eta: 0:00:02  lr: 0.000340  min_lr: 0.000340  loss: 0.5742 (0.5899)  class_acc: 0.6875 (0.6810)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (20.2471)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [12]  [240/316]  eta: 0:00:02  lr: 0.000340  min_lr: 0.000340  loss: 0.5742 (0.5896)  class_acc: 0.6250 (0.6818)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (20.2471)  time: 0.0335  data: 0.0001  max mem: 2403
Epoch: [12]  [250/316]  eta: 0:00:02  lr: 0.000340  min_lr: 0.000340  loss: 0.6183 (0.5910)  class_acc: 0.6250 (0.6818)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (20.2471)  time: 0.0335  data: 0.0001  max mem: 2403
Epoch: [12]  [260/316]  eta: 0:00:01  lr: 0.000340  min_lr: 0.000340  loss: 0.5674 (0.5897)  class_acc: 0.7500 (0.6832)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (20.2471)  time: 0.0335  data: 0.0001  max mem: 2403
Epoch: [12]  [270/316]  eta: 0:00:01  lr: 0.000340  min_lr: 0.000340  loss: 0.5460 (0.5888)  class_acc: 0.7500 (0.6831)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (20.2471)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [12]  [280/316]  eta: 0:00:01  lr: 0.000340  min_lr: 0.000340  loss: 0.5504 (0.5875)  class_acc: 0.7500 (0.6842)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (20.2471)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [12]  [290/316]  eta: 0:00:00  lr: 0.000340  min_lr: 0.000340  loss: 0.5646 (0.5877)  class_acc: 0.6875 (0.6838)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (20.2471)  time: 0.0335  data: 0.0001  max mem: 2403
Epoch: [12]  [300/316]  eta: 0:00:00  lr: 0.000340  min_lr: 0.000340  loss: 0.5963 (0.5870)  class_acc: 0.6875 (0.6848)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (17.6131)  time: 0.0311  data: 0.0001  max mem: 2403
Epoch: [12]  [310/316]  eta: 0:00:00  lr: 0.000340  min_lr: 0.000340  loss: 0.5963 (0.5870)  class_acc: 0.6875 (0.6848)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (17.6131)  time: 0.0144  data: 0.0001  max mem: 2403
Epoch: [12]  [315/316]  eta: 0:00:00  lr: 0.000340  min_lr: 0.000340  loss: 0.5963 (0.5870)  class_acc: 0.6875 (0.6848)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (17.6131)  time: 0.0062  data: 0.0000  max mem: 2403
Epoch: [12] Total time: 0:00:10 (0.0329 s / it)
Averaged stats: lr: 0.000340  min_lr: 0.000340  loss: 0.5963 (0.5870)  class_acc: 0.6875 (0.6848)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0829 (17.6131)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:15  loss: 0.6491 (0.6491)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2574  data: 0.2435  max mem: 2403
Val:  [10/60]  eta: 0:00:02  loss: 0.6909 (0.7023)  accuracy: 0.3333 (0.3220)  balanced_accuracy: 0.3438 (0.2967)  pr_auc: 0.4542 (0.3649)  roc_auc: 0.3481 (0.2571)  time: 0.0423  data: 0.0282  max mem: 2403
Val:  [20/60]  eta: 0:00:01  loss: 0.6909 (0.7139)  accuracy: 0.5000 (0.4385)  balanced_accuracy: 0.4214 (0.3704)  pr_auc: 0.5769 (0.5290)  roc_auc: 0.3714 (0.3361)  time: 0.0179  data: 0.0034  max mem: 2403
Val:  [30/60]  eta: 0:00:00  loss: 0.7038 (0.7137)  accuracy: 0.5417 (0.4879)  balanced_accuracy: 0.4722 (0.4195)  pr_auc: 0.6838 (0.5756)  roc_auc: 0.4375 (0.3821)  time: 0.0150  data: 0.0002  max mem: 2403
Val:  [40/60]  eta: 0:00:00  loss: 0.7357 (0.7240)  accuracy: 0.5417 (0.5000)  balanced_accuracy: 0.5000 (0.4373)  pr_auc: 0.6357 (0.5906)  roc_auc: 0.4375 (0.4080)  time: 0.0150  data: 0.0002  max mem: 2403
Val:  [50/60]  eta: 0:00:00  loss: 0.8057 (0.7520)  accuracy: 0.4583 (0.4877)  balanced_accuracy: 0.4444 (0.4348)  pr_auc: 0.5781 (0.5745)  roc_auc: 0.4071 (0.4014)  time: 0.0145  data: 0.0001  max mem: 2403
Val:  [59/60]  eta: 0:00:00  loss: 0.8808 (0.7782)  accuracy: 0.3750 (0.4458)  balanced_accuracy: 0.3556 (0.3987)  pr_auc: 0.4334 (0.5211)  roc_auc: 0.2593 (0.3671)  time: 0.0135  data: 0.0001  max mem: 2403
Val: Total time: 0:00:01 (0.0217 s / it)
* loss 0.778
Accuracy of the network on the 1431 test EEG: 0.51%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [13]  [  0/316]  eta: 0:01:21  lr: 0.000328  min_lr: 0.000328  loss: 0.4638 (0.4638)  class_acc: 0.7500 (0.7500)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2572  data: 0.2032  max mem: 2403
Epoch: [13]  [ 10/316]  eta: 0:00:16  lr: 0.000328  min_lr: 0.000328  loss: 0.5492 (0.5468)  class_acc: 0.7500 (0.7216)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0532  data: 0.0186  max mem: 2403
Epoch: [13]  [ 20/316]  eta: 0:00:12  lr: 0.000328  min_lr: 0.000328  loss: 0.5657 (0.5624)  class_acc: 0.6875 (0.7024)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0326  data: 0.0001  max mem: 2403
Epoch: [13]  [ 30/316]  eta: 0:00:11  lr: 0.000328  min_lr: 0.000328  loss: 0.5516 (0.5501)  class_acc: 0.6875 (0.7117)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0325  data: 0.0001  max mem: 2403
Epoch: [13]  [ 40/316]  eta: 0:00:10  lr: 0.000328  min_lr: 0.000328  loss: 0.5538 (0.5632)  class_acc: 0.6875 (0.6951)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [13]  [ 50/316]  eta: 0:00:09  lr: 0.000328  min_lr: 0.000328  loss: 0.5227 (0.5539)  class_acc: 0.6875 (0.7047)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [13]  [ 60/316]  eta: 0:00:09  lr: 0.000328  min_lr: 0.000328  loss: 0.5234 (0.5682)  class_acc: 0.6875 (0.6916)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [13]  [ 70/316]  eta: 0:00:08  lr: 0.000328  min_lr: 0.000328  loss: 0.6536 (0.5791)  class_acc: 0.6250 (0.6831)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0330  data: 0.0001  max mem: 2403
Epoch: [13]  [ 80/316]  eta: 0:00:08  lr: 0.000328  min_lr: 0.000328  loss: 0.5742 (0.5795)  class_acc: 0.6250 (0.6806)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0330  data: 0.0001  max mem: 2403
Epoch: [13]  [ 90/316]  eta: 0:00:07  lr: 0.000328  min_lr: 0.000328  loss: 0.5177 (0.5789)  class_acc: 0.6875 (0.6793)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [13]  [100/316]  eta: 0:00:07  lr: 0.000315  min_lr: 0.000315  loss: 0.5177 (0.5744)  class_acc: 0.6875 (0.6819)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8730 (10.8730)  time: 0.0327  data: 0.0001  max mem: 2403
Epoch: [13]  [110/316]  eta: 0:00:07  lr: 0.000315  min_lr: 0.000315  loss: 0.5602 (0.5760)  class_acc: 0.6875 (0.6824)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8730 (10.8730)  time: 0.0327  data: 0.0001  max mem: 2403
Epoch: [13]  [120/316]  eta: 0:00:06  lr: 0.000315  min_lr: 0.000315  loss: 0.5439 (0.5714)  class_acc: 0.6875 (0.6865)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8730 (10.8730)  time: 0.0325  data: 0.0001  max mem: 2403
Epoch: [13]  [130/316]  eta: 0:00:06  lr: 0.000315  min_lr: 0.000315  loss: 0.5421 (0.5718)  class_acc: 0.6875 (0.6823)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8730 (10.8730)  time: 0.0325  data: 0.0001  max mem: 2403
Epoch: [13]  [140/316]  eta: 0:00:06  lr: 0.000315  min_lr: 0.000315  loss: 0.5533 (0.5704)  class_acc: 0.6250 (0.6817)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8730 (10.8730)  time: 0.0324  data: 0.0001  max mem: 2403
Epoch: [13]  [150/316]  eta: 0:00:05  lr: 0.000315  min_lr: 0.000315  loss: 0.5767 (0.5740)  class_acc: 0.6875 (0.6800)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8730 (10.8730)  time: 0.0319  data: 0.0001  max mem: 2403
Epoch: [13]  [160/316]  eta: 0:00:05  lr: 0.000315  min_lr: 0.000315  loss: 0.5767 (0.5733)  class_acc: 0.6875 (0.6828)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8730 (10.8730)  time: 0.0318  data: 0.0001  max mem: 2403
Epoch: [13]  [170/316]  eta: 0:00:04  lr: 0.000315  min_lr: 0.000315  loss: 0.5485 (0.5738)  class_acc: 0.6875 (0.6813)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8730 (10.8730)  time: 0.0321  data: 0.0001  max mem: 2403
Epoch: [13]  [180/316]  eta: 0:00:04  lr: 0.000315  min_lr: 0.000315  loss: 0.5160 (0.5728)  class_acc: 0.6250 (0.6806)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8730 (10.8730)  time: 0.0321  data: 0.0001  max mem: 2403
Epoch: [13]  [190/316]  eta: 0:00:04  lr: 0.000315  min_lr: 0.000315  loss: 0.5152 (0.5717)  class_acc: 0.6875 (0.6842)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8730 (10.8730)  time: 0.0321  data: 0.0001  max mem: 2403
Epoch: [13]  [200/316]  eta: 0:00:03  lr: 0.000302  min_lr: 0.000302  loss: 0.5638 (0.5724)  class_acc: 0.6875 (0.6853)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8730 (16.1065)  time: 0.0322  data: 0.0001  max mem: 2403
Epoch: [13]  [210/316]  eta: 0:00:03  lr: 0.000302  min_lr: 0.000302  loss: 0.5541 (0.5723)  class_acc: 0.6875 (0.6860)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8730 (16.1065)  time: 0.0324  data: 0.0001  max mem: 2403
Epoch: [13]  [220/316]  eta: 0:00:03  lr: 0.000302  min_lr: 0.000302  loss: 0.5727 (0.5742)  class_acc: 0.6875 (0.6850)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8730 (16.1065)  time: 0.0322  data: 0.0001  max mem: 2403
Epoch: [13]  [230/316]  eta: 0:00:02  lr: 0.000302  min_lr: 0.000302  loss: 0.6210 (0.5775)  class_acc: 0.6250 (0.6834)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8730 (16.1065)  time: 0.0319  data: 0.0001  max mem: 2403
Epoch: [13]  [240/316]  eta: 0:00:02  lr: 0.000302  min_lr: 0.000302  loss: 0.5980 (0.5764)  class_acc: 0.6875 (0.6859)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8730 (16.1065)  time: 0.0319  data: 0.0001  max mem: 2403
Epoch: [13]  [250/316]  eta: 0:00:02  lr: 0.000302  min_lr: 0.000302  loss: 0.5870 (0.5769)  class_acc: 0.6875 (0.6863)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8730 (16.1065)  time: 0.0319  data: 0.0001  max mem: 2403
Epoch: [13]  [260/316]  eta: 0:00:01  lr: 0.000302  min_lr: 0.000302  loss: 0.6003 (0.5773)  class_acc: 0.6875 (0.6856)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8730 (16.1065)  time: 0.0321  data: 0.0001  max mem: 2403
Epoch: [13]  [270/316]  eta: 0:00:01  lr: 0.000302  min_lr: 0.000302  loss: 0.5842 (0.5770)  class_acc: 0.6875 (0.6866)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8730 (16.1065)  time: 0.0323  data: 0.0001  max mem: 2403
Epoch: [13]  [280/316]  eta: 0:00:01  lr: 0.000302  min_lr: 0.000302  loss: 0.5319 (0.5754)  class_acc: 0.7500 (0.6882)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8730 (16.1065)  time: 0.0326  data: 0.0001  max mem: 2403
Epoch: [13]  [290/316]  eta: 0:00:00  lr: 0.000302  min_lr: 0.000302  loss: 0.5344 (0.5742)  class_acc: 0.7500 (0.6894)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8730 (16.1065)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [13]  [300/316]  eta: 0:00:00  lr: 0.000302  min_lr: 0.000302  loss: 0.5456 (0.5737)  class_acc: 0.7500 (0.6902)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9289 (14.3806)  time: 0.0305  data: 0.0001  max mem: 2403
Epoch: [13]  [310/316]  eta: 0:00:00  lr: 0.000302  min_lr: 0.000302  loss: 0.5456 (0.5737)  class_acc: 0.7500 (0.6902)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9289 (14.3806)  time: 0.0142  data: 0.0001  max mem: 2403
Epoch: [13]  [315/316]  eta: 0:00:00  lr: 0.000302  min_lr: 0.000302  loss: 0.5456 (0.5737)  class_acc: 0.7500 (0.6902)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9289 (14.3806)  time: 0.0062  data: 0.0000  max mem: 2403
Epoch: [13] Total time: 0:00:10 (0.0319 s / it)
Averaged stats: lr: 0.000302  min_lr: 0.000302  loss: 0.5456 (0.5737)  class_acc: 0.7500 (0.6902)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9289 (14.3806)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:16  loss: 0.5843 (0.5843)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2700  data: 0.2446  max mem: 2403
Val:  [10/60]  eta: 0:00:01  loss: 0.6999 (0.7018)  accuracy: 0.3333 (0.3182)  balanced_accuracy: 0.3778 (0.2785)  pr_auc: 0.4502 (0.3586)  roc_auc: 0.3185 (0.2469)  time: 0.0393  data: 0.0240  max mem: 2403
Val:  [20/60]  eta: 0:00:01  loss: 0.6999 (0.7167)  accuracy: 0.5417 (0.4524)  balanced_accuracy: 0.4571 (0.3694)  pr_auc: 0.5752 (0.5176)  roc_auc: 0.3704 (0.3208)  time: 0.0156  data: 0.0011  max mem: 2403
Val:  [30/60]  eta: 0:00:00  loss: 0.7256 (0.7222)  accuracy: 0.5833 (0.4960)  balanced_accuracy: 0.4778 (0.4122)  pr_auc: 0.6498 (0.5614)  roc_auc: 0.4357 (0.3674)  time: 0.0150  data: 0.0002  max mem: 2403
Val:  [40/60]  eta: 0:00:00  loss: 0.7451 (0.7327)  accuracy: 0.5417 (0.5020)  balanced_accuracy: 0.5000 (0.4240)  pr_auc: 0.6259 (0.5823)  roc_auc: 0.4297 (0.4011)  time: 0.0150  data: 0.0002  max mem: 2403
Val:  [50/60]  eta: 0:00:00  loss: 0.8296 (0.7644)  accuracy: 0.4583 (0.4951)  balanced_accuracy: 0.4333 (0.4296)  pr_auc: 0.5863 (0.5714)  roc_auc: 0.4222 (0.4010)  time: 0.0145  data: 0.0001  max mem: 2403
Val:  [59/60]  eta: 0:00:00  loss: 0.9075 (0.7993)  accuracy: 0.3750 (0.4507)  balanced_accuracy: 0.3750 (0.3937)  pr_auc: 0.4557 (0.5185)  roc_auc: 0.2986 (0.3665)  time: 0.0135  data: 0.0001  max mem: 2403
Val: Total time: 0:00:01 (0.0212 s / it)
* loss 0.799
Accuracy of the network on the 1431 test EEG: 0.52%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [14]  [  0/316]  eta: 0:01:23  lr: 0.000290  min_lr: 0.000290  loss: 0.3937 (0.3937)  class_acc: 0.8125 (0.8125)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2633  data: 0.2026  max mem: 2403
Epoch: [14]  [ 10/316]  eta: 0:00:16  lr: 0.000290  min_lr: 0.000290  loss: 0.4909 (0.5225)  class_acc: 0.8125 (0.7500)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0532  data: 0.0186  max mem: 2403
Epoch: [14]  [ 20/316]  eta: 0:00:12  lr: 0.000290  min_lr: 0.000290  loss: 0.5183 (0.5296)  class_acc: 0.7500 (0.7440)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0320  data: 0.0002  max mem: 2403
Epoch: [14]  [ 30/316]  eta: 0:00:11  lr: 0.000290  min_lr: 0.000290  loss: 0.5352 (0.5192)  class_acc: 0.7500 (0.7500)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0318  data: 0.0002  max mem: 2403
Epoch: [14]  [ 40/316]  eta: 0:00:10  lr: 0.000290  min_lr: 0.000290  loss: 0.5352 (0.5308)  class_acc: 0.6875 (0.7363)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0317  data: 0.0002  max mem: 2403
Epoch: [14]  [ 50/316]  eta: 0:00:09  lr: 0.000290  min_lr: 0.000290  loss: 0.5495 (0.5376)  class_acc: 0.6875 (0.7292)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0323  data: 0.0002  max mem: 2403
Epoch: [14]  [ 60/316]  eta: 0:00:09  lr: 0.000290  min_lr: 0.000290  loss: 0.5687 (0.5538)  class_acc: 0.6250 (0.7141)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0319  data: 0.0002  max mem: 2403
Epoch: [14]  [ 70/316]  eta: 0:00:08  lr: 0.000290  min_lr: 0.000290  loss: 0.5995 (0.5541)  class_acc: 0.6875 (0.7113)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0315  data: 0.0002  max mem: 2403
Epoch: [14]  [ 80/316]  eta: 0:00:08  lr: 0.000290  min_lr: 0.000290  loss: 0.5836 (0.5543)  class_acc: 0.6875 (0.7068)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0318  data: 0.0002  max mem: 2403
Epoch: [14]  [ 90/316]  eta: 0:00:07  lr: 0.000290  min_lr: 0.000290  loss: 0.5290 (0.5520)  class_acc: 0.6875 (0.7081)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0309  data: 0.0001  max mem: 2403
Epoch: [14]  [100/316]  eta: 0:00:07  lr: 0.000277  min_lr: 0.000277  loss: 0.5073 (0.5492)  class_acc: 0.7500 (0.7092)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4809 (4.4809)  time: 0.0321  data: 0.0001  max mem: 2403
Epoch: [14]  [110/316]  eta: 0:00:07  lr: 0.000277  min_lr: 0.000277  loss: 0.5692 (0.5520)  class_acc: 0.7500 (0.7066)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4809 (4.4809)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [14]  [120/316]  eta: 0:00:06  lr: 0.000277  min_lr: 0.000277  loss: 0.5528 (0.5454)  class_acc: 0.7500 (0.7144)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4809 (4.4809)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [14]  [130/316]  eta: 0:00:06  lr: 0.000277  min_lr: 0.000277  loss: 0.5403 (0.5494)  class_acc: 0.7500 (0.7104)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4809 (4.4809)  time: 0.0326  data: 0.0001  max mem: 2403
Epoch: [14]  [140/316]  eta: 0:00:05  lr: 0.000277  min_lr: 0.000277  loss: 0.5711 (0.5509)  class_acc: 0.6875 (0.7105)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4809 (4.4809)  time: 0.0326  data: 0.0001  max mem: 2403
Epoch: [14]  [150/316]  eta: 0:00:05  lr: 0.000277  min_lr: 0.000277  loss: 0.5340 (0.5494)  class_acc: 0.7500 (0.7123)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4809 (4.4809)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [14]  [160/316]  eta: 0:00:05  lr: 0.000277  min_lr: 0.000277  loss: 0.5337 (0.5473)  class_acc: 0.7500 (0.7143)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4809 (4.4809)  time: 0.0330  data: 0.0001  max mem: 2403
Epoch: [14]  [170/316]  eta: 0:00:04  lr: 0.000277  min_lr: 0.000277  loss: 0.5369 (0.5454)  class_acc: 0.6875 (0.7160)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4809 (4.4809)  time: 0.0331  data: 0.0001  max mem: 2403
Epoch: [14]  [180/316]  eta: 0:00:04  lr: 0.000277  min_lr: 0.000277  loss: 0.5760 (0.5489)  class_acc: 0.6875 (0.7124)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4809 (4.4809)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [14]  [190/316]  eta: 0:00:04  lr: 0.000277  min_lr: 0.000277  loss: 0.5768 (0.5498)  class_acc: 0.6875 (0.7117)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4809 (4.4809)  time: 0.0337  data: 0.0001  max mem: 2403
Epoch: [14]  [200/316]  eta: 0:00:03  lr: 0.000264  min_lr: 0.000264  loss: 0.5572 (0.5532)  class_acc: 0.6250 (0.7090)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4809 (9.4124)  time: 0.0335  data: 0.0001  max mem: 2403
Epoch: [14]  [210/316]  eta: 0:00:03  lr: 0.000264  min_lr: 0.000264  loss: 0.5517 (0.5532)  class_acc: 0.6875 (0.7097)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4809 (9.4124)  time: 0.0336  data: 0.0001  max mem: 2403
Epoch: [14]  [220/316]  eta: 0:00:03  lr: 0.000264  min_lr: 0.000264  loss: 0.5461 (0.5549)  class_acc: 0.7500 (0.7093)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4809 (9.4124)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [14]  [230/316]  eta: 0:00:02  lr: 0.000264  min_lr: 0.000264  loss: 0.5779 (0.5580)  class_acc: 0.6875 (0.7081)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4809 (9.4124)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [14]  [240/316]  eta: 0:00:02  lr: 0.000264  min_lr: 0.000264  loss: 0.5857 (0.5584)  class_acc: 0.6875 (0.7077)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4809 (9.4124)  time: 0.0333  data: 0.0001  max mem: 2403
Epoch: [14]  [250/316]  eta: 0:00:02  lr: 0.000264  min_lr: 0.000264  loss: 0.5417 (0.5595)  class_acc: 0.6875 (0.7082)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4809 (9.4124)  time: 0.0335  data: 0.0001  max mem: 2403
Epoch: [14]  [260/316]  eta: 0:00:01  lr: 0.000264  min_lr: 0.000264  loss: 0.5417 (0.5579)  class_acc: 0.6875 (0.7086)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4809 (9.4124)  time: 0.0331  data: 0.0001  max mem: 2403
Epoch: [14]  [270/316]  eta: 0:00:01  lr: 0.000264  min_lr: 0.000264  loss: 0.5440 (0.5582)  class_acc: 0.6875 (0.7085)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4809 (9.4124)  time: 0.0330  data: 0.0001  max mem: 2403
Epoch: [14]  [280/316]  eta: 0:00:01  lr: 0.000264  min_lr: 0.000264  loss: 0.5450 (0.5568)  class_acc: 0.6875 (0.7091)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4809 (9.4124)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [14]  [290/316]  eta: 0:00:00  lr: 0.000264  min_lr: 0.000264  loss: 0.5117 (0.5578)  class_acc: 0.6875 (0.7073)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4809 (9.4124)  time: 0.0336  data: 0.0001  max mem: 2403
Epoch: [14]  [300/316]  eta: 0:00:00  lr: 0.000264  min_lr: 0.000264  loss: 0.5505 (0.5578)  class_acc: 0.6875 (0.7083)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.3438 (11.3490)  time: 0.0316  data: 0.0001  max mem: 2403
Epoch: [14]  [310/316]  eta: 0:00:00  lr: 0.000264  min_lr: 0.000264  loss: 0.5505 (0.5578)  class_acc: 0.6875 (0.7083)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.3438 (11.3490)  time: 0.0149  data: 0.0001  max mem: 2403
Epoch: [14]  [315/316]  eta: 0:00:00  lr: 0.000264  min_lr: 0.000264  loss: 0.5505 (0.5578)  class_acc: 0.6875 (0.7083)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.3438 (11.3490)  time: 0.0062  data: 0.0000  max mem: 2403
Epoch: [14] Total time: 0:00:10 (0.0324 s / it)
Averaged stats: lr: 0.000264  min_lr: 0.000264  loss: 0.5505 (0.5578)  class_acc: 0.6875 (0.7083)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.3438 (11.3490)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:16  loss: 0.6035 (0.6035)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2684  data: 0.2277  max mem: 2403
Val:  [10/60]  eta: 0:00:02  loss: 0.7159 (0.7292)  accuracy: 0.3333 (0.3106)  balanced_accuracy: 0.3778 (0.2733)  pr_auc: 0.4443 (0.3560)  roc_auc: 0.3071 (0.2398)  time: 0.0410  data: 0.0228  max mem: 2403
Val:  [20/60]  eta: 0:00:01  loss: 0.7159 (0.7446)  accuracy: 0.5000 (0.4345)  balanced_accuracy: 0.4441 (0.3584)  pr_auc: 0.5583 (0.5141)  roc_auc: 0.3519 (0.3122)  time: 0.0165  data: 0.0012  max mem: 2403
Val:  [30/60]  eta: 0:00:00  loss: 0.7462 (0.7486)  accuracy: 0.5417 (0.4798)  balanced_accuracy: 0.4688 (0.4021)  pr_auc: 0.6455 (0.5535)  roc_auc: 0.3984 (0.3556)  time: 0.0147  data: 0.0002  max mem: 2403
Val:  [40/60]  eta: 0:00:00  loss: 0.7462 (0.7536)  accuracy: 0.5417 (0.4939)  balanced_accuracy: 0.4889 (0.4214)  pr_auc: 0.6175 (0.5764)  roc_auc: 0.4296 (0.3942)  time: 0.0147  data: 0.0002  max mem: 2403
Val:  [50/60]  eta: 0:00:00  loss: 0.8303 (0.7821)  accuracy: 0.4583 (0.4869)  balanced_accuracy: 0.4222 (0.4258)  pr_auc: 0.6175 (0.5693)  roc_auc: 0.4296 (0.3988)  time: 0.0144  data: 0.0001  max mem: 2403
Val:  [59/60]  eta: 0:00:00  loss: 0.9373 (0.8116)  accuracy: 0.3750 (0.4451)  balanced_accuracy: 0.3750 (0.3915)  pr_auc: 0.4570 (0.5180)  roc_auc: 0.3194 (0.3673)  time: 0.0135  data: 0.0001  max mem: 2403
Val: Total time: 0:00:01 (0.0213 s / it)
* loss 0.812
Accuracy of the network on the 1431 test EEG: 0.52%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [15]  [  0/316]  eta: 0:01:15  lr: 0.000251  min_lr: 0.000251  loss: 0.5435 (0.5435)  class_acc: 0.7500 (0.7500)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2376  data: 0.1870  max mem: 2403
Epoch: [15]  [ 10/316]  eta: 0:00:15  lr: 0.000251  min_lr: 0.000251  loss: 0.5410 (0.5551)  class_acc: 0.6875 (0.7273)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0512  data: 0.0171  max mem: 2403
Epoch: [15]  [ 20/316]  eta: 0:00:12  lr: 0.000251  min_lr: 0.000251  loss: 0.5132 (0.5249)  class_acc: 0.7500 (0.7440)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0323  data: 0.0001  max mem: 2403
Epoch: [15]  [ 30/316]  eta: 0:00:11  lr: 0.000251  min_lr: 0.000251  loss: 0.4993 (0.5214)  class_acc: 0.7500 (0.7359)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0320  data: 0.0001  max mem: 2403
Epoch: [15]  [ 40/316]  eta: 0:00:10  lr: 0.000251  min_lr: 0.000251  loss: 0.4766 (0.5127)  class_acc: 0.6875 (0.7470)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0324  data: 0.0001  max mem: 2403
Epoch: [15]  [ 50/316]  eta: 0:00:09  lr: 0.000251  min_lr: 0.000251  loss: 0.5015 (0.5189)  class_acc: 0.6875 (0.7402)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0322  data: 0.0001  max mem: 2403
Epoch: [15]  [ 60/316]  eta: 0:00:09  lr: 0.000251  min_lr: 0.000251  loss: 0.5556 (0.5336)  class_acc: 0.6875 (0.7346)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0318  data: 0.0001  max mem: 2403
Epoch: [15]  [ 70/316]  eta: 0:00:08  lr: 0.000251  min_lr: 0.000251  loss: 0.5354 (0.5324)  class_acc: 0.7500 (0.7315)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0319  data: 0.0001  max mem: 2403
Epoch: [15]  [ 80/316]  eta: 0:00:08  lr: 0.000251  min_lr: 0.000251  loss: 0.5222 (0.5394)  class_acc: 0.6875 (0.7245)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0320  data: 0.0001  max mem: 2403
Epoch: [15]  [ 90/316]  eta: 0:00:07  lr: 0.000251  min_lr: 0.000251  loss: 0.5747 (0.5401)  class_acc: 0.6875 (0.7225)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0320  data: 0.0001  max mem: 2403
Epoch: [15]  [100/316]  eta: 0:00:07  lr: 0.000237  min_lr: 0.000237  loss: 0.4806 (0.5367)  class_acc: 0.7500 (0.7271)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5616 (4.5616)  time: 0.0318  data: 0.0001  max mem: 2403
Epoch: [15]  [110/316]  eta: 0:00:06  lr: 0.000237  min_lr: 0.000237  loss: 0.5207 (0.5361)  class_acc: 0.7500 (0.7258)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5616 (4.5616)  time: 0.0314  data: 0.0001  max mem: 2403
Epoch: [15]  [120/316]  eta: 0:00:06  lr: 0.000237  min_lr: 0.000237  loss: 0.5004 (0.5291)  class_acc: 0.7500 (0.7335)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5616 (4.5616)  time: 0.0312  data: 0.0001  max mem: 2403
Epoch: [15]  [130/316]  eta: 0:00:06  lr: 0.000237  min_lr: 0.000237  loss: 0.5004 (0.5325)  class_acc: 0.7500 (0.7281)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5616 (4.5616)  time: 0.0315  data: 0.0001  max mem: 2403
Epoch: [15]  [140/316]  eta: 0:00:05  lr: 0.000237  min_lr: 0.000237  loss: 0.5111 (0.5321)  class_acc: 0.6875 (0.7305)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5616 (4.5616)  time: 0.0317  data: 0.0001  max mem: 2403
Epoch: [15]  [150/316]  eta: 0:00:05  lr: 0.000237  min_lr: 0.000237  loss: 0.5699 (0.5379)  class_acc: 0.6875 (0.7260)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5616 (4.5616)  time: 0.0318  data: 0.0001  max mem: 2403
Epoch: [15]  [160/316]  eta: 0:00:05  lr: 0.000237  min_lr: 0.000237  loss: 0.5583 (0.5357)  class_acc: 0.6875 (0.7275)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5616 (4.5616)  time: 0.0320  data: 0.0001  max mem: 2403
Epoch: [15]  [170/316]  eta: 0:00:04  lr: 0.000237  min_lr: 0.000237  loss: 0.4607 (0.5329)  class_acc: 0.7500 (0.7299)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5616 (4.5616)  time: 0.0321  data: 0.0001  max mem: 2403
Epoch: [15]  [180/316]  eta: 0:00:04  lr: 0.000237  min_lr: 0.000237  loss: 0.5482 (0.5348)  class_acc: 0.7500 (0.7272)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5616 (4.5616)  time: 0.0321  data: 0.0001  max mem: 2403
Epoch: [15]  [190/316]  eta: 0:00:04  lr: 0.000237  min_lr: 0.000237  loss: 0.5281 (0.5315)  class_acc: 0.6875 (0.7297)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5616 (4.5616)  time: 0.0326  data: 0.0001  max mem: 2403
Epoch: [15]  [200/316]  eta: 0:00:03  lr: 0.000224  min_lr: 0.000224  loss: 0.4785 (0.5302)  class_acc: 0.7500 (0.7298)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5616 (8.8301)  time: 0.0332  data: 0.0001  max mem: 2403
Epoch: [15]  [210/316]  eta: 0:00:03  lr: 0.000224  min_lr: 0.000224  loss: 0.4785 (0.5298)  class_acc: 0.7500 (0.7305)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5616 (8.8301)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [15]  [220/316]  eta: 0:00:03  lr: 0.000224  min_lr: 0.000224  loss: 0.5130 (0.5302)  class_acc: 0.7500 (0.7308)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5616 (8.8301)  time: 0.0336  data: 0.0001  max mem: 2403
Epoch: [15]  [230/316]  eta: 0:00:02  lr: 0.000224  min_lr: 0.000224  loss: 0.5540 (0.5307)  class_acc: 0.7500 (0.7305)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5616 (8.8301)  time: 0.0338  data: 0.0001  max mem: 2403
Epoch: [15]  [240/316]  eta: 0:00:02  lr: 0.000224  min_lr: 0.000224  loss: 0.5266 (0.5316)  class_acc: 0.6875 (0.7295)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5616 (8.8301)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [15]  [250/316]  eta: 0:00:02  lr: 0.000224  min_lr: 0.000224  loss: 0.5309 (0.5337)  class_acc: 0.6875 (0.7286)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5616 (8.8301)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [15]  [260/316]  eta: 0:00:01  lr: 0.000224  min_lr: 0.000224  loss: 0.5829 (0.5353)  class_acc: 0.6875 (0.7282)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5616 (8.8301)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [15]  [270/316]  eta: 0:00:01  lr: 0.000224  min_lr: 0.000224  loss: 0.5670 (0.5350)  class_acc: 0.7500 (0.7283)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5616 (8.8301)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [15]  [280/316]  eta: 0:00:01  lr: 0.000224  min_lr: 0.000224  loss: 0.5138 (0.5324)  class_acc: 0.7500 (0.7300)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5616 (8.8301)  time: 0.0327  data: 0.0001  max mem: 2403
Epoch: [15]  [290/316]  eta: 0:00:00  lr: 0.000224  min_lr: 0.000224  loss: 0.5079 (0.5325)  class_acc: 0.7500 (0.7309)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5616 (8.8301)  time: 0.0330  data: 0.0001  max mem: 2403
Epoch: [15]  [300/316]  eta: 0:00:00  lr: 0.000224  min_lr: 0.000224  loss: 0.5059 (0.5316)  class_acc: 0.7500 (0.7321)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4401 (9.3668)  time: 0.0313  data: 0.0001  max mem: 2403
Epoch: [15]  [310/316]  eta: 0:00:00  lr: 0.000224  min_lr: 0.000224  loss: 0.5059 (0.5316)  class_acc: 0.7500 (0.7321)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4401 (9.3668)  time: 0.0146  data: 0.0001  max mem: 2403
Epoch: [15]  [315/316]  eta: 0:00:00  lr: 0.000224  min_lr: 0.000224  loss: 0.5059 (0.5316)  class_acc: 0.7500 (0.7321)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4401 (9.3668)  time: 0.0062  data: 0.0000  max mem: 2403
Epoch: [15] Total time: 0:00:10 (0.0319 s / it)
Averaged stats: lr: 0.000224  min_lr: 0.000224  loss: 0.5059 (0.5316)  class_acc: 0.7500 (0.7321)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4401 (9.3668)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:15  loss: 0.6468 (0.6468)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2656  data: 0.2512  max mem: 2403
Val:  [10/60]  eta: 0:00:02  loss: 0.7367 (0.7591)  accuracy: 0.3333 (0.2879)  balanced_accuracy: 0.3438 (0.2677)  pr_auc: 0.4477 (0.3563)  roc_auc: 0.3037 (0.2391)  time: 0.0401  data: 0.0237  max mem: 2403
Val:  [20/60]  eta: 0:00:01  loss: 0.7405 (0.7740)  accuracy: 0.4583 (0.4087)  balanced_accuracy: 0.3778 (0.3472)  pr_auc: 0.5544 (0.5128)  roc_auc: 0.3519 (0.3100)  time: 0.0164  data: 0.0006  max mem: 2403
Val:  [30/60]  eta: 0:00:00  loss: 0.7644 (0.7737)  accuracy: 0.5000 (0.4583)  balanced_accuracy: 0.4571 (0.3931)  pr_auc: 0.6418 (0.5493)  roc_auc: 0.3852 (0.3486)  time: 0.0152  data: 0.0002  max mem: 2403
Val:  [40/60]  eta: 0:00:00  loss: 0.7644 (0.7739)  accuracy: 0.5417 (0.4807)  balanced_accuracy: 0.4714 (0.4203)  pr_auc: 0.6270 (0.5758)  roc_auc: 0.4519 (0.3912)  time: 0.0154  data: 0.0002  max mem: 2403
Val:  [50/60]  eta: 0:00:00  loss: 0.8286 (0.7995)  accuracy: 0.5000 (0.4755)  balanced_accuracy: 0.4583 (0.4246)  pr_auc: 0.6149 (0.5688)  roc_auc: 0.4826 (0.3959)  time: 0.0148  data: 0.0001  max mem: 2403
Val:  [59/60]  eta: 0:00:00  loss: 0.9293 (0.8213)  accuracy: 0.4167 (0.4389)  balanced_accuracy: 0.3750 (0.3933)  pr_auc: 0.4511 (0.5182)  roc_auc: 0.3147 (0.3662)  time: 0.0135  data: 0.0001  max mem: 2403
Val: Total time: 0:00:01 (0.0215 s / it)
* loss 0.821
Accuracy of the network on the 1431 test EEG: 0.51%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [16]  [  0/316]  eta: 0:01:24  lr: 0.000211  min_lr: 0.000211  loss: 0.5089 (0.5089)  class_acc: 0.7500 (0.7500)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2681  data: 0.2083  max mem: 2403
Epoch: [16]  [ 10/316]  eta: 0:00:16  lr: 0.000211  min_lr: 0.000211  loss: 0.4846 (0.4859)  class_acc: 0.8125 (0.8011)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0542  data: 0.0190  max mem: 2403
Epoch: [16]  [ 20/316]  eta: 0:00:13  lr: 0.000211  min_lr: 0.000211  loss: 0.4583 (0.4733)  class_acc: 0.8125 (0.7887)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0327  data: 0.0001  max mem: 2403
Epoch: [16]  [ 30/316]  eta: 0:00:11  lr: 0.000211  min_lr: 0.000211  loss: 0.4644 (0.4794)  class_acc: 0.7500 (0.7722)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0325  data: 0.0001  max mem: 2403
Epoch: [16]  [ 40/316]  eta: 0:00:10  lr: 0.000211  min_lr: 0.000211  loss: 0.4866 (0.4900)  class_acc: 0.7500 (0.7652)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0324  data: 0.0001  max mem: 2403
Epoch: [16]  [ 50/316]  eta: 0:00:09  lr: 0.000211  min_lr: 0.000211  loss: 0.5228 (0.4942)  class_acc: 0.7500 (0.7623)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0325  data: 0.0001  max mem: 2403
Epoch: [16]  [ 60/316]  eta: 0:00:09  lr: 0.000211  min_lr: 0.000211  loss: 0.5525 (0.5035)  class_acc: 0.7500 (0.7541)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0327  data: 0.0001  max mem: 2403
Epoch: [16]  [ 70/316]  eta: 0:00:08  lr: 0.000211  min_lr: 0.000211  loss: 0.4925 (0.5091)  class_acc: 0.7500 (0.7482)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0326  data: 0.0001  max mem: 2403
Epoch: [16]  [ 80/316]  eta: 0:00:08  lr: 0.000211  min_lr: 0.000211  loss: 0.5089 (0.5122)  class_acc: 0.7500 (0.7508)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [16]  [ 90/316]  eta: 0:00:07  lr: 0.000211  min_lr: 0.000211  loss: 0.5092 (0.5161)  class_acc: 0.7500 (0.7486)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0331  data: 0.0001  max mem: 2403
Epoch: [16]  [100/316]  eta: 0:00:07  lr: 0.000199  min_lr: 0.000199  loss: 0.4786 (0.5118)  class_acc: 0.7500 (0.7519)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7348 (5.7348)  time: 0.0335  data: 0.0001  max mem: 2403
Epoch: [16]  [110/316]  eta: 0:00:07  lr: 0.000199  min_lr: 0.000199  loss: 0.4936 (0.5129)  class_acc: 0.7500 (0.7517)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7348 (5.7348)  time: 0.0336  data: 0.0001  max mem: 2403
Epoch: [16]  [120/316]  eta: 0:00:06  lr: 0.000199  min_lr: 0.000199  loss: 0.4936 (0.5106)  class_acc: 0.7500 (0.7546)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7348 (5.7348)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [16]  [130/316]  eta: 0:00:06  lr: 0.000199  min_lr: 0.000199  loss: 0.5226 (0.5160)  class_acc: 0.7500 (0.7505)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7348 (5.7348)  time: 0.0335  data: 0.0001  max mem: 2403
Epoch: [16]  [140/316]  eta: 0:00:06  lr: 0.000199  min_lr: 0.000199  loss: 0.5386 (0.5164)  class_acc: 0.7500 (0.7513)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7348 (5.7348)  time: 0.0335  data: 0.0001  max mem: 2403
Epoch: [16]  [150/316]  eta: 0:00:05  lr: 0.000199  min_lr: 0.000199  loss: 0.4935 (0.5153)  class_acc: 0.7500 (0.7525)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7348 (5.7348)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [16]  [160/316]  eta: 0:00:05  lr: 0.000199  min_lr: 0.000199  loss: 0.4756 (0.5123)  class_acc: 0.7500 (0.7550)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7348 (5.7348)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [16]  [170/316]  eta: 0:00:05  lr: 0.000199  min_lr: 0.000199  loss: 0.4552 (0.5087)  class_acc: 0.8125 (0.7551)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7348 (5.7348)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [16]  [180/316]  eta: 0:00:04  lr: 0.000199  min_lr: 0.000199  loss: 0.4301 (0.5074)  class_acc: 0.7500 (0.7559)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7348 (5.7348)  time: 0.0335  data: 0.0001  max mem: 2403
Epoch: [16]  [190/316]  eta: 0:00:04  lr: 0.000199  min_lr: 0.000199  loss: 0.4659 (0.5060)  class_acc: 0.7500 (0.7559)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7348 (5.7348)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [16]  [200/316]  eta: 0:00:03  lr: 0.000186  min_lr: 0.000186  loss: 0.4613 (0.5053)  class_acc: 0.7500 (0.7562)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7348 (5.7915)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [16]  [210/316]  eta: 0:00:03  lr: 0.000186  min_lr: 0.000186  loss: 0.4613 (0.5058)  class_acc: 0.7500 (0.7559)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7348 (5.7915)  time: 0.0336  data: 0.0001  max mem: 2403
Epoch: [16]  [220/316]  eta: 0:00:03  lr: 0.000186  min_lr: 0.000186  loss: 0.4434 (0.5077)  class_acc: 0.7500 (0.7551)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7348 (5.7915)  time: 0.0335  data: 0.0001  max mem: 2403
Epoch: [16]  [230/316]  eta: 0:00:02  lr: 0.000186  min_lr: 0.000186  loss: 0.4607 (0.5080)  class_acc: 0.7500 (0.7546)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7348 (5.7915)  time: 0.0333  data: 0.0001  max mem: 2403
Epoch: [16]  [240/316]  eta: 0:00:02  lr: 0.000186  min_lr: 0.000186  loss: 0.4939 (0.5095)  class_acc: 0.7500 (0.7536)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7348 (5.7915)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [16]  [250/316]  eta: 0:00:02  lr: 0.000186  min_lr: 0.000186  loss: 0.5281 (0.5116)  class_acc: 0.6875 (0.7517)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7348 (5.7915)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [16]  [260/316]  eta: 0:00:01  lr: 0.000186  min_lr: 0.000186  loss: 0.4774 (0.5119)  class_acc: 0.7500 (0.7510)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7348 (5.7915)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [16]  [270/316]  eta: 0:00:01  lr: 0.000186  min_lr: 0.000186  loss: 0.4774 (0.5132)  class_acc: 0.7500 (0.7498)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7348 (5.7915)  time: 0.0327  data: 0.0001  max mem: 2403
Epoch: [16]  [280/316]  eta: 0:00:01  lr: 0.000186  min_lr: 0.000186  loss: 0.4465 (0.5124)  class_acc: 0.7500 (0.7496)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7348 (5.7915)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [16]  [290/316]  eta: 0:00:00  lr: 0.000186  min_lr: 0.000186  loss: 0.5109 (0.5138)  class_acc: 0.7500 (0.7489)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7348 (5.7915)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [16]  [300/316]  eta: 0:00:00  lr: 0.000186  min_lr: 0.000186  loss: 0.5109 (0.5135)  class_acc: 0.7500 (0.7481)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8483 (8.7743)  time: 0.0311  data: 0.0002  max mem: 2403
Epoch: [16]  [310/316]  eta: 0:00:00  lr: 0.000186  min_lr: 0.000186  loss: 0.5109 (0.5135)  class_acc: 0.7500 (0.7481)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8483 (8.7743)  time: 0.0145  data: 0.0001  max mem: 2403
Epoch: [16]  [315/316]  eta: 0:00:00  lr: 0.000186  min_lr: 0.000186  loss: 0.5109 (0.5135)  class_acc: 0.7500 (0.7481)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8483 (8.7743)  time: 0.0062  data: 0.0000  max mem: 2403
Epoch: [16] Total time: 0:00:10 (0.0327 s / it)
Averaged stats: lr: 0.000186  min_lr: 0.000186  loss: 0.5109 (0.5135)  class_acc: 0.7500 (0.7481)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8483 (8.7743)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:15  loss: 0.5807 (0.5807)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2607  data: 0.2477  max mem: 2403
Val:  [10/60]  eta: 0:00:01  loss: 0.7511 (0.7515)  accuracy: 0.3333 (0.2917)  balanced_accuracy: 0.3601 (0.2551)  pr_auc: 0.4520 (0.3586)  roc_auc: 0.3037 (0.2437)  time: 0.0395  data: 0.0254  max mem: 2403
Val:  [20/60]  eta: 0:00:01  loss: 0.7511 (0.7710)  accuracy: 0.5000 (0.4306)  balanced_accuracy: 0.4167 (0.3516)  pr_auc: 0.5692 (0.5113)  roc_auc: 0.3426 (0.3081)  time: 0.0160  data: 0.0016  max mem: 2403
Val:  [30/60]  eta: 0:00:00  loss: 0.7742 (0.7779)  accuracy: 0.5833 (0.4839)  balanced_accuracy: 0.4688 (0.4030)  pr_auc: 0.6397 (0.5474)  roc_auc: 0.3828 (0.3464)  time: 0.0146  data: 0.0002  max mem: 2403
Val:  [40/60]  eta: 0:00:00  loss: 0.7742 (0.7817)  accuracy: 0.5833 (0.4970)  balanced_accuracy: 0.5000 (0.4222)  pr_auc: 0.6270 (0.5739)  roc_auc: 0.4370 (0.3892)  time: 0.0145  data: 0.0002  max mem: 2403
Val:  [50/60]  eta: 0:00:00  loss: 0.8555 (0.8118)  accuracy: 0.4583 (0.4926)  balanced_accuracy: 0.4333 (0.4297)  pr_auc: 0.6088 (0.5694)  roc_auc: 0.4786 (0.3972)  time: 0.0142  data: 0.0001  max mem: 2403
Val:  [59/60]  eta: 0:00:00  loss: 0.9923 (0.8418)  accuracy: 0.4167 (0.4514)  balanced_accuracy: 0.3929 (0.3964)  pr_auc: 0.4604 (0.5188)  roc_auc: 0.3287 (0.3674)  time: 0.0134  data: 0.0001  max mem: 2403
Val: Total time: 0:00:01 (0.0209 s / it)
* loss 0.842
Accuracy of the network on the 1431 test EEG: 0.52%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [17]  [  0/316]  eta: 0:01:28  lr: 0.000173  min_lr: 0.000173  loss: 0.3105 (0.3105)  class_acc: 0.8750 (0.8750)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2793  data: 0.2189  max mem: 2403
Epoch: [17]  [ 10/316]  eta: 0:00:17  lr: 0.000173  min_lr: 0.000173  loss: 0.4952 (0.5004)  class_acc: 0.8125 (0.7614)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0556  data: 0.0200  max mem: 2403
Epoch: [17]  [ 20/316]  eta: 0:00:13  lr: 0.000173  min_lr: 0.000173  loss: 0.4952 (0.4984)  class_acc: 0.7500 (0.7440)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0333  data: 0.0001  max mem: 2403
Epoch: [17]  [ 30/316]  eta: 0:00:11  lr: 0.000173  min_lr: 0.000173  loss: 0.4372 (0.4836)  class_acc: 0.7500 (0.7500)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0330  data: 0.0001  max mem: 2403
Epoch: [17]  [ 40/316]  eta: 0:00:10  lr: 0.000173  min_lr: 0.000173  loss: 0.4791 (0.4971)  class_acc: 0.7500 (0.7424)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0331  data: 0.0001  max mem: 2403
Epoch: [17]  [ 50/316]  eta: 0:00:10  lr: 0.000173  min_lr: 0.000173  loss: 0.5058 (0.4921)  class_acc: 0.7500 (0.7512)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0335  data: 0.0001  max mem: 2403
Epoch: [17]  [ 60/316]  eta: 0:00:09  lr: 0.000173  min_lr: 0.000173  loss: 0.5058 (0.5108)  class_acc: 0.7500 (0.7408)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0338  data: 0.0001  max mem: 2403
Epoch: [17]  [ 70/316]  eta: 0:00:09  lr: 0.000173  min_lr: 0.000173  loss: 0.5315 (0.5114)  class_acc: 0.7500 (0.7456)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0340  data: 0.0001  max mem: 2403
Epoch: [17]  [ 80/316]  eta: 0:00:08  lr: 0.000173  min_lr: 0.000173  loss: 0.5230 (0.5145)  class_acc: 0.7500 (0.7438)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0336  data: 0.0001  max mem: 2403
Epoch: [17]  [ 90/316]  eta: 0:00:08  lr: 0.000173  min_lr: 0.000173  loss: 0.4921 (0.5105)  class_acc: 0.7500 (0.7466)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [17]  [100/316]  eta: 0:00:07  lr: 0.000161  min_lr: 0.000161  loss: 0.4592 (0.5053)  class_acc: 0.7500 (0.7506)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9367 (5.9367)  time: 0.0333  data: 0.0001  max mem: 2403
Epoch: [17]  [110/316]  eta: 0:00:07  lr: 0.000161  min_lr: 0.000161  loss: 0.4678 (0.5060)  class_acc: 0.7500 (0.7500)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9367 (5.9367)  time: 0.0331  data: 0.0001  max mem: 2403
Epoch: [17]  [120/316]  eta: 0:00:06  lr: 0.000161  min_lr: 0.000161  loss: 0.5042 (0.5035)  class_acc: 0.8125 (0.7546)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9367 (5.9367)  time: 0.0330  data: 0.0001  max mem: 2403
Epoch: [17]  [130/316]  eta: 0:00:06  lr: 0.000161  min_lr: 0.000161  loss: 0.5066 (0.5075)  class_acc: 0.8125 (0.7529)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9367 (5.9367)  time: 0.0331  data: 0.0001  max mem: 2403
Epoch: [17]  [140/316]  eta: 0:00:06  lr: 0.000161  min_lr: 0.000161  loss: 0.4974 (0.5057)  class_acc: 0.7500 (0.7527)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9367 (5.9367)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [17]  [150/316]  eta: 0:00:05  lr: 0.000161  min_lr: 0.000161  loss: 0.4820 (0.5065)  class_acc: 0.7500 (0.7525)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9367 (5.9367)  time: 0.0325  data: 0.0001  max mem: 2403
Epoch: [17]  [160/316]  eta: 0:00:05  lr: 0.000161  min_lr: 0.000161  loss: 0.4669 (0.5041)  class_acc: 0.7500 (0.7535)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9367 (5.9367)  time: 0.0324  data: 0.0001  max mem: 2403
Epoch: [17]  [170/316]  eta: 0:00:05  lr: 0.000161  min_lr: 0.000161  loss: 0.4631 (0.5019)  class_acc: 0.7500 (0.7537)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9367 (5.9367)  time: 0.0326  data: 0.0001  max mem: 2403
Epoch: [17]  [180/316]  eta: 0:00:04  lr: 0.000161  min_lr: 0.000161  loss: 0.5020 (0.5048)  class_acc: 0.6875 (0.7493)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9367 (5.9367)  time: 0.0327  data: 0.0001  max mem: 2403
Epoch: [17]  [190/316]  eta: 0:00:04  lr: 0.000161  min_lr: 0.000161  loss: 0.4673 (0.5008)  class_acc: 0.7500 (0.7546)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9367 (5.9367)  time: 0.0326  data: 0.0001  max mem: 2403
Epoch: [17]  [200/316]  eta: 0:00:03  lr: 0.000149  min_lr: 0.000149  loss: 0.4611 (0.5017)  class_acc: 0.8125 (0.7556)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9367 (7.1800)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [17]  [210/316]  eta: 0:00:03  lr: 0.000149  min_lr: 0.000149  loss: 0.4752 (0.5008)  class_acc: 0.7500 (0.7556)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9367 (7.1800)  time: 0.0317  data: 0.0001  max mem: 2403
Epoch: [17]  [220/316]  eta: 0:00:03  lr: 0.000149  min_lr: 0.000149  loss: 0.4883 (0.5025)  class_acc: 0.7500 (0.7542)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9367 (7.1800)  time: 0.0302  data: 0.0001  max mem: 2403
Epoch: [17]  [230/316]  eta: 0:00:02  lr: 0.000149  min_lr: 0.000149  loss: 0.5135 (0.5022)  class_acc: 0.7500 (0.7551)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9367 (7.1800)  time: 0.0302  data: 0.0001  max mem: 2403
Epoch: [17]  [240/316]  eta: 0:00:02  lr: 0.000149  min_lr: 0.000149  loss: 0.5104 (0.5023)  class_acc: 0.7500 (0.7552)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9367 (7.1800)  time: 0.0305  data: 0.0001  max mem: 2403
Epoch: [17]  [250/316]  eta: 0:00:02  lr: 0.000149  min_lr: 0.000149  loss: 0.4599 (0.5015)  class_acc: 0.7500 (0.7552)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9367 (7.1800)  time: 0.0306  data: 0.0001  max mem: 2403
Epoch: [17]  [260/316]  eta: 0:00:01  lr: 0.000149  min_lr: 0.000149  loss: 0.4599 (0.5002)  class_acc: 0.7500 (0.7557)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9367 (7.1800)  time: 0.0308  data: 0.0001  max mem: 2403
Epoch: [17]  [270/316]  eta: 0:00:01  lr: 0.000149  min_lr: 0.000149  loss: 0.4703 (0.4983)  class_acc: 0.7500 (0.7567)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9367 (7.1800)  time: 0.0309  data: 0.0001  max mem: 2403
Epoch: [17]  [280/316]  eta: 0:00:01  lr: 0.000149  min_lr: 0.000149  loss: 0.4385 (0.4966)  class_acc: 0.8125 (0.7576)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9367 (7.1800)  time: 0.0311  data: 0.0001  max mem: 2403
Epoch: [17]  [290/316]  eta: 0:00:00  lr: 0.000149  min_lr: 0.000149  loss: 0.4587 (0.4977)  class_acc: 0.8125 (0.7575)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9367 (7.1800)  time: 0.0314  data: 0.0001  max mem: 2403
Epoch: [17]  [300/316]  eta: 0:00:00  lr: 0.000149  min_lr: 0.000149  loss: 0.4788 (0.4976)  class_acc: 0.8125 (0.7569)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5320 (7.2973)  time: 0.0304  data: 0.0001  max mem: 2403
Epoch: [17]  [310/316]  eta: 0:00:00  lr: 0.000149  min_lr: 0.000149  loss: 0.4788 (0.4976)  class_acc: 0.8125 (0.7569)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5320 (7.2973)  time: 0.0146  data: 0.0001  max mem: 2403
Epoch: [17]  [315/316]  eta: 0:00:00  lr: 0.000149  min_lr: 0.000149  loss: 0.4788 (0.4976)  class_acc: 0.8125 (0.7569)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5320 (7.2973)  time: 0.0062  data: 0.0000  max mem: 2403
Epoch: [17] Total time: 0:00:10 (0.0320 s / it)
Averaged stats: lr: 0.000149  min_lr: 0.000149  loss: 0.4788 (0.4976)  class_acc: 0.8125 (0.7569)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5320 (7.2973)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:13  loss: 0.6593 (0.6593)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2314  data: 0.2180  max mem: 2403
Val:  [10/60]  eta: 0:00:01  loss: 0.7746 (0.7849)  accuracy: 0.3750 (0.2841)  balanced_accuracy: 0.2812 (0.2664)  pr_auc: 0.4564 (0.3599)  roc_auc: 0.2889 (0.2459)  time: 0.0388  data: 0.0225  max mem: 2403
Val:  [20/60]  eta: 0:00:01  loss: 0.7746 (0.8024)  accuracy: 0.4583 (0.4048)  balanced_accuracy: 0.4143 (0.3499)  pr_auc: 0.5692 (0.5133)  roc_auc: 0.3579 (0.3130)  time: 0.0168  data: 0.0015  max mem: 2403
Val:  [30/60]  eta: 0:00:00  loss: 0.7842 (0.8023)  accuracy: 0.5417 (0.4489)  balanced_accuracy: 0.4421 (0.3895)  pr_auc: 0.6360 (0.5493)  roc_auc: 0.3778 (0.3492)  time: 0.0141  data: 0.0001  max mem: 2403
Val:  [40/60]  eta: 0:00:00  loss: 0.7842 (0.8005)  accuracy: 0.5417 (0.4736)  balanced_accuracy: 0.4755 (0.4181)  pr_auc: 0.6256 (0.5750)  roc_auc: 0.4357 (0.3902)  time: 0.0141  data: 0.0001  max mem: 2403
Val:  [50/60]  eta: 0:00:00  loss: 0.8383 (0.8259)  accuracy: 0.5000 (0.4657)  balanced_accuracy: 0.4583 (0.4188)  pr_auc: 0.6221 (0.5696)  roc_auc: 0.4704 (0.3962)  time: 0.0140  data: 0.0001  max mem: 2403
Val:  [59/60]  eta: 0:00:00  loss: 0.9140 (0.8440)  accuracy: 0.3750 (0.4298)  balanced_accuracy: 0.3333 (0.3871)  pr_auc: 0.4592 (0.5193)  roc_auc: 0.3217 (0.3677)  time: 0.0135  data: 0.0001  max mem: 2403
Val: Total time: 0:00:01 (0.0205 s / it)
* loss 0.844
Accuracy of the network on the 1431 test EEG: 0.50%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [18]  [  0/316]  eta: 0:01:23  lr: 0.000137  min_lr: 0.000137  loss: 0.4030 (0.4030)  class_acc: 0.8750 (0.8750)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2640  data: 0.2139  max mem: 2403
Epoch: [18]  [ 10/316]  eta: 0:00:15  lr: 0.000137  min_lr: 0.000137  loss: 0.4030 (0.4496)  class_acc: 0.8750 (0.7898)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0523  data: 0.0195  max mem: 2403
Epoch: [18]  [ 20/316]  eta: 0:00:12  lr: 0.000137  min_lr: 0.000137  loss: 0.4008 (0.4465)  class_acc: 0.7500 (0.7917)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0317  data: 0.0001  max mem: 2403
Epoch: [18]  [ 30/316]  eta: 0:00:11  lr: 0.000137  min_lr: 0.000137  loss: 0.4480 (0.4440)  class_acc: 0.7500 (0.7863)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0325  data: 0.0001  max mem: 2403
Epoch: [18]  [ 40/316]  eta: 0:00:10  lr: 0.000137  min_lr: 0.000137  loss: 0.4480 (0.4485)  class_acc: 0.7500 (0.7805)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [18]  [ 50/316]  eta: 0:00:09  lr: 0.000137  min_lr: 0.000137  loss: 0.4580 (0.4573)  class_acc: 0.7500 (0.7770)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0330  data: 0.0001  max mem: 2403
Epoch: [18]  [ 60/316]  eta: 0:00:09  lr: 0.000137  min_lr: 0.000137  loss: 0.4848 (0.4714)  class_acc: 0.7500 (0.7623)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0331  data: 0.0001  max mem: 2403
Epoch: [18]  [ 70/316]  eta: 0:00:08  lr: 0.000137  min_lr: 0.000137  loss: 0.4573 (0.4710)  class_acc: 0.7500 (0.7632)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0330  data: 0.0001  max mem: 2403
Epoch: [18]  [ 80/316]  eta: 0:00:08  lr: 0.000137  min_lr: 0.000137  loss: 0.4205 (0.4713)  class_acc: 0.7500 (0.7608)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [18]  [ 90/316]  eta: 0:00:07  lr: 0.000137  min_lr: 0.000137  loss: 0.5066 (0.4754)  class_acc: 0.7500 (0.7603)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [18]  [100/316]  eta: 0:00:07  lr: 0.000126  min_lr: 0.000126  loss: 0.5052 (0.4735)  class_acc: 0.7500 (0.7649)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5881 (6.5881)  time: 0.0330  data: 0.0001  max mem: 2403
Epoch: [18]  [110/316]  eta: 0:00:07  lr: 0.000126  min_lr: 0.000126  loss: 0.4630 (0.4734)  class_acc: 0.7500 (0.7658)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5881 (6.5881)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [18]  [120/316]  eta: 0:00:06  lr: 0.000126  min_lr: 0.000126  loss: 0.4630 (0.4695)  class_acc: 0.8125 (0.7696)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5881 (6.5881)  time: 0.0323  data: 0.0001  max mem: 2403
Epoch: [18]  [130/316]  eta: 0:00:06  lr: 0.000126  min_lr: 0.000126  loss: 0.4814 (0.4738)  class_acc: 0.8125 (0.7677)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5881 (6.5881)  time: 0.0319  data: 0.0001  max mem: 2403
Epoch: [18]  [140/316]  eta: 0:00:06  lr: 0.000126  min_lr: 0.000126  loss: 0.5032 (0.4730)  class_acc: 0.7500 (0.7677)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5881 (6.5881)  time: 0.0320  data: 0.0001  max mem: 2403
Epoch: [18]  [150/316]  eta: 0:00:05  lr: 0.000126  min_lr: 0.000126  loss: 0.5033 (0.4743)  class_acc: 0.7500 (0.7674)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5881 (6.5881)  time: 0.0326  data: 0.0001  max mem: 2403
Epoch: [18]  [160/316]  eta: 0:00:05  lr: 0.000126  min_lr: 0.000126  loss: 0.4649 (0.4712)  class_acc: 0.8125 (0.7706)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5881 (6.5881)  time: 0.0327  data: 0.0001  max mem: 2403
Epoch: [18]  [170/316]  eta: 0:00:04  lr: 0.000126  min_lr: 0.000126  loss: 0.4361 (0.4695)  class_acc: 0.8125 (0.7708)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5881 (6.5881)  time: 0.0325  data: 0.0001  max mem: 2403
Epoch: [18]  [180/316]  eta: 0:00:04  lr: 0.000126  min_lr: 0.000126  loss: 0.4919 (0.4711)  class_acc: 0.7500 (0.7693)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5881 (6.5881)  time: 0.0327  data: 0.0001  max mem: 2403
Epoch: [18]  [190/316]  eta: 0:00:04  lr: 0.000126  min_lr: 0.000126  loss: 0.4450 (0.4702)  class_acc: 0.8125 (0.7706)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5881 (6.5881)  time: 0.0327  data: 0.0001  max mem: 2403
Epoch: [18]  [200/316]  eta: 0:00:03  lr: 0.000115  min_lr: 0.000115  loss: 0.4450 (0.4738)  class_acc: 0.7500 (0.7677)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8052 (5.6966)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [18]  [210/316]  eta: 0:00:03  lr: 0.000115  min_lr: 0.000115  loss: 0.5054 (0.4760)  class_acc: 0.6875 (0.7660)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8052 (5.6966)  time: 0.0330  data: 0.0001  max mem: 2403
Epoch: [18]  [220/316]  eta: 0:00:03  lr: 0.000115  min_lr: 0.000115  loss: 0.4571 (0.4749)  class_acc: 0.7500 (0.7653)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8052 (5.6966)  time: 0.0330  data: 0.0001  max mem: 2403
Epoch: [18]  [230/316]  eta: 0:00:02  lr: 0.000115  min_lr: 0.000115  loss: 0.4534 (0.4765)  class_acc: 0.7500 (0.7652)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8052 (5.6966)  time: 0.0330  data: 0.0001  max mem: 2403
Epoch: [18]  [240/316]  eta: 0:00:02  lr: 0.000115  min_lr: 0.000115  loss: 0.4753 (0.4787)  class_acc: 0.7500 (0.7624)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8052 (5.6966)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [18]  [250/316]  eta: 0:00:02  lr: 0.000115  min_lr: 0.000115  loss: 0.5115 (0.4803)  class_acc: 0.7500 (0.7620)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8052 (5.6966)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [18]  [260/316]  eta: 0:00:01  lr: 0.000115  min_lr: 0.000115  loss: 0.4981 (0.4805)  class_acc: 0.7500 (0.7603)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8052 (5.6966)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [18]  [270/316]  eta: 0:00:01  lr: 0.000115  min_lr: 0.000115  loss: 0.5194 (0.4826)  class_acc: 0.7500 (0.7611)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8052 (5.6966)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [18]  [280/316]  eta: 0:00:01  lr: 0.000115  min_lr: 0.000115  loss: 0.4562 (0.4812)  class_acc: 0.8125 (0.7618)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8052 (5.6966)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [18]  [290/316]  eta: 0:00:00  lr: 0.000115  min_lr: 0.000115  loss: 0.4562 (0.4832)  class_acc: 0.8125 (0.7610)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8052 (5.6966)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [18]  [300/316]  eta: 0:00:00  lr: 0.000115  min_lr: 0.000115  loss: 0.5085 (0.4823)  class_acc: 0.8125 (0.7623)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5881 (7.4804)  time: 0.0307  data: 0.0001  max mem: 2403
Epoch: [18]  [310/316]  eta: 0:00:00  lr: 0.000115  min_lr: 0.000115  loss: 0.5085 (0.4823)  class_acc: 0.8125 (0.7623)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5881 (7.4804)  time: 0.0142  data: 0.0001  max mem: 2403
Epoch: [18]  [315/316]  eta: 0:00:00  lr: 0.000115  min_lr: 0.000115  loss: 0.5085 (0.4823)  class_acc: 0.8125 (0.7623)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5881 (7.4804)  time: 0.0062  data: 0.0000  max mem: 2403
Epoch: [18] Total time: 0:00:10 (0.0322 s / it)
Averaged stats: lr: 0.000115  min_lr: 0.000115  loss: 0.5085 (0.4823)  class_acc: 0.8125 (0.7623)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5881 (7.4804)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:15  loss: 0.5909 (0.5909)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2608  data: 0.2432  max mem: 2403
Val:  [10/60]  eta: 0:00:01  loss: 0.7785 (0.7699)  accuracy: 0.3333 (0.2917)  balanced_accuracy: 0.3438 (0.2568)  pr_auc: 0.4523 (0.3610)  roc_auc: 0.3037 (0.2502)  time: 0.0395  data: 0.0227  max mem: 2403
Val:  [20/60]  eta: 0:00:01  loss: 0.7785 (0.7924)  accuracy: 0.5000 (0.4206)  balanced_accuracy: 0.4167 (0.3465)  pr_auc: 0.5835 (0.5099)  roc_auc: 0.3429 (0.3082)  time: 0.0159  data: 0.0004  max mem: 2403
Val:  [30/60]  eta: 0:00:00  loss: 0.7937 (0.8002)  accuracy: 0.5417 (0.4718)  balanced_accuracy: 0.4667 (0.3944)  pr_auc: 0.6279 (0.5462)  roc_auc: 0.3704 (0.3442)  time: 0.0144  data: 0.0001  max mem: 2403
Val:  [40/60]  eta: 0:00:00  loss: 0.7937 (0.8021)  accuracy: 0.5417 (0.4888)  balanced_accuracy: 0.4688 (0.4171)  pr_auc: 0.6261 (0.5725)  roc_auc: 0.4357 (0.3857)  time: 0.0144  data: 0.0001  max mem: 2403
Val:  [50/60]  eta: 0:00:00  loss: 0.8508 (0.8312)  accuracy: 0.4583 (0.4853)  balanced_accuracy: 0.4336 (0.4249)  pr_auc: 0.6153 (0.5693)  roc_auc: 0.4667 (0.3963)  time: 0.0142  data: 0.0001  max mem: 2403
Val:  [59/60]  eta: 0:00:00  loss: 1.0079 (0.8590)  accuracy: 0.3750 (0.4472)  balanced_accuracy: 0.3750 (0.3946)  pr_auc: 0.4582 (0.5188)  roc_auc: 0.3147 (0.3666)  time: 0.0135  data: 0.0001  max mem: 2403
Val: Total time: 0:00:01 (0.0209 s / it)
* loss 0.859
Accuracy of the network on the 1431 test EEG: 0.52%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [19]  [  0/316]  eta: 0:01:30  lr: 0.000104  min_lr: 0.000104  loss: 0.2948 (0.2948)  class_acc: 0.8750 (0.8750)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2877  data: 0.2434  max mem: 2403
Epoch: [19]  [ 10/316]  eta: 0:00:16  lr: 0.000104  min_lr: 0.000104  loss: 0.4303 (0.4538)  class_acc: 0.8125 (0.7841)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0553  data: 0.0222  max mem: 2403
Epoch: [19]  [ 20/316]  eta: 0:00:13  lr: 0.000104  min_lr: 0.000104  loss: 0.4014 (0.4349)  class_acc: 0.8125 (0.7946)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0320  data: 0.0001  max mem: 2403
Epoch: [19]  [ 30/316]  eta: 0:00:11  lr: 0.000104  min_lr: 0.000104  loss: 0.4107 (0.4443)  class_acc: 0.8125 (0.7984)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0318  data: 0.0001  max mem: 2403
Epoch: [19]  [ 40/316]  eta: 0:00:10  lr: 0.000104  min_lr: 0.000104  loss: 0.4379 (0.4476)  class_acc: 0.7500 (0.7912)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0317  data: 0.0001  max mem: 2403
Epoch: [19]  [ 50/316]  eta: 0:00:09  lr: 0.000104  min_lr: 0.000104  loss: 0.4697 (0.4601)  class_acc: 0.7500 (0.7770)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0324  data: 0.0001  max mem: 2403
Epoch: [19]  [ 60/316]  eta: 0:00:09  lr: 0.000104  min_lr: 0.000104  loss: 0.5406 (0.4798)  class_acc: 0.6875 (0.7664)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0332  data: 0.0001  max mem: 2403
Epoch: [19]  [ 70/316]  eta: 0:00:08  lr: 0.000104  min_lr: 0.000104  loss: 0.5406 (0.4883)  class_acc: 0.7500 (0.7614)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0335  data: 0.0001  max mem: 2403
Epoch: [19]  [ 80/316]  eta: 0:00:08  lr: 0.000104  min_lr: 0.000104  loss: 0.5006 (0.4869)  class_acc: 0.7500 (0.7585)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0334  data: 0.0001  max mem: 2403
Epoch: [19]  [ 90/316]  eta: 0:00:08  lr: 0.000104  min_lr: 0.000104  loss: 0.4906 (0.4837)  class_acc: 0.7500 (0.7630)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0331  data: 0.0001  max mem: 2403
Epoch: [19]  [100/316]  eta: 0:00:07  lr: 0.000093  min_lr: 0.000093  loss: 0.4454 (0.4787)  class_acc: 0.8125 (0.7667)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6560 (6.6560)  time: 0.0335  data: 0.0001  max mem: 2403
Epoch: [19]  [110/316]  eta: 0:00:07  lr: 0.000093  min_lr: 0.000093  loss: 0.4686 (0.4809)  class_acc: 0.8125 (0.7658)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6560 (6.6560)  time: 0.0339  data: 0.0001  max mem: 2403
Epoch: [19]  [120/316]  eta: 0:00:06  lr: 0.000093  min_lr: 0.000093  loss: 0.4407 (0.4744)  class_acc: 0.8125 (0.7717)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6560 (6.6560)  time: 0.0333  data: 0.0001  max mem: 2403
Epoch: [19]  [130/316]  eta: 0:00:06  lr: 0.000093  min_lr: 0.000093  loss: 0.4187 (0.4749)  class_acc: 0.8125 (0.7710)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6560 (6.6560)  time: 0.0330  data: 0.0001  max mem: 2403
Epoch: [19]  [140/316]  eta: 0:00:06  lr: 0.000093  min_lr: 0.000093  loss: 0.4980 (0.4760)  class_acc: 0.7500 (0.7704)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6560 (6.6560)  time: 0.0331  data: 0.0001  max mem: 2403
Epoch: [19]  [150/316]  eta: 0:00:05  lr: 0.000093  min_lr: 0.000093  loss: 0.3981 (0.4730)  class_acc: 0.8125 (0.7719)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6560 (6.6560)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [19]  [160/316]  eta: 0:00:05  lr: 0.000093  min_lr: 0.000093  loss: 0.3981 (0.4708)  class_acc: 0.8125 (0.7733)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6560 (6.6560)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [19]  [170/316]  eta: 0:00:05  lr: 0.000093  min_lr: 0.000093  loss: 0.4162 (0.4693)  class_acc: 0.7500 (0.7738)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6560 (6.6560)  time: 0.0329  data: 0.0001  max mem: 2403
Epoch: [19]  [180/316]  eta: 0:00:04  lr: 0.000093  min_lr: 0.000093  loss: 0.4379 (0.4700)  class_acc: 0.7500 (0.7731)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6560 (6.6560)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [19]  [190/316]  eta: 0:00:04  lr: 0.000093  min_lr: 0.000093  loss: 0.4379 (0.4700)  class_acc: 0.7500 (0.7729)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6560 (6.6560)  time: 0.0328  data: 0.0001  max mem: 2403
Epoch: [19]  [200/316]  eta: 0:00:03  lr: 0.000084  min_lr: 0.000084  loss: 0.4867 (0.4720)  class_acc: 0.7500 (0.7730)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3298 (5.4929)  time: 0.0327  data: 0.0001  max mem: 2403
Epoch: [19]  [210/316]  eta: 0:00:03  lr: 0.000084  min_lr: 0.000084  loss: 0.4999 (0.4735)  class_acc: 0.7500 (0.7707)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3298 (5.4929)  time: 0.0320  data: 0.0001  max mem: 2403
Epoch: [19]  [220/316]  eta: 0:00:03  lr: 0.000084  min_lr: 0.000084  loss: 0.5256 (0.4762)  class_acc: 0.6875 (0.7695)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3298 (5.4929)  time: 0.0313  data: 0.0001  max mem: 2403
Epoch: [19]  [230/316]  eta: 0:00:02  lr: 0.000084  min_lr: 0.000084  loss: 0.5334 (0.4772)  class_acc: 0.7500 (0.7692)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3298 (5.4929)  time: 0.0313  data: 0.0001  max mem: 2403
Epoch: [19]  [240/316]  eta: 0:00:02  lr: 0.000084  min_lr: 0.000084  loss: 0.4659 (0.4766)  class_acc: 0.7500 (0.7705)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3298 (5.4929)  time: 0.0315  data: 0.0001  max mem: 2403
Epoch: [19]  [250/316]  eta: 0:00:02  lr: 0.000084  min_lr: 0.000084  loss: 0.5028 (0.4776)  class_acc: 0.7500 (0.7704)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3298 (5.4929)  time: 0.0315  data: 0.0001  max mem: 2403
Epoch: [19]  [260/316]  eta: 0:00:01  lr: 0.000084  min_lr: 0.000084  loss: 0.4590 (0.4758)  class_acc: 0.8125 (0.7720)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3298 (5.4929)  time: 0.0310  data: 0.0001  max mem: 2403
Epoch: [19]  [270/316]  eta: 0:00:01  lr: 0.000084  min_lr: 0.000084  loss: 0.4389 (0.4763)  class_acc: 0.7500 (0.7712)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3298 (5.4929)  time: 0.0310  data: 0.0001  max mem: 2403
Epoch: [19]  [280/316]  eta: 0:00:01  lr: 0.000084  min_lr: 0.000084  loss: 0.4367 (0.4748)  class_acc: 0.7500 (0.7725)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3298 (5.4929)  time: 0.0315  data: 0.0001  max mem: 2403
Epoch: [19]  [290/316]  eta: 0:00:00  lr: 0.000084  min_lr: 0.000084  loss: 0.4346 (0.4743)  class_acc: 0.7500 (0.7713)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3298 (5.4929)  time: 0.0317  data: 0.0001  max mem: 2403
Epoch: [19]  [300/316]  eta: 0:00:00  lr: 0.000084  min_lr: 0.000084  loss: 0.4255 (0.4725)  class_acc: 0.7500 (0.7721)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6703 (5.5520)  time: 0.0297  data: 0.0001  max mem: 2403
Epoch: [19]  [310/316]  eta: 0:00:00  lr: 0.000084  min_lr: 0.000084  loss: 0.4255 (0.4725)  class_acc: 0.7500 (0.7721)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6703 (5.5520)  time: 0.0138  data: 0.0001  max mem: 2403
Epoch: [19]  [315/316]  eta: 0:00:00  lr: 0.000084  min_lr: 0.000084  loss: 0.4255 (0.4725)  class_acc: 0.7500 (0.7721)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6703 (5.5520)  time: 0.0062  data: 0.0001  max mem: 2403
Epoch: [19] Total time: 0:00:10 (0.0320 s / it)
Averaged stats: lr: 0.000084  min_lr: 0.000084  loss: 0.4255 (0.4725)  class_acc: 0.7500 (0.7721)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6703 (5.5520)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:16  loss: 0.7140 (0.7140)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2722  data: 0.2577  max mem: 2403
Val:  [10/60]  eta: 0:00:02  loss: 0.8001 (0.8188)  accuracy: 0.3750 (0.2765)  balanced_accuracy: 0.2812 (0.2583)  pr_auc: 0.4542 (0.3604)  roc_auc: 0.2889 (0.2459)  time: 0.0407  data: 0.0255  max mem: 2403
Val:  [20/60]  eta: 0:00:01  loss: 0.8107 (0.8361)  accuracy: 0.4167 (0.3889)  balanced_accuracy: 0.4126 (0.3434)  pr_auc: 0.5720 (0.5123)  roc_auc: 0.3556 (0.3121)  time: 0.0161  data: 0.0012  max mem: 2403
Val:  [30/60]  eta: 0:00:00  loss: 0.8107 (0.8314)  accuracy: 0.5000 (0.4274)  balanced_accuracy: 0.4222 (0.3772)  pr_auc: 0.6283 (0.5478)  roc_auc: 0.3778 (0.3473)  time: 0.0147  data: 0.0002  max mem: 2403
Val:  [40/60]  eta: 0:00:00  loss: 0.7987 (0.8248)  accuracy: 0.5000 (0.4533)  balanced_accuracy: 0.4714 (0.4063)  pr_auc: 0.6210 (0.5715)  roc_auc: 0.4357 (0.3852)  time: 0.0145  data: 0.0002  max mem: 2403
Val:  [50/60]  eta: 0:00:00  loss: 0.8251 (0.8460)  accuracy: 0.5000 (0.4518)  balanced_accuracy: 0.4714 (0.4120)  pr_auc: 0.6097 (0.5675)  roc_auc: 0.4857 (0.3938)  time: 0.0142  data: 0.0001  max mem: 2403
Val:  [59/60]  eta: 0:00:00  loss: 0.8984 (0.8555)  accuracy: 0.3750 (0.4207)  balanced_accuracy: 0.3438 (0.3830)  pr_auc: 0.4651 (0.5179)  roc_auc: 0.2986 (0.3666)  time: 0.0135  data: 0.0001  max mem: 2403
Val: Total time: 0:00:01 (0.0213 s / it)
* loss 0.856
Accuracy of the network on the 1431 test EEG: 0.49%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [20]  [  0/316]  eta: 0:01:21  lr: 0.000074  min_lr: 0.000074  loss: 0.3549 (0.3549)  class_acc: 0.8125 (0.8125)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2586  data: 0.2126  max mem: 2403
Epoch: [20]  [ 10/316]  eta: 0:00:16  lr: 0.000074  min_lr: 0.000074  loss: 0.3701 (0.4057)  class_acc: 0.8125 (0.7955)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0542  data: 0.0195  max mem: 2404
Epoch: [20]  [ 20/316]  eta: 0:00:13  lr: 0.000074  min_lr: 0.000074  loss: 0.3959 (0.4143)  class_acc: 0.8125 (0.8065)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0336  data: 0.0002  max mem: 2404
Epoch: [20]  [ 30/316]  eta: 0:00:11  lr: 0.000074  min_lr: 0.000074  loss: 0.3975 (0.4146)  class_acc: 0.8125 (0.8105)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0333  data: 0.0001  max mem: 2404
Epoch: [20]  [ 40/316]  eta: 0:00:10  lr: 0.000074  min_lr: 0.000074  loss: 0.3975 (0.4300)  class_acc: 0.8125 (0.7973)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0332  data: 0.0001  max mem: 2404
Epoch: [20]  [ 50/316]  eta: 0:00:10  lr: 0.000074  min_lr: 0.000074  loss: 0.3950 (0.4267)  class_acc: 0.8125 (0.8039)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0331  data: 0.0001  max mem: 2404
Epoch: [20]  [ 60/316]  eta: 0:00:09  lr: 0.000074  min_lr: 0.000074  loss: 0.4285 (0.4428)  class_acc: 0.8125 (0.7961)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0335  data: 0.0001  max mem: 2404
Epoch: [20]  [ 70/316]  eta: 0:00:09  lr: 0.000074  min_lr: 0.000074  loss: 0.4556 (0.4424)  class_acc: 0.8125 (0.7949)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0341  data: 0.0001  max mem: 2404
Epoch: [20]  [ 80/316]  eta: 0:00:08  lr: 0.000074  min_lr: 0.000074  loss: 0.4829 (0.4471)  class_acc: 0.8125 (0.7909)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0338  data: 0.0001  max mem: 2404
Epoch: [20]  [ 90/316]  eta: 0:00:08  lr: 0.000074  min_lr: 0.000074  loss: 0.4882 (0.4518)  class_acc: 0.7500 (0.7857)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0334  data: 0.0001  max mem: 2404
Epoch: [20]  [100/316]  eta: 0:00:07  lr: 0.000065  min_lr: 0.000065  loss: 0.4455 (0.4526)  class_acc: 0.7500 (0.7847)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7926 (7.7926)  time: 0.0336  data: 0.0001  max mem: 2404
Epoch: [20]  [110/316]  eta: 0:00:07  lr: 0.000065  min_lr: 0.000065  loss: 0.4465 (0.4564)  class_acc: 0.7500 (0.7832)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7926 (7.7926)  time: 0.0338  data: 0.0001  max mem: 2404
Epoch: [20]  [120/316]  eta: 0:00:06  lr: 0.000065  min_lr: 0.000065  loss: 0.4665 (0.4552)  class_acc: 0.7500 (0.7841)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7926 (7.7926)  time: 0.0334  data: 0.0001  max mem: 2404
Epoch: [20]  [130/316]  eta: 0:00:06  lr: 0.000065  min_lr: 0.000065  loss: 0.4965 (0.4572)  class_acc: 0.8125 (0.7824)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7926 (7.7926)  time: 0.0333  data: 0.0001  max mem: 2404
Epoch: [20]  [140/316]  eta: 0:00:06  lr: 0.000065  min_lr: 0.000065  loss: 0.4774 (0.4595)  class_acc: 0.7500 (0.7828)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7926 (7.7926)  time: 0.0333  data: 0.0001  max mem: 2404
Epoch: [20]  [150/316]  eta: 0:00:05  lr: 0.000065  min_lr: 0.000065  loss: 0.4696 (0.4610)  class_acc: 0.7500 (0.7802)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7926 (7.7926)  time: 0.0329  data: 0.0001  max mem: 2404
Epoch: [20]  [160/316]  eta: 0:00:05  lr: 0.000065  min_lr: 0.000065  loss: 0.4696 (0.4602)  class_acc: 0.7500 (0.7814)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7926 (7.7926)  time: 0.0325  data: 0.0001  max mem: 2404
Epoch: [20]  [170/316]  eta: 0:00:05  lr: 0.000065  min_lr: 0.000065  loss: 0.4788 (0.4586)  class_acc: 0.8125 (0.7814)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7926 (7.7926)  time: 0.0323  data: 0.0001  max mem: 2404
Epoch: [20]  [180/316]  eta: 0:00:04  lr: 0.000065  min_lr: 0.000065  loss: 0.4756 (0.4582)  class_acc: 0.8125 (0.7818)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7926 (7.7926)  time: 0.0319  data: 0.0001  max mem: 2404
Epoch: [20]  [190/316]  eta: 0:00:04  lr: 0.000065  min_lr: 0.000065  loss: 0.4670 (0.4585)  class_acc: 0.8125 (0.7811)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7926 (7.7926)  time: 0.0318  data: 0.0001  max mem: 2404
Epoch: [20]  [200/316]  eta: 0:00:03  lr: 0.000057  min_lr: 0.000057  loss: 0.4694 (0.4614)  class_acc: 0.7500 (0.7805)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7480 (7.2703)  time: 0.0320  data: 0.0001  max mem: 2404
Epoch: [20]  [210/316]  eta: 0:00:03  lr: 0.000057  min_lr: 0.000057  loss: 0.4696 (0.4613)  class_acc: 0.7500 (0.7796)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7480 (7.2703)  time: 0.0316  data: 0.0001  max mem: 2404
Epoch: [20]  [220/316]  eta: 0:00:03  lr: 0.000057  min_lr: 0.000057  loss: 0.4298 (0.4622)  class_acc: 0.7500 (0.7800)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7480 (7.2703)  time: 0.0309  data: 0.0001  max mem: 2404
Epoch: [20]  [230/316]  eta: 0:00:02  lr: 0.000057  min_lr: 0.000057  loss: 0.5054 (0.4661)  class_acc: 0.7500 (0.7757)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7480 (7.2703)  time: 0.0306  data: 0.0001  max mem: 2404
Epoch: [20]  [240/316]  eta: 0:00:02  lr: 0.000057  min_lr: 0.000057  loss: 0.5124 (0.4662)  class_acc: 0.7500 (0.7749)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7480 (7.2703)  time: 0.0305  data: 0.0001  max mem: 2404
Epoch: [20]  [250/316]  eta: 0:00:02  lr: 0.000057  min_lr: 0.000057  loss: 0.5124 (0.4703)  class_acc: 0.7500 (0.7719)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7480 (7.2703)  time: 0.0308  data: 0.0001  max mem: 2404
Epoch: [20]  [260/316]  eta: 0:00:01  lr: 0.000057  min_lr: 0.000057  loss: 0.4571 (0.4695)  class_acc: 0.7500 (0.7723)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7480 (7.2703)  time: 0.0309  data: 0.0001  max mem: 2404
Epoch: [20]  [270/316]  eta: 0:00:01  lr: 0.000057  min_lr: 0.000057  loss: 0.3986 (0.4686)  class_acc: 0.8125 (0.7728)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7480 (7.2703)  time: 0.0309  data: 0.0001  max mem: 2404
Epoch: [20]  [280/316]  eta: 0:00:01  lr: 0.000057  min_lr: 0.000057  loss: 0.3851 (0.4657)  class_acc: 0.8125 (0.7751)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7480 (7.2703)  time: 0.0307  data: 0.0002  max mem: 2404
Epoch: [20]  [290/316]  eta: 0:00:00  lr: 0.000057  min_lr: 0.000057  loss: 0.3704 (0.4649)  class_acc: 0.8125 (0.7760)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7480 (7.2703)  time: 0.0306  data: 0.0002  max mem: 2404
Epoch: [20]  [300/316]  eta: 0:00:00  lr: 0.000057  min_lr: 0.000057  loss: 0.3957 (0.4646)  class_acc: 0.8125 (0.7769)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7480 (6.3814)  time: 0.0291  data: 0.0002  max mem: 2404
Epoch: [20]  [310/316]  eta: 0:00:00  lr: 0.000057  min_lr: 0.000057  loss: 0.3957 (0.4646)  class_acc: 0.8125 (0.7769)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7480 (6.3814)  time: 0.0137  data: 0.0001  max mem: 2404
Epoch: [20]  [315/316]  eta: 0:00:00  lr: 0.000057  min_lr: 0.000057  loss: 0.3957 (0.4646)  class_acc: 0.8125 (0.7769)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7480 (6.3814)  time: 0.0062  data: 0.0001  max mem: 2404
Epoch: [20] Total time: 0:00:10 (0.0319 s / it)
Averaged stats: lr: 0.000057  min_lr: 0.000057  loss: 0.3957 (0.4646)  class_acc: 0.8125 (0.7769)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7480 (6.3814)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:14  loss: 0.6380 (0.6380)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2483  data: 0.2247  max mem: 2404
Val:  [10/60]  eta: 0:00:01  loss: 0.7926 (0.7984)  accuracy: 0.3333 (0.2689)  balanced_accuracy: 0.2812 (0.2412)  pr_auc: 0.4542 (0.3618)  roc_auc: 0.2929 (0.2484)  time: 0.0398  data: 0.0219  max mem: 2404
Val:  [20/60]  eta: 0:00:01  loss: 0.7926 (0.8195)  accuracy: 0.4583 (0.3988)  balanced_accuracy: 0.4000 (0.3384)  pr_auc: 0.5946 (0.5086)  roc_auc: 0.3426 (0.3065)  time: 0.0169  data: 0.0009  max mem: 2404
Val:  [30/60]  eta: 0:00:00  loss: 0.8072 (0.8223)  accuracy: 0.5417 (0.4462)  balanced_accuracy: 0.4222 (0.3813)  pr_auc: 0.6182 (0.5434)  roc_auc: 0.3704 (0.3400)  time: 0.0147  data: 0.0002  max mem: 2404
Val:  [40/60]  eta: 0:00:00  loss: 0.7926 (0.8192)  accuracy: 0.5417 (0.4695)  balanced_accuracy: 0.4714 (0.4089)  pr_auc: 0.6171 (0.5692)  roc_auc: 0.4321 (0.3816)  time: 0.0146  data: 0.0002  max mem: 2404
Val:  [50/60]  eta: 0:00:00  loss: 0.8315 (0.8440)  accuracy: 0.5000 (0.4690)  balanced_accuracy: 0.4583 (0.4177)  pr_auc: 0.6155 (0.5668)  roc_auc: 0.5000 (0.3935)  time: 0.0143  data: 0.0001  max mem: 2404
Val:  [59/60]  eta: 0:00:00  loss: 0.9632 (0.8634)  accuracy: 0.3750 (0.4340)  balanced_accuracy: 0.3750 (0.3890)  pr_auc: 0.4630 (0.5173)  roc_auc: 0.2917 (0.3661)  time: 0.0135  data: 0.0001  max mem: 2404
Val: Total time: 0:00:01 (0.0211 s / it)
* loss 0.863
Accuracy of the network on the 1431 test EEG: 0.50%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [21]  [  0/316]  eta: 0:01:24  lr: 0.000049  min_lr: 0.000049  loss: 0.4101 (0.4101)  class_acc: 0.8750 (0.8750)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2680  data: 0.2357  max mem: 2404
Epoch: [21]  [ 10/316]  eta: 0:00:16  lr: 0.000049  min_lr: 0.000049  loss: 0.4012 (0.4266)  class_acc: 0.8750 (0.8125)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0532  data: 0.0215  max mem: 2404
Epoch: [21]  [ 20/316]  eta: 0:00:12  lr: 0.000049  min_lr: 0.000049  loss: 0.4012 (0.4318)  class_acc: 0.8125 (0.8065)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0323  data: 0.0001  max mem: 2404
Epoch: [21]  [ 30/316]  eta: 0:00:11  lr: 0.000049  min_lr: 0.000049  loss: 0.4247 (0.4378)  class_acc: 0.8125 (0.8105)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0333  data: 0.0001  max mem: 2404
Epoch: [21]  [ 40/316]  eta: 0:00:10  lr: 0.000049  min_lr: 0.000049  loss: 0.4247 (0.4473)  class_acc: 0.8125 (0.7927)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0337  data: 0.0001  max mem: 2404
Epoch: [21]  [ 50/316]  eta: 0:00:10  lr: 0.000049  min_lr: 0.000049  loss: 0.4307 (0.4469)  class_acc: 0.7500 (0.7904)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0336  data: 0.0001  max mem: 2404
Epoch: [21]  [ 60/316]  eta: 0:00:09  lr: 0.000049  min_lr: 0.000049  loss: 0.4909 (0.4620)  class_acc: 0.7500 (0.7828)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0331  data: 0.0001  max mem: 2404
Epoch: [21]  [ 70/316]  eta: 0:00:08  lr: 0.000049  min_lr: 0.000049  loss: 0.5118 (0.4693)  class_acc: 0.7500 (0.7773)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0334  data: 0.0001  max mem: 2404
Epoch: [21]  [ 80/316]  eta: 0:00:08  lr: 0.000049  min_lr: 0.000049  loss: 0.4750 (0.4673)  class_acc: 0.7500 (0.7809)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0339  data: 0.0001  max mem: 2404
Epoch: [21]  [ 90/316]  eta: 0:00:08  lr: 0.000049  min_lr: 0.000049  loss: 0.4816 (0.4728)  class_acc: 0.8125 (0.7788)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0339  data: 0.0001  max mem: 2404
Epoch: [21]  [100/316]  eta: 0:00:07  lr: 0.000041  min_lr: 0.000041  loss: 0.4869 (0.4672)  class_acc: 0.7500 (0.7766)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2380 (4.2380)  time: 0.0340  data: 0.0001  max mem: 2404
Epoch: [21]  [110/316]  eta: 0:00:07  lr: 0.000041  min_lr: 0.000041  loss: 0.4372 (0.4689)  class_acc: 0.7500 (0.7714)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2380 (4.2380)  time: 0.0340  data: 0.0001  max mem: 2404
Epoch: [21]  [120/316]  eta: 0:00:06  lr: 0.000041  min_lr: 0.000041  loss: 0.4061 (0.4682)  class_acc: 0.7500 (0.7732)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2380 (4.2380)  time: 0.0340  data: 0.0001  max mem: 2404
Epoch: [21]  [130/316]  eta: 0:00:06  lr: 0.000041  min_lr: 0.000041  loss: 0.4576 (0.4695)  class_acc: 0.7500 (0.7729)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2380 (4.2380)  time: 0.0341  data: 0.0001  max mem: 2404
Epoch: [21]  [140/316]  eta: 0:00:06  lr: 0.000041  min_lr: 0.000041  loss: 0.4796 (0.4718)  class_acc: 0.7500 (0.7699)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2380 (4.2380)  time: 0.0341  data: 0.0001  max mem: 2404
Epoch: [21]  [150/316]  eta: 0:00:05  lr: 0.000041  min_lr: 0.000041  loss: 0.4796 (0.4699)  class_acc: 0.7500 (0.7732)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2380 (4.2380)  time: 0.0341  data: 0.0001  max mem: 2404
Epoch: [21]  [160/316]  eta: 0:00:05  lr: 0.000041  min_lr: 0.000041  loss: 0.3831 (0.4649)  class_acc: 0.8125 (0.7776)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2380 (4.2380)  time: 0.0341  data: 0.0001  max mem: 2404
Epoch: [21]  [170/316]  eta: 0:00:05  lr: 0.000041  min_lr: 0.000041  loss: 0.3763 (0.4615)  class_acc: 0.8750 (0.7822)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2380 (4.2380)  time: 0.0341  data: 0.0001  max mem: 2404
Epoch: [21]  [180/316]  eta: 0:00:04  lr: 0.000041  min_lr: 0.000041  loss: 0.4185 (0.4613)  class_acc: 0.8125 (0.7835)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2380 (4.2380)  time: 0.0339  data: 0.0001  max mem: 2404
Epoch: [21]  [190/316]  eta: 0:00:04  lr: 0.000041  min_lr: 0.000041  loss: 0.4301 (0.4606)  class_acc: 0.8125 (0.7857)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2380 (4.2380)  time: 0.0339  data: 0.0001  max mem: 2404
Epoch: [21]  [200/316]  eta: 0:00:04  lr: 0.000034  min_lr: 0.000034  loss: 0.4365 (0.4613)  class_acc: 0.8125 (0.7848)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1253 (4.1816)  time: 0.0339  data: 0.0001  max mem: 2404
Epoch: [21]  [210/316]  eta: 0:00:03  lr: 0.000034  min_lr: 0.000034  loss: 0.4382 (0.4618)  class_acc: 0.8125 (0.7844)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1253 (4.1816)  time: 0.0339  data: 0.0001  max mem: 2404
Epoch: [21]  [220/316]  eta: 0:00:03  lr: 0.000034  min_lr: 0.000034  loss: 0.5000 (0.4644)  class_acc: 0.7500 (0.7822)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1253 (4.1816)  time: 0.0332  data: 0.0001  max mem: 2404
Epoch: [21]  [230/316]  eta: 0:00:02  lr: 0.000034  min_lr: 0.000034  loss: 0.5012 (0.4652)  class_acc: 0.7500 (0.7827)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1253 (4.1816)  time: 0.0326  data: 0.0001  max mem: 2404
Epoch: [21]  [240/316]  eta: 0:00:02  lr: 0.000034  min_lr: 0.000034  loss: 0.4313 (0.4641)  class_acc: 0.7500 (0.7819)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1253 (4.1816)  time: 0.0325  data: 0.0001  max mem: 2404
Epoch: [21]  [250/316]  eta: 0:00:02  lr: 0.000034  min_lr: 0.000034  loss: 0.4313 (0.4659)  class_acc: 0.7500 (0.7796)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1253 (4.1816)  time: 0.0322  data: 0.0001  max mem: 2404
Epoch: [21]  [260/316]  eta: 0:00:01  lr: 0.000034  min_lr: 0.000034  loss: 0.4343 (0.4655)  class_acc: 0.7500 (0.7799)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1253 (4.1816)  time: 0.0323  data: 0.0001  max mem: 2404
Epoch: [21]  [270/316]  eta: 0:00:01  lr: 0.000034  min_lr: 0.000034  loss: 0.4343 (0.4653)  class_acc: 0.7500 (0.7791)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1253 (4.1816)  time: 0.0323  data: 0.0001  max mem: 2404
Epoch: [21]  [280/316]  eta: 0:00:01  lr: 0.000034  min_lr: 0.000034  loss: 0.4482 (0.4633)  class_acc: 0.7500 (0.7814)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1253 (4.1816)  time: 0.0321  data: 0.0001  max mem: 2404
Epoch: [21]  [290/316]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000034  loss: 0.4455 (0.4639)  class_acc: 0.8125 (0.7805)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1253 (4.1816)  time: 0.0319  data: 0.0001  max mem: 2404
Epoch: [21]  [300/316]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000034  loss: 0.4455 (0.4636)  class_acc: 0.8125 (0.7812)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2380 (5.1820)  time: 0.0299  data: 0.0001  max mem: 2404
Epoch: [21]  [310/316]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000034  loss: 0.4455 (0.4636)  class_acc: 0.8125 (0.7812)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2380 (5.1820)  time: 0.0140  data: 0.0001  max mem: 2404
Epoch: [21]  [315/316]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000034  loss: 0.4455 (0.4636)  class_acc: 0.8125 (0.7812)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2380 (5.1820)  time: 0.0062  data: 0.0000  max mem: 2404
Epoch: [21] Total time: 0:00:10 (0.0328 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000034  loss: 0.4455 (0.4636)  class_acc: 0.8125 (0.7812)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2380 (5.1820)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:16  loss: 0.6202 (0.6202)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2753  data: 0.2290  max mem: 2404
Val:  [10/60]  eta: 0:00:01  loss: 0.7938 (0.7961)  accuracy: 0.3333 (0.2765)  balanced_accuracy: 0.3125 (0.2440)  pr_auc: 0.4504 (0.3608)  roc_auc: 0.2929 (0.2460)  time: 0.0394  data: 0.0210  max mem: 2404
Val:  [20/60]  eta: 0:00:01  loss: 0.7938 (0.8177)  accuracy: 0.4583 (0.4008)  balanced_accuracy: 0.4000 (0.3359)  pr_auc: 0.6030 (0.5079)  roc_auc: 0.3426 (0.3048)  time: 0.0152  data: 0.0002  max mem: 2404
Val:  [30/60]  eta: 0:00:00  loss: 0.8124 (0.8228)  accuracy: 0.5417 (0.4503)  balanced_accuracy: 0.4375 (0.3808)  pr_auc: 0.6173 (0.5426)  roc_auc: 0.3672 (0.3384)  time: 0.0147  data: 0.0001  max mem: 2404
Val:  [40/60]  eta: 0:00:00  loss: 0.7922 (0.8205)  accuracy: 0.5417 (0.4736)  balanced_accuracy: 0.4688 (0.4092)  pr_auc: 0.6179 (0.5695)  roc_auc: 0.4500 (0.3813)  time: 0.0147  data: 0.0001  max mem: 2404
Val:  [50/60]  eta: 0:00:00  loss: 0.8348 (0.8463)  accuracy: 0.5000 (0.4706)  balanced_accuracy: 0.4583 (0.4163)  pr_auc: 0.6179 (0.5674)  roc_auc: 0.5000 (0.3939)  time: 0.0143  data: 0.0001  max mem: 2404
Val:  [59/60]  eta: 0:00:00  loss: 0.9794 (0.8681)  accuracy: 0.3750 (0.4354)  balanced_accuracy: 0.3750 (0.3880)  pr_auc: 0.4630 (0.5177)  roc_auc: 0.2917 (0.3663)  time: 0.0135  data: 0.0001  max mem: 2404
Val: Total time: 0:00:01 (0.0210 s / it)
* loss 0.868
Accuracy of the network on the 1431 test EEG: 0.50%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [22]  [  0/316]  eta: 0:01:26  lr: 0.000028  min_lr: 0.000028  loss: 0.3527 (0.3527)  class_acc: 0.8750 (0.8750)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2749  data: 0.2120  max mem: 2404
Epoch: [22]  [ 10/316]  eta: 0:00:17  lr: 0.000028  min_lr: 0.000028  loss: 0.4207 (0.4359)  class_acc: 0.8750 (0.8295)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0558  data: 0.0194  max mem: 2404
Epoch: [22]  [ 20/316]  eta: 0:00:13  lr: 0.000028  min_lr: 0.000028  loss: 0.3891 (0.4166)  class_acc: 0.8125 (0.8155)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0340  data: 0.0001  max mem: 2404
Epoch: [22]  [ 30/316]  eta: 0:00:11  lr: 0.000028  min_lr: 0.000028  loss: 0.3891 (0.4180)  class_acc: 0.8125 (0.8085)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0341  data: 0.0001  max mem: 2404
Epoch: [22]  [ 40/316]  eta: 0:00:11  lr: 0.000028  min_lr: 0.000028  loss: 0.4169 (0.4283)  class_acc: 0.8125 (0.8018)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0343  data: 0.0001  max mem: 2404
Epoch: [22]  [ 50/316]  eta: 0:00:10  lr: 0.000028  min_lr: 0.000028  loss: 0.4453 (0.4314)  class_acc: 0.8125 (0.8027)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0343  data: 0.0001  max mem: 2404
Epoch: [22]  [ 60/316]  eta: 0:00:09  lr: 0.000028  min_lr: 0.000028  loss: 0.4553 (0.4394)  class_acc: 0.8125 (0.8023)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0343  data: 0.0001  max mem: 2404
Epoch: [22]  [ 70/316]  eta: 0:00:09  lr: 0.000028  min_lr: 0.000028  loss: 0.4442 (0.4426)  class_acc: 0.8125 (0.8028)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0343  data: 0.0001  max mem: 2404
Epoch: [22]  [ 80/316]  eta: 0:00:08  lr: 0.000028  min_lr: 0.000028  loss: 0.4165 (0.4486)  class_acc: 0.7500 (0.7932)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0341  data: 0.0001  max mem: 2404
Epoch: [22]  [ 90/316]  eta: 0:00:08  lr: 0.000028  min_lr: 0.000028  loss: 0.5063 (0.4534)  class_acc: 0.7500 (0.7878)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0342  data: 0.0001  max mem: 2404
Epoch: [22]  [100/316]  eta: 0:00:07  lr: 0.000023  min_lr: 0.000023  loss: 0.4540 (0.4519)  class_acc: 0.7500 (0.7884)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9063 (5.9063)  time: 0.0338  data: 0.0001  max mem: 2404
Epoch: [22]  [110/316]  eta: 0:00:07  lr: 0.000023  min_lr: 0.000023  loss: 0.4702 (0.4536)  class_acc: 0.7500 (0.7855)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9063 (5.9063)  time: 0.0333  data: 0.0001  max mem: 2404
Epoch: [22]  [120/316]  eta: 0:00:07  lr: 0.000023  min_lr: 0.000023  loss: 0.4576 (0.4509)  class_acc: 0.7500 (0.7851)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9063 (5.9063)  time: 0.0330  data: 0.0001  max mem: 2404
Epoch: [22]  [130/316]  eta: 0:00:06  lr: 0.000023  min_lr: 0.000023  loss: 0.4395 (0.4546)  class_acc: 0.7500 (0.7853)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9063 (5.9063)  time: 0.0328  data: 0.0001  max mem: 2404
Epoch: [22]  [140/316]  eta: 0:00:06  lr: 0.000023  min_lr: 0.000023  loss: 0.4788 (0.4511)  class_acc: 0.8125 (0.7863)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9063 (5.9063)  time: 0.0325  data: 0.0001  max mem: 2404
Epoch: [22]  [150/316]  eta: 0:00:05  lr: 0.000023  min_lr: 0.000023  loss: 0.4535 (0.4527)  class_acc: 0.8125 (0.7835)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9063 (5.9063)  time: 0.0320  data: 0.0001  max mem: 2404
Epoch: [22]  [160/316]  eta: 0:00:05  lr: 0.000023  min_lr: 0.000023  loss: 0.4338 (0.4508)  class_acc: 0.8125 (0.7861)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9063 (5.9063)  time: 0.0318  data: 0.0001  max mem: 2404
Epoch: [22]  [170/316]  eta: 0:00:05  lr: 0.000023  min_lr: 0.000023  loss: 0.4288 (0.4486)  class_acc: 0.8750 (0.7895)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9063 (5.9063)  time: 0.0318  data: 0.0001  max mem: 2404
Epoch: [22]  [180/316]  eta: 0:00:04  lr: 0.000023  min_lr: 0.000023  loss: 0.4292 (0.4514)  class_acc: 0.8125 (0.7866)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9063 (5.9063)  time: 0.0319  data: 0.0001  max mem: 2404
Epoch: [22]  [190/316]  eta: 0:00:04  lr: 0.000023  min_lr: 0.000023  loss: 0.4419 (0.4515)  class_acc: 0.8125 (0.7863)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9063 (5.9063)  time: 0.0321  data: 0.0001  max mem: 2404
Epoch: [22]  [200/316]  eta: 0:00:03  lr: 0.000018  min_lr: 0.000018  loss: 0.4375 (0.4516)  class_acc: 0.8125 (0.7864)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2528 (5.0796)  time: 0.0327  data: 0.0001  max mem: 2404
Epoch: [22]  [210/316]  eta: 0:00:03  lr: 0.000018  min_lr: 0.000018  loss: 0.4146 (0.4519)  class_acc: 0.8125 (0.7864)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2528 (5.0796)  time: 0.0330  data: 0.0001  max mem: 2404
Epoch: [22]  [220/316]  eta: 0:00:03  lr: 0.000018  min_lr: 0.000018  loss: 0.4190 (0.4507)  class_acc: 0.8125 (0.7873)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2528 (5.0796)  time: 0.0329  data: 0.0001  max mem: 2404
Epoch: [22]  [230/316]  eta: 0:00:02  lr: 0.000018  min_lr: 0.000018  loss: 0.4207 (0.4513)  class_acc: 0.8125 (0.7876)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2528 (5.0796)  time: 0.0328  data: 0.0001  max mem: 2404
Epoch: [22]  [240/316]  eta: 0:00:02  lr: 0.000018  min_lr: 0.000018  loss: 0.4143 (0.4501)  class_acc: 0.8125 (0.7881)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2528 (5.0796)  time: 0.0328  data: 0.0001  max mem: 2404
Epoch: [22]  [250/316]  eta: 0:00:02  lr: 0.000018  min_lr: 0.000018  loss: 0.4097 (0.4508)  class_acc: 0.8125 (0.7881)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2528 (5.0796)  time: 0.0329  data: 0.0001  max mem: 2404
Epoch: [22]  [260/316]  eta: 0:00:01  lr: 0.000018  min_lr: 0.000018  loss: 0.4552 (0.4507)  class_acc: 0.7500 (0.7888)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2528 (5.0796)  time: 0.0327  data: 0.0002  max mem: 2404
Epoch: [22]  [270/316]  eta: 0:00:01  lr: 0.000018  min_lr: 0.000018  loss: 0.4744 (0.4519)  class_acc: 0.7500 (0.7864)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2528 (5.0796)  time: 0.0327  data: 0.0001  max mem: 2404
Epoch: [22]  [280/316]  eta: 0:00:01  lr: 0.000018  min_lr: 0.000018  loss: 0.4409 (0.4506)  class_acc: 0.7500 (0.7874)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2528 (5.0796)  time: 0.0331  data: 0.0001  max mem: 2404
Epoch: [22]  [290/316]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000018  loss: 0.4081 (0.4512)  class_acc: 0.8125 (0.7872)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2528 (5.0796)  time: 0.0320  data: 0.0001  max mem: 2404
Epoch: [22]  [300/316]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000018  loss: 0.4562 (0.4517)  class_acc: 0.8125 (0.7871)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5088 (5.2226)  time: 0.0296  data: 0.0001  max mem: 2404
Epoch: [22]  [310/316]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000018  loss: 0.4562 (0.4517)  class_acc: 0.8125 (0.7871)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5088 (5.2226)  time: 0.0142  data: 0.0001  max mem: 2404
Epoch: [22]  [315/316]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000018  loss: 0.4562 (0.4517)  class_acc: 0.8125 (0.7871)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5088 (5.2226)  time: 0.0062  data: 0.0000  max mem: 2404
Epoch: [22] Total time: 0:00:10 (0.0326 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000018  loss: 0.4562 (0.4517)  class_acc: 0.8125 (0.7871)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5088 (5.2226)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:14  loss: 0.6579 (0.6579)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2444  data: 0.2222  max mem: 2404
Val:  [10/60]  eta: 0:00:01  loss: 0.7978 (0.8099)  accuracy: 0.3750 (0.2765)  balanced_accuracy: 0.3125 (0.2471)  pr_auc: 0.4504 (0.3601)  roc_auc: 0.2857 (0.2448)  time: 0.0376  data: 0.0207  max mem: 2404
Val:  [20/60]  eta: 0:00:01  loss: 0.7978 (0.8294)  accuracy: 0.4167 (0.3968)  balanced_accuracy: 0.4056 (0.3373)  pr_auc: 0.5948 (0.5088)  roc_auc: 0.3426 (0.3062)  time: 0.0157  data: 0.0003  max mem: 2404
Val:  [30/60]  eta: 0:00:00  loss: 0.8127 (0.8307)  accuracy: 0.5417 (0.4449)  balanced_accuracy: 0.4222 (0.3813)  pr_auc: 0.6277 (0.5437)  roc_auc: 0.3667 (0.3399)  time: 0.0146  data: 0.0001  max mem: 2404
Val:  [40/60]  eta: 0:00:00  loss: 0.7931 (0.8258)  accuracy: 0.5417 (0.4695)  balanced_accuracy: 0.4714 (0.4100)  pr_auc: 0.6198 (0.5700)  roc_auc: 0.4357 (0.3818)  time: 0.0145  data: 0.0001  max mem: 2404
Val:  [50/60]  eta: 0:00:00  loss: 0.8298 (0.8496)  accuracy: 0.5000 (0.4698)  balanced_accuracy: 0.4714 (0.4197)  pr_auc: 0.6155 (0.5675)  roc_auc: 0.5000 (0.3937)  time: 0.0141  data: 0.0001  max mem: 2404
Val:  [59/60]  eta: 0:00:00  loss: 0.9468 (0.8660)  accuracy: 0.4167 (0.4375)  balanced_accuracy: 0.4167 (0.3933)  pr_auc: 0.4630 (0.5180)  roc_auc: 0.2917 (0.3669)  time: 0.0135  data: 0.0001  max mem: 2404
Val: Total time: 0:00:01 (0.0207 s / it)
* loss 0.866
Accuracy of the network on the 1431 test EEG: 0.51%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [23]  [  0/316]  eta: 0:01:27  lr: 0.000013  min_lr: 0.000013  loss: 0.4588 (0.4588)  class_acc: 0.8125 (0.8125)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2772  data: 0.2416  max mem: 2404
Epoch: [23]  [ 10/316]  eta: 0:00:16  lr: 0.000013  min_lr: 0.000013  loss: 0.5003 (0.4978)  class_acc: 0.8125 (0.7443)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0534  data: 0.0221  max mem: 2404
Epoch: [23]  [ 20/316]  eta: 0:00:12  lr: 0.000013  min_lr: 0.000013  loss: 0.4418 (0.4393)  class_acc: 0.8125 (0.8006)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0312  data: 0.0002  max mem: 2404
Epoch: [23]  [ 30/316]  eta: 0:00:11  lr: 0.000013  min_lr: 0.000013  loss: 0.4091 (0.4254)  class_acc: 0.8125 (0.8065)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0311  data: 0.0001  max mem: 2404
Epoch: [23]  [ 40/316]  eta: 0:00:10  lr: 0.000013  min_lr: 0.000013  loss: 0.4231 (0.4378)  class_acc: 0.7500 (0.7912)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0310  data: 0.0001  max mem: 2404
Epoch: [23]  [ 50/316]  eta: 0:00:09  lr: 0.000013  min_lr: 0.000013  loss: 0.4140 (0.4417)  class_acc: 0.8125 (0.7917)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0310  data: 0.0001  max mem: 2404
Epoch: [23]  [ 60/316]  eta: 0:00:08  lr: 0.000013  min_lr: 0.000013  loss: 0.4680 (0.4519)  class_acc: 0.8125 (0.7859)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0312  data: 0.0001  max mem: 2404
Epoch: [23]  [ 70/316]  eta: 0:00:08  lr: 0.000013  min_lr: 0.000013  loss: 0.4730 (0.4596)  class_acc: 0.7500 (0.7808)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0314  data: 0.0001  max mem: 2404
Epoch: [23]  [ 80/316]  eta: 0:00:08  lr: 0.000013  min_lr: 0.000013  loss: 0.4485 (0.4600)  class_acc: 0.7500 (0.7770)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0314  data: 0.0001  max mem: 2404
Epoch: [23]  [ 90/316]  eta: 0:00:07  lr: 0.000013  min_lr: 0.000013  loss: 0.4365 (0.4589)  class_acc: 0.8125 (0.7802)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0314  data: 0.0001  max mem: 2404
Epoch: [23]  [100/316]  eta: 0:00:07  lr: 0.000010  min_lr: 0.000010  loss: 0.4160 (0.4557)  class_acc: 0.8125 (0.7847)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1465 (4.1465)  time: 0.0321  data: 0.0001  max mem: 2404
Epoch: [23]  [110/316]  eta: 0:00:06  lr: 0.000010  min_lr: 0.000010  loss: 0.4100 (0.4587)  class_acc: 0.8125 (0.7815)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1465 (4.1465)  time: 0.0325  data: 0.0001  max mem: 2404
Epoch: [23]  [120/316]  eta: 0:00:06  lr: 0.000010  min_lr: 0.000010  loss: 0.4338 (0.4561)  class_acc: 0.7500 (0.7851)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1465 (4.1465)  time: 0.0321  data: 0.0001  max mem: 2404
Epoch: [23]  [130/316]  eta: 0:00:06  lr: 0.000010  min_lr: 0.000010  loss: 0.4650 (0.4570)  class_acc: 0.8125 (0.7858)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1465 (4.1465)  time: 0.0322  data: 0.0001  max mem: 2404
Epoch: [23]  [140/316]  eta: 0:00:05  lr: 0.000010  min_lr: 0.000010  loss: 0.4685 (0.4578)  class_acc: 0.7500 (0.7859)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1465 (4.1465)  time: 0.0325  data: 0.0001  max mem: 2404
Epoch: [23]  [150/316]  eta: 0:00:05  lr: 0.000010  min_lr: 0.000010  loss: 0.4331 (0.4557)  class_acc: 0.7500 (0.7844)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1465 (4.1465)  time: 0.0326  data: 0.0001  max mem: 2404
Epoch: [23]  [160/316]  eta: 0:00:05  lr: 0.000010  min_lr: 0.000010  loss: 0.4292 (0.4542)  class_acc: 0.7500 (0.7845)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1465 (4.1465)  time: 0.0325  data: 0.0001  max mem: 2404
Epoch: [23]  [170/316]  eta: 0:00:04  lr: 0.000010  min_lr: 0.000010  loss: 0.4295 (0.4529)  class_acc: 0.7500 (0.7840)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1465 (4.1465)  time: 0.0327  data: 0.0001  max mem: 2404
Epoch: [23]  [180/316]  eta: 0:00:04  lr: 0.000010  min_lr: 0.000010  loss: 0.4295 (0.4519)  class_acc: 0.8125 (0.7852)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1465 (4.1465)  time: 0.0331  data: 0.0001  max mem: 2404
Epoch: [23]  [190/316]  eta: 0:00:04  lr: 0.000010  min_lr: 0.000010  loss: 0.4355 (0.4542)  class_acc: 0.8125 (0.7844)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1465 (4.1465)  time: 0.0331  data: 0.0001  max mem: 2404
Epoch: [23]  [200/316]  eta: 0:00:03  lr: 0.000006  min_lr: 0.000006  loss: 0.4416 (0.4560)  class_acc: 0.7500 (0.7839)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1465 (4.2401)  time: 0.0332  data: 0.0001  max mem: 2404
Epoch: [23]  [210/316]  eta: 0:00:03  lr: 0.000006  min_lr: 0.000006  loss: 0.4465 (0.4561)  class_acc: 0.7500 (0.7838)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1465 (4.2401)  time: 0.0332  data: 0.0001  max mem: 2404
Epoch: [23]  [220/316]  eta: 0:00:03  lr: 0.000006  min_lr: 0.000006  loss: 0.4465 (0.4562)  class_acc: 0.7500 (0.7856)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1465 (4.2401)  time: 0.0326  data: 0.0001  max mem: 2404
Epoch: [23]  [230/316]  eta: 0:00:02  lr: 0.000006  min_lr: 0.000006  loss: 0.4462 (0.4558)  class_acc: 0.7500 (0.7865)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1465 (4.2401)  time: 0.0320  data: 0.0001  max mem: 2404
Epoch: [23]  [240/316]  eta: 0:00:02  lr: 0.000006  min_lr: 0.000006  loss: 0.4385 (0.4545)  class_acc: 0.7500 (0.7860)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1465 (4.2401)  time: 0.0319  data: 0.0001  max mem: 2404
Epoch: [23]  [250/316]  eta: 0:00:02  lr: 0.000006  min_lr: 0.000006  loss: 0.4412 (0.4555)  class_acc: 0.7500 (0.7854)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1465 (4.2401)  time: 0.0320  data: 0.0001  max mem: 2404
Epoch: [23]  [260/316]  eta: 0:00:01  lr: 0.000006  min_lr: 0.000006  loss: 0.3989 (0.4538)  class_acc: 0.7500 (0.7862)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1465 (4.2401)  time: 0.0321  data: 0.0001  max mem: 2404
Epoch: [23]  [270/316]  eta: 0:00:01  lr: 0.000006  min_lr: 0.000006  loss: 0.3989 (0.4539)  class_acc: 0.8125 (0.7860)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1465 (4.2401)  time: 0.0319  data: 0.0001  max mem: 2404
Epoch: [23]  [280/316]  eta: 0:00:01  lr: 0.000006  min_lr: 0.000006  loss: 0.3781 (0.4508)  class_acc: 0.8125 (0.7880)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1465 (4.2401)  time: 0.0316  data: 0.0001  max mem: 2404
Epoch: [23]  [290/316]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000006  loss: 0.3787 (0.4509)  class_acc: 0.8125 (0.7878)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1465 (4.2401)  time: 0.0313  data: 0.0001  max mem: 2404
Epoch: [23]  [300/316]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000006  loss: 0.4185 (0.4509)  class_acc: 0.7500 (0.7877)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3336 (4.3417)  time: 0.0295  data: 0.0001  max mem: 2404
Epoch: [23]  [310/316]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000006  loss: 0.4185 (0.4509)  class_acc: 0.7500 (0.7877)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3336 (4.3417)  time: 0.0139  data: 0.0001  max mem: 2404
Epoch: [23]  [315/316]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000006  loss: 0.4185 (0.4509)  class_acc: 0.7500 (0.7877)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3336 (4.3417)  time: 0.0062  data: 0.0000  max mem: 2404
Epoch: [23] Total time: 0:00:09 (0.0316 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000006  loss: 0.4185 (0.4509)  class_acc: 0.7500 (0.7877)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3336 (4.3417)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/60]  eta: 0:00:16  loss: 0.6758 (0.6758)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2828  data: 0.2350  max mem: 2404
Val:  [10/60]  eta: 0:00:01  loss: 0.8006 (0.8167)  accuracy: 0.3750 (0.2727)  balanced_accuracy: 0.2812 (0.2519)  pr_auc: 0.4504 (0.3592)  roc_auc: 0.2857 (0.2435)  time: 0.0397  data: 0.0216  max mem: 2404
Val:  [20/60]  eta: 0:00:01  loss: 0.8043 (0.8352)  accuracy: 0.4167 (0.3929)  balanced_accuracy: 0.4000 (0.3418)  pr_auc: 0.5835 (0.5082)  roc_auc: 0.3426 (0.3055)  time: 0.0151  data: 0.0002  max mem: 2404
Val:  [30/60]  eta: 0:00:00  loss: 0.8137 (0.8347)  accuracy: 0.5417 (0.4368)  balanced_accuracy: 0.4222 (0.3800)  pr_auc: 0.6233 (0.5431)  roc_auc: 0.3630 (0.3394)  time: 0.0146  data: 0.0001  max mem: 2404
Val:  [40/60]  eta: 0:00:00  loss: 0.7946 (0.8286)  accuracy: 0.5417 (0.4614)  balanced_accuracy: 0.4714 (0.4072)  pr_auc: 0.6198 (0.5694)  roc_auc: 0.4357 (0.3810)  time: 0.0143  data: 0.0001  max mem: 2404
Val:  [50/60]  eta: 0:00:00  loss: 0.8276 (0.8515)  accuracy: 0.5000 (0.4608)  balanced_accuracy: 0.4583 (0.4151)  pr_auc: 0.6155 (0.5667)  roc_auc: 0.4929 (0.3925)  time: 0.0141  data: 0.0001  max mem: 2404
Val:  [59/60]  eta: 0:00:00  loss: 0.9258 (0.8655)  accuracy: 0.3750 (0.4284)  balanced_accuracy: 0.3881 (0.3876)  pr_auc: 0.4630 (0.5175)  roc_auc: 0.2917 (0.3659)  time: 0.0135  data: 0.0001  max mem: 2404
Val: Total time: 0:00:01 (0.0210 s / it)
* loss 0.866
Accuracy of the network on the 1431 test EEG: 0.50%
Max accuracy val: 0.58%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [24]  [  0/316]  eta: 0:01:08  lr: 0.000004  min_lr: 0.000004  loss: 0.3434 (0.3434)  class_acc: 0.8750 (0.8750)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2182  data: 0.1717  max mem: 2404
Epoch: [24]  [ 10/316]  eta: 0:00:15  lr: 0.000004  min_lr: 0.000004  loss: 0.3600 (0.4006)  class_acc: 0.8750 (0.8523)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0504  data: 0.0183  max mem: 2404
Epoch: [24]  [ 20/316]  eta: 0:00:12  lr: 0.000004  min_lr: 0.000004  loss: 0.3794 (0.4141)  class_acc: 0.8125 (0.8333)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0322  data: 0.0016  max mem: 2404
Epoch: [24]  [ 30/316]  eta: 0:00:10  lr: 0.000004  min_lr: 0.000004  loss: 0.4150 (0.4088)  class_acc: 0.7500 (0.8185)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0309  data: 0.0002  max mem: 2404
Epoch: [24]  [ 40/316]  eta: 0:00:09  lr: 0.000004  min_lr: 0.000004  loss: 0.4226 (0.4217)  class_acc: 0.7500 (0.8064)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0308  data: 0.0002  max mem: 2404
Epoch: [24]  [ 50/316]  eta: 0:00:09  lr: 0.000004  min_lr: 0.000004  loss: 0.4468 (0.4304)  class_acc: 0.7500 (0.8027)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0309  data: 0.0001  max mem: 2404
Epoch: [24]  [ 60/316]  eta: 0:00:08  lr: 0.000004  min_lr: 0.000004  loss: 0.5352 (0.4510)  class_acc: 0.7500 (0.7838)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0312  data: 0.0001  max mem: 2404
Epoch: [24]  [ 70/316]  eta: 0:00:08  lr: 0.000004  min_lr: 0.000004  loss: 0.4851 (0.4550)  class_acc: 0.7500 (0.7843)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0317  data: 0.0001  max mem: 2404
Epoch: [24]  [ 80/316]  eta: 0:00:08  lr: 0.000004  min_lr: 0.000004  loss: 0.4563 (0.4543)  class_acc: 0.7500 (0.7832)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0323  data: 0.0001  max mem: 2404
Epoch: [24]  [ 90/316]  eta: 0:00:07  lr: 0.000004  min_lr: 0.000004  loss: 0.4584 (0.4568)  class_acc: 0.7500 (0.7809)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0325  data: 0.0001  max mem: 2404
Epoch: [24]  [100/316]  eta: 0:00:07  lr: 0.000002  min_lr: 0.000002  loss: 0.4584 (0.4535)  class_acc: 0.8125 (0.7834)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.2200)  time: 0.0328  data: 0.0001  max mem: 2404
Epoch: [24]  [110/316]  eta: 0:00:06  lr: 0.000002  min_lr: 0.000002  loss: 0.3991 (0.4519)  class_acc: 0.8125 (0.7855)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.2200)  time: 0.0329  data: 0.0001  max mem: 2404
Epoch: [24]  [120/316]  eta: 0:00:06  lr: 0.000002  min_lr: 0.000002  loss: 0.4288 (0.4469)  class_acc: 0.8125 (0.7913)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.2200)  time: 0.0327  data: 0.0001  max mem: 2404
Epoch: [24]  [130/316]  eta: 0:00:06  lr: 0.000002  min_lr: 0.000002  loss: 0.4542 (0.4499)  class_acc: 0.8125 (0.7915)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.2200)  time: 0.0327  data: 0.0001  max mem: 2404
Epoch: [24]  [140/316]  eta: 0:00:05  lr: 0.000002  min_lr: 0.000002  loss: 0.4571 (0.4488)  class_acc: 0.8125 (0.7908)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.2200)  time: 0.0331  data: 0.0001  max mem: 2404
Epoch: [24]  [150/316]  eta: 0:00:05  lr: 0.000002  min_lr: 0.000002  loss: 0.4350 (0.4519)  class_acc: 0.7500 (0.7885)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.2200)  time: 0.0335  data: 0.0001  max mem: 2404
Epoch: [24]  [160/316]  eta: 0:00:05  lr: 0.000002  min_lr: 0.000002  loss: 0.4548 (0.4502)  class_acc: 0.7500 (0.7908)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.2200)  time: 0.0336  data: 0.0001  max mem: 2404
Epoch: [24]  [170/316]  eta: 0:00:04  lr: 0.000002  min_lr: 0.000002  loss: 0.4322 (0.4470)  class_acc: 0.8125 (0.7909)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.2200)  time: 0.0325  data: 0.0001  max mem: 2404
Epoch: [24]  [180/316]  eta: 0:00:04  lr: 0.000002  min_lr: 0.000002  loss: 0.4064 (0.4476)  class_acc: 0.8125 (0.7907)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.2200)  time: 0.0315  data: 0.0001  max mem: 2404
Epoch: [24]  [190/316]  eta: 0:00:04  lr: 0.000002  min_lr: 0.000002  loss: 0.4538 (0.4472)  class_acc: 0.7500 (0.7906)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.2200)  time: 0.0319  data: 0.0001  max mem: 2404
Epoch: [24]  [200/316]  eta: 0:00:03  lr: 0.000001  min_lr: 0.000001  loss: 0.4538 (0.4486)  class_acc: 0.7500 (0.7889)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.7040)  time: 0.0321  data: 0.0001  max mem: 2404
Epoch: [24]  [210/316]  eta: 0:00:03  lr: 0.000001  min_lr: 0.000001  loss: 0.4442 (0.4467)  class_acc: 0.8125 (0.7903)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.7040)  time: 0.0317  data: 0.0001  max mem: 2404
Epoch: [24]  [220/316]  eta: 0:00:03  lr: 0.000001  min_lr: 0.000001  loss: 0.4603 (0.4481)  class_acc: 0.8125 (0.7890)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.7040)  time: 0.0314  data: 0.0001  max mem: 2404
Epoch: [24]  [230/316]  eta: 0:00:02  lr: 0.000001  min_lr: 0.000001  loss: 0.4336 (0.4482)  class_acc: 0.7500 (0.7881)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.7040)  time: 0.0315  data: 0.0001  max mem: 2404
Epoch: [24]  [240/316]  eta: 0:00:02  lr: 0.000001  min_lr: 0.000001  loss: 0.4087 (0.4474)  class_acc: 0.7500 (0.7884)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.7040)  time: 0.0319  data: 0.0001  max mem: 2404
Epoch: [24]  [250/316]  eta: 0:00:02  lr: 0.000001  min_lr: 0.000001  loss: 0.4625 (0.4494)  class_acc: 0.7500 (0.7871)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.7040)  time: 0.0321  data: 0.0002  max mem: 2404
Epoch: [24]  [260/316]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 0.4639 (0.4494)  class_acc: 0.7500 (0.7883)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.7040)  time: 0.0319  data: 0.0002  max mem: 2404
Epoch: [24]  [270/316]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 0.4404 (0.4489)  class_acc: 0.8125 (0.7883)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.7040)  time: 0.0318  data: 0.0001  max mem: 2404
Epoch: [24]  [280/316]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 0.4173 (0.4467)  class_acc: 0.8125 (0.7905)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.7040)  time: 0.0314  data: 0.0001  max mem: 2404
Epoch: [24]  [290/316]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 0.4173 (0.4471)  class_acc: 0.8125 (0.7899)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.7040)  time: 0.0311  data: 0.0002  max mem: 2404
Epoch: [24]  [300/316]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 0.4358 (0.4474)  class_acc: 0.8125 (0.7904)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.5045)  time: 0.0295  data: 0.0001  max mem: 2404
Epoch: [24]  [310/316]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 0.4358 (0.4474)  class_acc: 0.8125 (0.7904)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.5045)  time: 0.0140  data: 0.0001  max mem: 2404
Epoch: [24]  [315/316]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 0.4358 (0.4474)  class_acc: 0.8125 (0.7904)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.5045)  time: 0.0062  data: 0.0001  max mem: 2404
Epoch: [24] Total time: 0:00:09 (0.0315 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 0.4358 (0.4474)  class_acc: 0.8125 (0.7904)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.5045)
Val:  [ 0/60]  eta: 0:00:14  loss: 0.6793 (0.6793)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2479  data: 0.2227  max mem: 2404
Val:  [10/60]  eta: 0:00:01  loss: 0.8010 (0.8179)  accuracy: 0.3750 (0.2727)  balanced_accuracy: 0.2812 (0.2519)  pr_auc: 0.4504 (0.3592)  roc_auc: 0.2857 (0.2435)  time: 0.0393  data: 0.0220  max mem: 2404
Val:  [20/60]  eta: 0:00:01  loss: 0.8062 (0.8363)  accuracy: 0.4167 (0.3929)  balanced_accuracy: 0.4000 (0.3418)  pr_auc: 0.5835 (0.5082)  roc_auc: 0.3407 (0.3058)  time: 0.0167  data: 0.0010  max mem: 2404
Val:  [30/60]  eta: 0:00:00  loss: 0.8139 (0.8355)  accuracy: 0.5417 (0.4355)  balanced_accuracy: 0.4222 (0.3790)  pr_auc: 0.6256 (0.5431)  roc_auc: 0.3630 (0.3391)  time: 0.0148  data: 0.0002  max mem: 2404
Val:  [40/60]  eta: 0:00:00  loss: 0.7951 (0.8292)  accuracy: 0.5417 (0.4604)  balanced_accuracy: 0.4714 (0.4065)  pr_auc: 0.6198 (0.5695)  roc_auc: 0.4357 (0.3807)  time: 0.0147  data: 0.0002  max mem: 2404
Val:  [50/60]  eta: 0:00:00  loss: 0.8273 (0.8520)  accuracy: 0.5000 (0.4608)  balanced_accuracy: 0.4714 (0.4153)  pr_auc: 0.6155 (0.5668)  roc_auc: 0.4929 (0.3921)  time: 0.0143  data: 0.0001  max mem: 2404
Val:  [59/60]  eta: 0:00:00  loss: 0.9217 (0.8655)  accuracy: 0.3750 (0.4277)  balanced_accuracy: 0.3750 (0.3868)  pr_auc: 0.4630 (0.5176)  roc_auc: 0.2917 (0.3658)  time: 0.0136  data: 0.0001  max mem: 2404
Val: Total time: 0:00:01 (0.0209 s / it)
* loss 0.866
Accuracy of the network on the 1431 test EEG: 0.50%
Max accuracy val: 0.58%
Training time 0:04:56
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/home/chntzi001/deepEEG/EEGPT/downstream_tueg/utils.py:505: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
Namespace(batch_size=16, epochs=25, update_freq=100, save_ckpt_freq=0, robust_test=None, model='EEGPT', qkv_bias=False, rel_pos_bias=False, abs_pos_emb=True, layer_scale_init_value=0.1, input_size=200, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.0005, layer_decay=0.65, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, smoothing=0.1, reprob=0.25, remode='pixel', recount=1, resplit=False, finetune='/home/chntzi001/deepEEG/EEGPT/downstream_tueg/Checkpoints/eegpt_mcae_58chs_4s_large4E.ckpt', model_key='model|module|state_dict', model_prefix='', model_filter_name='gzp', init_scale=0.001, use_mean_pooling=True, disable_weight_decay_on_rel_pos_bias=False, nb_classes=0, output_dir='./checkpoints/finetune_quero_eegpt/fold_2', log_dir='./log/finetune_quero_eegpt/fold_2', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', enable_deepspeed=False, dataset='QUERO', fold=2, kfoldcrossval=True, distributed=False)
5037 1463
5037 1463
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f61fc7c3a60>
Patch size = 200
Load ckpt from /home/chntzi001/deepEEG/EEGPT/downstream_tueg/Checkpoints/eegpt_mcae_58chs_4s_large4E.ckpt
Weights of EEGPTClassifier not initialized from pretrained model: ['chan_conv.0.weight', 'chan_conv.0.bias', 'chan_conv.1.weight', 'chan_conv.1.bias', 'chan_conv.1.running_mean', 'chan_conv.1.running_var', 'chan_conv.3.weight', 'chan_conv.3.bias', 'chan_conv.4.weight', 'chan_conv.4.bias', 'chan_conv.4.running_mean', 'chan_conv.4.running_var', 'reconstructor.cls_token', 'fc_norm.weight', 'fc_norm.bias', 'head.1.weight', 'head.1.bias']
Weights from pretrained model not used in EEGPTClassifier: ['encoder.summary_token', 'encoder.patch_embed.proj.weight', 'encoder.patch_embed.proj.bias', 'encoder.chan_embed.weight', 'encoder.blocks.0.norm1.weight', 'encoder.blocks.0.norm1.bias', 'encoder.blocks.0.attn.qkv.weight', 'encoder.blocks.0.attn.qkv.bias', 'encoder.blocks.0.attn.proj.weight', 'encoder.blocks.0.attn.proj.bias', 'encoder.blocks.0.norm2.weight', 'encoder.blocks.0.norm2.bias', 'encoder.blocks.0.mlp.fc1.weight', 'encoder.blocks.0.mlp.fc1.bias', 'encoder.blocks.0.mlp.fc2.weight', 'encoder.blocks.0.mlp.fc2.bias', 'encoder.blocks.1.norm1.weight', 'encoder.blocks.1.norm1.bias', 'encoder.blocks.1.attn.qkv.weight', 'encoder.blocks.1.attn.qkv.bias', 'encoder.blocks.1.attn.proj.weight', 'encoder.blocks.1.attn.proj.bias', 'encoder.blocks.1.norm2.weight', 'encoder.blocks.1.norm2.bias', 'encoder.blocks.1.mlp.fc1.weight', 'encoder.blocks.1.mlp.fc1.bias', 'encoder.blocks.1.mlp.fc2.weight', 'encoder.blocks.1.mlp.fc2.bias', 'encoder.blocks.2.norm1.weight', 'encoder.blocks.2.norm1.bias', 'encoder.blocks.2.attn.qkv.weight', 'encoder.blocks.2.attn.qkv.bias', 'encoder.blocks.2.attn.proj.weight', 'encoder.blocks.2.attn.proj.bias', 'encoder.blocks.2.norm2.weight', 'encoder.blocks.2.norm2.bias', 'encoder.blocks.2.mlp.fc1.weight', 'encoder.blocks.2.mlp.fc1.bias', 'encoder.blocks.2.mlp.fc2.weight', 'encoder.blocks.2.mlp.fc2.bias', 'encoder.blocks.3.norm1.weight', 'encoder.blocks.3.norm1.bias', 'encoder.blocks.3.attn.qkv.weight', 'encoder.blocks.3.attn.qkv.bias', 'encoder.blocks.3.attn.proj.weight', 'encoder.blocks.3.attn.proj.bias', 'encoder.blocks.3.norm2.weight', 'encoder.blocks.3.norm2.bias', 'encoder.blocks.3.mlp.fc1.weight', 'encoder.blocks.3.mlp.fc1.bias', 'encoder.blocks.3.mlp.fc2.weight', 'encoder.blocks.3.mlp.fc2.bias', 'encoder.blocks.4.norm1.weight', 'encoder.blocks.4.norm1.bias', 'encoder.blocks.4.attn.qkv.weight', 'encoder.blocks.4.attn.qkv.bias', 'encoder.blocks.4.attn.proj.weight', 'encoder.blocks.4.attn.proj.bias', 'encoder.blocks.4.norm2.weight', 'encoder.blocks.4.norm2.bias', 'encoder.blocks.4.mlp.fc1.weight', 'encoder.blocks.4.mlp.fc1.bias', 'encoder.blocks.4.mlp.fc2.weight', 'encoder.blocks.4.mlp.fc2.bias', 'encoder.blocks.5.norm1.weight', 'encoder.blocks.5.norm1.bias', 'encoder.blocks.5.attn.qkv.weight', 'encoder.blocks.5.attn.qkv.bias', 'encoder.blocks.5.attn.proj.weight', 'encoder.blocks.5.attn.proj.bias', 'encoder.blocks.5.norm2.weight', 'encoder.blocks.5.norm2.bias', 'encoder.blocks.5.mlp.fc1.weight', 'encoder.blocks.5.mlp.fc1.bias', 'encoder.blocks.5.mlp.fc2.weight', 'encoder.blocks.5.mlp.fc2.bias', 'encoder.blocks.6.norm1.weight', 'encoder.blocks.6.norm1.bias', 'encoder.blocks.6.attn.qkv.weight', 'encoder.blocks.6.attn.qkv.bias', 'encoder.blocks.6.attn.proj.weight', 'encoder.blocks.6.attn.proj.bias', 'encoder.blocks.6.norm2.weight', 'encoder.blocks.6.norm2.bias', 'encoder.blocks.6.mlp.fc1.weight', 'encoder.blocks.6.mlp.fc1.bias', 'encoder.blocks.6.mlp.fc2.weight', 'encoder.blocks.6.mlp.fc2.bias', 'encoder.blocks.7.norm1.weight', 'encoder.blocks.7.norm1.bias', 'encoder.blocks.7.attn.qkv.weight', 'encoder.blocks.7.attn.qkv.bias', 'encoder.blocks.7.attn.proj.weight', 'encoder.blocks.7.attn.proj.bias', 'encoder.blocks.7.norm2.weight', 'encoder.blocks.7.norm2.bias', 'encoder.blocks.7.mlp.fc1.weight', 'encoder.blocks.7.mlp.fc1.bias', 'encoder.blocks.7.mlp.fc2.weight', 'encoder.blocks.7.mlp.fc2.bias', 'encoder.norm.weight', 'encoder.norm.bias', 'predictor.mask_token', 'predictor.predictor_embed.weight', 'predictor.predictor_embed.bias', 'predictor.time_embed.freqs', 'predictor.predictor_blocks.0.norm1.weight', 'predictor.predictor_blocks.0.norm1.bias', 'predictor.predictor_blocks.0.attn.qkv.weight', 'predictor.predictor_blocks.0.attn.qkv.bias', 'predictor.predictor_blocks.0.attn.proj.weight', 'predictor.predictor_blocks.0.attn.proj.bias', 'predictor.predictor_blocks.0.norm2.weight', 'predictor.predictor_blocks.0.norm2.bias', 'predictor.predictor_blocks.0.mlp.fc1.weight', 'predictor.predictor_blocks.0.mlp.fc1.bias', 'predictor.predictor_blocks.0.mlp.fc2.weight', 'predictor.predictor_blocks.0.mlp.fc2.bias', 'predictor.predictor_blocks.1.norm1.weight', 'predictor.predictor_blocks.1.norm1.bias', 'predictor.predictor_blocks.1.attn.qkv.weight', 'predictor.predictor_blocks.1.attn.qkv.bias', 'predictor.predictor_blocks.1.attn.proj.weight', 'predictor.predictor_blocks.1.attn.proj.bias', 'predictor.predictor_blocks.1.norm2.weight', 'predictor.predictor_blocks.1.norm2.bias', 'predictor.predictor_blocks.1.mlp.fc1.weight', 'predictor.predictor_blocks.1.mlp.fc1.bias', 'predictor.predictor_blocks.1.mlp.fc2.weight', 'predictor.predictor_blocks.1.mlp.fc2.bias', 'predictor.predictor_blocks.2.norm1.weight', 'predictor.predictor_blocks.2.norm1.bias', 'predictor.predictor_blocks.2.attn.qkv.weight', 'predictor.predictor_blocks.2.attn.qkv.bias', 'predictor.predictor_blocks.2.attn.proj.weight', 'predictor.predictor_blocks.2.attn.proj.bias', 'predictor.predictor_blocks.2.norm2.weight', 'predictor.predictor_blocks.2.norm2.bias', 'predictor.predictor_blocks.2.mlp.fc1.weight', 'predictor.predictor_blocks.2.mlp.fc1.bias', 'predictor.predictor_blocks.2.mlp.fc2.weight', 'predictor.predictor_blocks.2.mlp.fc2.bias', 'predictor.predictor_blocks.3.norm1.weight', 'predictor.predictor_blocks.3.norm1.bias', 'predictor.predictor_blocks.3.attn.qkv.weight', 'predictor.predictor_blocks.3.attn.qkv.bias', 'predictor.predictor_blocks.3.attn.proj.weight', 'predictor.predictor_blocks.3.attn.proj.bias', 'predictor.predictor_blocks.3.norm2.weight', 'predictor.predictor_blocks.3.norm2.bias', 'predictor.predictor_blocks.3.mlp.fc1.weight', 'predictor.predictor_blocks.3.mlp.fc1.bias', 'predictor.predictor_blocks.3.mlp.fc2.weight', 'predictor.predictor_blocks.3.mlp.fc2.bias', 'predictor.predictor_blocks.4.norm1.weight', 'predictor.predictor_blocks.4.norm1.bias', 'predictor.predictor_blocks.4.attn.qkv.weight', 'predictor.predictor_blocks.4.attn.qkv.bias', 'predictor.predictor_blocks.4.attn.proj.weight', 'predictor.predictor_blocks.4.attn.proj.bias', 'predictor.predictor_blocks.4.norm2.weight', 'predictor.predictor_blocks.4.norm2.bias', 'predictor.predictor_blocks.4.mlp.fc1.weight', 'predictor.predictor_blocks.4.mlp.fc1.bias', 'predictor.predictor_blocks.4.mlp.fc2.weight', 'predictor.predictor_blocks.4.mlp.fc2.bias', 'predictor.predictor_blocks.5.norm1.weight', 'predictor.predictor_blocks.5.norm1.bias', 'predictor.predictor_blocks.5.attn.qkv.weight', 'predictor.predictor_blocks.5.attn.qkv.bias', 'predictor.predictor_blocks.5.attn.proj.weight', 'predictor.predictor_blocks.5.attn.proj.bias', 'predictor.predictor_blocks.5.norm2.weight', 'predictor.predictor_blocks.5.norm2.bias', 'predictor.predictor_blocks.5.mlp.fc1.weight', 'predictor.predictor_blocks.5.mlp.fc1.bias', 'predictor.predictor_blocks.5.mlp.fc2.weight', 'predictor.predictor_blocks.5.mlp.fc2.bias', 'predictor.predictor_blocks.6.norm1.weight', 'predictor.predictor_blocks.6.norm1.bias', 'predictor.predictor_blocks.6.attn.qkv.weight', 'predictor.predictor_blocks.6.attn.qkv.bias', 'predictor.predictor_blocks.6.attn.proj.weight', 'predictor.predictor_blocks.6.attn.proj.bias', 'predictor.predictor_blocks.6.norm2.weight', 'predictor.predictor_blocks.6.norm2.bias', 'predictor.predictor_blocks.6.mlp.fc1.weight', 'predictor.predictor_blocks.6.mlp.fc1.bias', 'predictor.predictor_blocks.6.mlp.fc2.weight', 'predictor.predictor_blocks.6.mlp.fc2.bias', 'predictor.predictor_blocks.7.norm1.weight', 'predictor.predictor_blocks.7.norm1.bias', 'predictor.predictor_blocks.7.attn.qkv.weight', 'predictor.predictor_blocks.7.attn.qkv.bias', 'predictor.predictor_blocks.7.attn.proj.weight', 'predictor.predictor_blocks.7.attn.proj.bias', 'predictor.predictor_blocks.7.norm2.weight', 'predictor.predictor_blocks.7.norm2.bias', 'predictor.predictor_blocks.7.mlp.fc1.weight', 'predictor.predictor_blocks.7.mlp.fc1.bias', 'predictor.predictor_blocks.7.mlp.fc2.weight', 'predictor.predictor_blocks.7.mlp.fc2.bias', 'predictor.predictor_norm.weight', 'predictor.predictor_norm.bias', 'predictor.predictor_proj.weight', 'predictor.predictor_proj.bias']
Model = EEGPTClassifier(
  (chan_conv): Sequential(
    (0): Conv2dWithConstraint(19, 20, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): GELU(approximate='none')
    (3): Conv2d(20, 20, kernel_size=(1, 15), stride=(1, 1), groups=20)
    (4): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): GELU(approximate='none')
    (6): Dropout(p=0.25, inplace=False)
  )
  (target_encoder): EEGTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(1, 512, kernel_size=(1, 64), stride=(1, 64))
    )
    (chan_embed): Embedding(62, 512)
    (blocks): ModuleList(
      (0-7): 8 x Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  )
  (reconstructor): EEGTransformerReconstructor(
    (reconstructor_embed): Linear(in_features=512, out_features=512, bias=True)
    (time_embed): RotaryEmbedding()
    (chan_embed): Embedding(62, 512)
    (reconstructor_blocks): ModuleList(
      (0-7): 8 x Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (reconstructor_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    (reconstructor_proj): Linear(in_features=512, out_features=64, bias=True)
  )
  (norm): Identity()
  (fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (head): Sequential(
    (0): Dropout(p=0.5, inplace=False)
    (1): LinearWithConstraint(in_features=63488, out_features=1, bias=True)
  )
)
number of params: 50900833
LR = 0.00050000
Batch size = 1600
Update frequent = 100
Number of training examples = 5037
Number of training training per epoch = 3
Assigned values = [0.0006599743590836596, 0.0010153451678210146, 0.0015620694889554071, 0.002403183829162165, 0.003697205891018715, 0.005688009063105715, 0.008750783174008792, 0.013462743344628911, 0.02071191283789063, 0.03186448128906251, 0.049022278906250015, 0.07541889062500001, 0.11602906250000002, 0.17850625000000003, 0.274625, 0.42250000000000004, 0.65, 1.0]
Skip weight decay name marked in model: {'target_encoder.summary_token', 'reconstructor.pos_embed', 'reconstructor.chan_embed', 'target_encoder.chan_embed', 'reconstructor.cls_token', 'reconstructor.time_embed'}
Param groups = {
  "layer_17_decay": {
    "weight_decay": 0.05,
    "params": [
      "chan_conv.0.weight",
      "chan_conv.3.weight",
      "target_encoder.patch_embed.proj.weight",
      "target_encoder.chan_embed.weight",
      "target_encoder.blocks.0.attn.qkv.weight",
      "target_encoder.blocks.0.attn.proj.weight",
      "target_encoder.blocks.0.mlp.fc1.weight",
      "target_encoder.blocks.0.mlp.fc2.weight",
      "target_encoder.blocks.1.attn.qkv.weight",
      "target_encoder.blocks.1.attn.proj.weight",
      "target_encoder.blocks.1.mlp.fc1.weight",
      "target_encoder.blocks.1.mlp.fc2.weight",
      "target_encoder.blocks.2.attn.qkv.weight",
      "target_encoder.blocks.2.attn.proj.weight",
      "target_encoder.blocks.2.mlp.fc1.weight",
      "target_encoder.blocks.2.mlp.fc2.weight",
      "target_encoder.blocks.3.attn.qkv.weight",
      "target_encoder.blocks.3.attn.proj.weight",
      "target_encoder.blocks.3.mlp.fc1.weight",
      "target_encoder.blocks.3.mlp.fc2.weight",
      "target_encoder.blocks.4.attn.qkv.weight",
      "target_encoder.blocks.4.attn.proj.weight",
      "target_encoder.blocks.4.mlp.fc1.weight",
      "target_encoder.blocks.4.mlp.fc2.weight",
      "target_encoder.blocks.5.attn.qkv.weight",
      "target_encoder.blocks.5.attn.proj.weight",
      "target_encoder.blocks.5.mlp.fc1.weight",
      "target_encoder.blocks.5.mlp.fc2.weight",
      "target_encoder.blocks.6.attn.qkv.weight",
      "target_encoder.blocks.6.attn.proj.weight",
      "target_encoder.blocks.6.mlp.fc1.weight",
      "target_encoder.blocks.6.mlp.fc2.weight",
      "target_encoder.blocks.7.attn.qkv.weight",
      "target_encoder.blocks.7.attn.proj.weight",
      "target_encoder.blocks.7.mlp.fc1.weight",
      "target_encoder.blocks.7.mlp.fc2.weight",
      "reconstructor.mask_token",
      "reconstructor.reconstructor_embed.weight",
      "reconstructor.chan_embed.weight",
      "reconstructor.reconstructor_blocks.0.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.0.attn.proj.weight",
      "reconstructor.reconstructor_blocks.0.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.0.mlp.fc2.weight",
      "reconstructor.reconstructor_blocks.1.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.1.attn.proj.weight",
      "reconstructor.reconstructor_blocks.1.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.1.mlp.fc2.weight",
      "reconstructor.reconstructor_blocks.2.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.2.attn.proj.weight",
      "reconstructor.reconstructor_blocks.2.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.2.mlp.fc2.weight",
      "reconstructor.reconstructor_blocks.3.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.3.attn.proj.weight",
      "reconstructor.reconstructor_blocks.3.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.3.mlp.fc2.weight",
      "reconstructor.reconstructor_blocks.4.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.4.attn.proj.weight",
      "reconstructor.reconstructor_blocks.4.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.4.mlp.fc2.weight",
      "reconstructor.reconstructor_blocks.5.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.5.attn.proj.weight",
      "reconstructor.reconstructor_blocks.5.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.5.mlp.fc2.weight",
      "reconstructor.reconstructor_blocks.6.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.6.attn.proj.weight",
      "reconstructor.reconstructor_blocks.6.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.6.mlp.fc2.weight",
      "reconstructor.reconstructor_blocks.7.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.7.attn.proj.weight",
      "reconstructor.reconstructor_blocks.7.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.7.mlp.fc2.weight",
      "reconstructor.reconstructor_proj.weight",
      "head.1.weight"
    ],
    "lr_scale": 1.0
  },
  "layer_17_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "chan_conv.0.bias",
      "chan_conv.1.weight",
      "chan_conv.1.bias",
      "chan_conv.3.bias",
      "chan_conv.4.weight",
      "chan_conv.4.bias",
      "target_encoder.summary_token",
      "target_encoder.patch_embed.proj.bias",
      "target_encoder.blocks.0.norm1.weight",
      "target_encoder.blocks.0.norm1.bias",
      "target_encoder.blocks.0.attn.qkv.bias",
      "target_encoder.blocks.0.attn.proj.bias",
      "target_encoder.blocks.0.norm2.weight",
      "target_encoder.blocks.0.norm2.bias",
      "target_encoder.blocks.0.mlp.fc1.bias",
      "target_encoder.blocks.0.mlp.fc2.bias",
      "target_encoder.blocks.1.norm1.weight",
      "target_encoder.blocks.1.norm1.bias",
      "target_encoder.blocks.1.attn.qkv.bias",
      "target_encoder.blocks.1.attn.proj.bias",
      "target_encoder.blocks.1.norm2.weight",
      "target_encoder.blocks.1.norm2.bias",
      "target_encoder.blocks.1.mlp.fc1.bias",
      "target_encoder.blocks.1.mlp.fc2.bias",
      "target_encoder.blocks.2.norm1.weight",
      "target_encoder.blocks.2.norm1.bias",
      "target_encoder.blocks.2.attn.qkv.bias",
      "target_encoder.blocks.2.attn.proj.bias",
      "target_encoder.blocks.2.norm2.weight",
      "target_encoder.blocks.2.norm2.bias",
      "target_encoder.blocks.2.mlp.fc1.bias",
      "target_encoder.blocks.2.mlp.fc2.bias",
      "target_encoder.blocks.3.norm1.weight",
      "target_encoder.blocks.3.norm1.bias",
      "target_encoder.blocks.3.attn.qkv.bias",
      "target_encoder.blocks.3.attn.proj.bias",
      "target_encoder.blocks.3.norm2.weight",
      "target_encoder.blocks.3.norm2.bias",
      "target_encoder.blocks.3.mlp.fc1.bias",
      "target_encoder.blocks.3.mlp.fc2.bias",
      "target_encoder.blocks.4.norm1.weight",
      "target_encoder.blocks.4.norm1.bias",
      "target_encoder.blocks.4.attn.qkv.bias",
      "target_encoder.blocks.4.attn.proj.bias",
      "target_encoder.blocks.4.norm2.weight",
      "target_encoder.blocks.4.norm2.bias",
      "target_encoder.blocks.4.mlp.fc1.bias",
      "target_encoder.blocks.4.mlp.fc2.bias",
      "target_encoder.blocks.5.norm1.weight",
      "target_encoder.blocks.5.norm1.bias",
      "target_encoder.blocks.5.attn.qkv.bias",
      "target_encoder.blocks.5.attn.proj.bias",
      "target_encoder.blocks.5.norm2.weight",
      "target_encoder.blocks.5.norm2.bias",
      "target_encoder.blocks.5.mlp.fc1.bias",
      "target_encoder.blocks.5.mlp.fc2.bias",
      "target_encoder.blocks.6.norm1.weight",
      "target_encoder.blocks.6.norm1.bias",
      "target_encoder.blocks.6.attn.qkv.bias",
      "target_encoder.blocks.6.attn.proj.bias",
      "target_encoder.blocks.6.norm2.weight",
      "target_encoder.blocks.6.norm2.bias",
      "target_encoder.blocks.6.mlp.fc1.bias",
      "target_encoder.blocks.6.mlp.fc2.bias",
      "target_encoder.blocks.7.norm1.weight",
      "target_encoder.blocks.7.norm1.bias",
      "target_encoder.blocks.7.attn.qkv.bias",
      "target_encoder.blocks.7.attn.proj.bias",
      "target_encoder.blocks.7.norm2.weight",
      "target_encoder.blocks.7.norm2.bias",
      "target_encoder.blocks.7.mlp.fc1.bias",
      "target_encoder.blocks.7.mlp.fc2.bias",
      "target_encoder.norm.weight",
      "target_encoder.norm.bias",
      "reconstructor.cls_token",
      "reconstructor.reconstructor_embed.bias",
      "reconstructor.reconstructor_blocks.0.norm1.weight",
      "reconstructor.reconstructor_blocks.0.norm1.bias",
      "reconstructor.reconstructor_blocks.0.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.0.attn.proj.bias",
      "reconstructor.reconstructor_blocks.0.norm2.weight",
      "reconstructor.reconstructor_blocks.0.norm2.bias",
      "reconstructor.reconstructor_blocks.0.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.0.mlp.fc2.bias",
      "reconstructor.reconstructor_blocks.1.norm1.weight",
      "reconstructor.reconstructor_blocks.1.norm1.bias",
      "reconstructor.reconstructor_blocks.1.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.1.attn.proj.bias",
      "reconstructor.reconstructor_blocks.1.norm2.weight",
      "reconstructor.reconstructor_blocks.1.norm2.bias",
      "reconstructor.reconstructor_blocks.1.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.1.mlp.fc2.bias",
      "reconstructor.reconstructor_blocks.2.norm1.weight",
      "reconstructor.reconstructor_blocks.2.norm1.bias",
      "reconstructor.reconstructor_blocks.2.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.2.attn.proj.bias",
      "reconstructor.reconstructor_blocks.2.norm2.weight",
      "reconstructor.reconstructor_blocks.2.norm2.bias",
      "reconstructor.reconstructor_blocks.2.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.2.mlp.fc2.bias",
      "reconstructor.reconstructor_blocks.3.norm1.weight",
      "reconstructor.reconstructor_blocks.3.norm1.bias",
      "reconstructor.reconstructor_blocks.3.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.3.attn.proj.bias",
      "reconstructor.reconstructor_blocks.3.norm2.weight",
      "reconstructor.reconstructor_blocks.3.norm2.bias",
      "reconstructor.reconstructor_blocks.3.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.3.mlp.fc2.bias",
      "reconstructor.reconstructor_blocks.4.norm1.weight",
      "reconstructor.reconstructor_blocks.4.norm1.bias",
      "reconstructor.reconstructor_blocks.4.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.4.attn.proj.bias",
      "reconstructor.reconstructor_blocks.4.norm2.weight",
      "reconstructor.reconstructor_blocks.4.norm2.bias",
      "reconstructor.reconstructor_blocks.4.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.4.mlp.fc2.bias",
      "reconstructor.reconstructor_blocks.5.norm1.weight",
      "reconstructor.reconstructor_blocks.5.norm1.bias",
      "reconstructor.reconstructor_blocks.5.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.5.attn.proj.bias",
      "reconstructor.reconstructor_blocks.5.norm2.weight",
      "reconstructor.reconstructor_blocks.5.norm2.bias",
      "reconstructor.reconstructor_blocks.5.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.5.mlp.fc2.bias",
      "reconstructor.reconstructor_blocks.6.norm1.weight",
      "reconstructor.reconstructor_blocks.6.norm1.bias",
      "reconstructor.reconstructor_blocks.6.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.6.attn.proj.bias",
      "reconstructor.reconstructor_blocks.6.norm2.weight",
      "reconstructor.reconstructor_blocks.6.norm2.bias",
      "reconstructor.reconstructor_blocks.6.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.6.mlp.fc2.bias",
      "reconstructor.reconstructor_blocks.7.norm1.weight",
      "reconstructor.reconstructor_blocks.7.norm1.bias",
      "reconstructor.reconstructor_blocks.7.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.7.attn.proj.bias",
      "reconstructor.reconstructor_blocks.7.norm2.weight",
      "reconstructor.reconstructor_blocks.7.norm2.bias",
      "reconstructor.reconstructor_blocks.7.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.7.mlp.fc2.bias",
      "reconstructor.reconstructor_norm.weight",
      "reconstructor.reconstructor_norm.bias",
      "reconstructor.reconstructor_proj.bias",
      "fc_norm.weight",
      "fc_norm.bias",
      "head.1.bias"
    ],
    "lr_scale": 1.0
  }
}
Optimizer config: {'lr': 0.0005, 'weight_decay': 0.0, 'eps': 1e-08}
Use step level LR scheduler!
Set warmup steps = 15
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = BCEWithLogitsLoss()
Auto resume checkpoint: 
Start training for 25 epochs
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [0]  [  0/314]  eta: 0:03:59  lr: 0.000000  min_lr: 0.000000  loss: 0.7719 (0.7719)  class_acc: 0.6250 (0.6250)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7616  data: 0.1991  max mem: 2109
Epoch: [0]  [ 10/314]  eta: 0:00:29  lr: 0.000000  min_lr: 0.000000  loss: 0.8714 (0.8851)  class_acc: 0.4375 (0.4148)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0983  data: 0.0183  max mem: 2210
Epoch: [0]  [ 20/314]  eta: 0:00:19  lr: 0.000000  min_lr: 0.000000  loss: 0.8897 (0.8892)  class_acc: 0.4375 (0.4196)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0316  data: 0.0002  max mem: 2210
Epoch: [0]  [ 30/314]  eta: 0:00:15  lr: 0.000000  min_lr: 0.000000  loss: 0.9003 (0.8811)  class_acc: 0.4375 (0.4395)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0312  data: 0.0002  max mem: 2210
Epoch: [0]  [ 40/314]  eta: 0:00:13  lr: 0.000000  min_lr: 0.000000  loss: 0.8539 (0.8690)  class_acc: 0.5000 (0.4436)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0308  data: 0.0002  max mem: 2210
Epoch: [0]  [ 50/314]  eta: 0:00:12  lr: 0.000000  min_lr: 0.000000  loss: 0.8544 (0.8706)  class_acc: 0.4375 (0.4436)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0307  data: 0.0003  max mem: 2210
Epoch: [0]  [ 60/314]  eta: 0:00:10  lr: 0.000000  min_lr: 0.000000  loss: 0.8718 (0.8775)  class_acc: 0.5000 (0.4467)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0307  data: 0.0003  max mem: 2210
Epoch: [0]  [ 70/314]  eta: 0:00:10  lr: 0.000000  min_lr: 0.000000  loss: 0.8796 (0.8856)  class_acc: 0.5000 (0.4463)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0307  data: 0.0002  max mem: 2210
Epoch: [0]  [ 80/314]  eta: 0:00:09  lr: 0.000000  min_lr: 0.000000  loss: 0.8845 (0.8908)  class_acc: 0.3750 (0.4390)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0313  data: 0.0002  max mem: 2210
Epoch: [0]  [ 90/314]  eta: 0:00:08  lr: 0.000000  min_lr: 0.000000  loss: 0.9593 (0.9003)  class_acc: 0.3750 (0.4361)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0311  data: 0.0002  max mem: 2210
Epoch: [0]  [100/314]  eta: 0:00:08  lr: 0.000036  min_lr: 0.000036  loss: 0.8844 (0.8998)  class_acc: 0.3750 (0.4387)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (69.2934)  time: 0.0319  data: 0.0002  max mem: 2301
Epoch: [0]  [110/314]  eta: 0:00:07  lr: 0.000036  min_lr: 0.000036  loss: 0.9023 (0.9109)  class_acc: 0.4375 (0.4341)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (69.2934)  time: 0.0317  data: 0.0002  max mem: 2401
Epoch: [0]  [120/314]  eta: 0:00:07  lr: 0.000036  min_lr: 0.000036  loss: 0.8869 (0.9120)  class_acc: 0.4375 (0.4370)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (69.2934)  time: 0.0302  data: 0.0002  max mem: 2401
Epoch: [0]  [130/314]  eta: 0:00:06  lr: 0.000036  min_lr: 0.000036  loss: 0.8869 (0.9134)  class_acc: 0.4375 (0.4380)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (69.2934)  time: 0.0302  data: 0.0002  max mem: 2401
Epoch: [0]  [140/314]  eta: 0:00:06  lr: 0.000036  min_lr: 0.000036  loss: 0.9223 (0.9148)  class_acc: 0.3750 (0.4340)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (69.2934)  time: 0.0308  data: 0.0003  max mem: 2401
Epoch: [0]  [150/314]  eta: 0:00:05  lr: 0.000036  min_lr: 0.000036  loss: 0.8961 (0.9118)  class_acc: 0.4375 (0.4383)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (69.2934)  time: 0.0309  data: 0.0002  max mem: 2401
Epoch: [0]  [160/314]  eta: 0:00:05  lr: 0.000036  min_lr: 0.000036  loss: 0.8500 (0.9073)  class_acc: 0.4375 (0.4371)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (69.2934)  time: 0.0307  data: 0.0001  max mem: 2401
Epoch: [0]  [170/314]  eta: 0:00:05  lr: 0.000036  min_lr: 0.000036  loss: 0.8378 (0.9070)  class_acc: 0.4375 (0.4371)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (69.2934)  time: 0.0307  data: 0.0002  max mem: 2401
Epoch: [0]  [180/314]  eta: 0:00:04  lr: 0.000036  min_lr: 0.000036  loss: 0.8348 (0.9016)  class_acc: 0.5000 (0.4434)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (69.2934)  time: 0.0306  data: 0.0001  max mem: 2401
Epoch: [0]  [190/314]  eta: 0:00:04  lr: 0.000036  min_lr: 0.000036  loss: 0.8176 (0.9009)  class_acc: 0.5000 (0.4431)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (69.2934)  time: 0.0308  data: 0.0001  max mem: 2401
Epoch: [0]  [200/314]  eta: 0:00:03  lr: 0.000071  min_lr: 0.000071  loss: 0.9366 (0.9032)  class_acc: 0.4375 (0.4437)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (70.0826)  time: 0.0311  data: 0.0002  max mem: 2401
Epoch: [0]  [210/314]  eta: 0:00:03  lr: 0.000071  min_lr: 0.000071  loss: 0.9321 (0.9053)  class_acc: 0.5000 (0.4514)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (70.0826)  time: 0.0312  data: 0.0002  max mem: 2401
Epoch: [0]  [220/314]  eta: 0:00:03  lr: 0.000071  min_lr: 0.000071  loss: 0.8659 (0.9036)  class_acc: 0.6250 (0.4593)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (70.0826)  time: 0.0310  data: 0.0002  max mem: 2401
Epoch: [0]  [230/314]  eta: 0:00:02  lr: 0.000071  min_lr: 0.000071  loss: 0.9023 (0.9092)  class_acc: 0.5625 (0.4624)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (70.0826)  time: 0.0308  data: 0.0002  max mem: 2401
Epoch: [0]  [240/314]  eta: 0:00:02  lr: 0.000071  min_lr: 0.000071  loss: 1.0039 (0.9123)  class_acc: 0.5000 (0.4694)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (70.0826)  time: 0.0306  data: 0.0002  max mem: 2401
Epoch: [0]  [250/314]  eta: 0:00:02  lr: 0.000071  min_lr: 0.000071  loss: 0.9902 (0.9120)  class_acc: 0.6250 (0.4766)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (70.0826)  time: 0.0304  data: 0.0002  max mem: 2401
Epoch: [0]  [260/314]  eta: 0:00:01  lr: 0.000071  min_lr: 0.000071  loss: 0.9467 (0.9170)  class_acc: 0.5625 (0.4789)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (70.0826)  time: 0.0304  data: 0.0002  max mem: 2401
Epoch: [0]  [270/314]  eta: 0:00:01  lr: 0.000071  min_lr: 0.000071  loss: 0.9081 (0.9172)  class_acc: 0.5625 (0.4839)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (70.0826)  time: 0.0306  data: 0.0002  max mem: 2401
Epoch: [0]  [280/314]  eta: 0:00:01  lr: 0.000071  min_lr: 0.000071  loss: 0.9081 (0.9196)  class_acc: 0.6250 (0.4880)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (70.0826)  time: 0.0307  data: 0.0002  max mem: 2401
Epoch: [0]  [290/314]  eta: 0:00:00  lr: 0.000071  min_lr: 0.000071  loss: 0.9354 (0.9203)  class_acc: 0.5625 (0.4910)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (70.0826)  time: 0.0304  data: 0.0001  max mem: 2401
Epoch: [0]  [300/314]  eta: 0:00:00  lr: 0.000071  min_lr: 0.000071  loss: 0.8229 (0.9184)  class_acc: 0.5625 (0.4952)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (69.3757)  time: 0.0288  data: 0.0001  max mem: 2401
Epoch: [0]  [310/314]  eta: 0:00:00  lr: 0.000071  min_lr: 0.000071  loss: 0.8229 (0.9184)  class_acc: 0.5625 (0.4952)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (69.3757)  time: 0.0137  data: 0.0001  max mem: 2401
Epoch: [0]  [313/314]  eta: 0:00:00  lr: 0.000071  min_lr: 0.000071  loss: 0.8229 (0.9184)  class_acc: 0.5625 (0.4952)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (69.3757)  time: 0.0091  data: 0.0000  max mem: 2401
Epoch: [0] Total time: 0:00:10 (0.0321 s / it)
Averaged stats: lr: 0.000071  min_lr: 0.000071  loss: 0.8229 (0.9184)  class_acc: 0.5625 (0.4952)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 69.2934 (69.3757)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/61]  eta: 0:00:20  loss: 0.1847 (0.1847)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.3374  data: 0.2395  max mem: 2401
Val:  [10/61]  eta: 0:00:02  loss: 0.2136 (0.2690)  accuracy: 0.0000 (0.2273)  balanced_accuracy: 0.0000 (0.1364)  pr_auc: 0.0000 (0.2449)  roc_auc: 0.0000 (0.1544)  time: 0.0440  data: 0.0219  max mem: 2401
Val:  [20/61]  eta: 0:00:01  loss: 0.4540 (0.4767)  accuracy: 0.6250 (0.4365)  balanced_accuracy: 0.5000 (0.3095)  pr_auc: 0.6496 (0.4781)  roc_auc: 0.4028 (0.3393)  time: 0.0147  data: 0.0002  max mem: 2401
Val:  [30/61]  eta: 0:00:00  loss: 0.9238 (0.6695)  accuracy: 0.5000 (0.4368)  balanced_accuracy: 0.5000 (0.3710)  pr_auc: 0.6496 (0.4978)  roc_auc: 0.5315 (0.3982)  time: 0.0149  data: 0.0002  max mem: 2401
Val:  [40/61]  eta: 0:00:00  loss: 1.0406 (0.7584)  accuracy: 0.4583 (0.4451)  balanced_accuracy: 0.5000 (0.4014)  pr_auc: 0.5065 (0.4999)  roc_auc: 0.4615 (0.4132)  time: 0.0148  data: 0.0002  max mem: 2401
Val:  [50/61]  eta: 0:00:00  loss: 1.0824 (0.8335)  accuracy: 0.4583 (0.4428)  balanced_accuracy: 0.5000 (0.4208)  pr_auc: 0.4146 (0.4807)  roc_auc: 0.3704 (0.4008)  time: 0.0143  data: 0.0001  max mem: 2401
Val:  [60/61]  eta: 0:00:00  loss: 1.1542 (0.8633)  accuracy: 0.3750 (0.4074)  balanced_accuracy: 0.5000 (0.4094)  pr_auc: 0.3554 (0.4340)  roc_auc: 0.3143 (0.3661)  time: 0.0157  data: 0.0001  max mem: 2401
Val: Total time: 0:00:01 (0.0223 s / it)
* loss 0.863
Accuracy of the network on the 1463 test EEG: 0.59%
Max accuracy val: 0.59%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [1]  [  0/314]  eta: 0:01:22  lr: 0.000107  min_lr: 0.000107  loss: 1.3703 (1.3703)  class_acc: 0.6250 (0.6250)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2625  data: 0.2014  max mem: 2401
Epoch: [1]  [ 10/314]  eta: 0:00:15  lr: 0.000107  min_lr: 0.000107  loss: 1.2406 (1.1903)  class_acc: 0.6250 (0.6364)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0512  data: 0.0184  max mem: 2401
Epoch: [1]  [ 20/314]  eta: 0:00:12  lr: 0.000107  min_lr: 0.000107  loss: 1.1872 (1.2343)  class_acc: 0.6250 (0.6190)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0299  data: 0.0001  max mem: 2401
Epoch: [1]  [ 30/314]  eta: 0:00:10  lr: 0.000107  min_lr: 0.000107  loss: 1.2410 (1.2882)  class_acc: 0.5625 (0.5988)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0301  data: 0.0002  max mem: 2401
Epoch: [1]  [ 40/314]  eta: 0:00:09  lr: 0.000107  min_lr: 0.000107  loss: 1.3977 (1.2985)  class_acc: 0.5625 (0.5945)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0303  data: 0.0002  max mem: 2401
Epoch: [1]  [ 50/314]  eta: 0:00:09  lr: 0.000107  min_lr: 0.000107  loss: 1.3013 (1.3203)  class_acc: 0.5625 (0.5833)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0303  data: 0.0002  max mem: 2401
Epoch: [1]  [ 60/314]  eta: 0:00:08  lr: 0.000107  min_lr: 0.000107  loss: 1.3052 (1.3113)  class_acc: 0.5625 (0.5902)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0305  data: 0.0002  max mem: 2401
Epoch: [1]  [ 70/314]  eta: 0:00:08  lr: 0.000107  min_lr: 0.000107  loss: 1.3052 (1.3261)  class_acc: 0.6250 (0.5889)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0302  data: 0.0002  max mem: 2401
Epoch: [1]  [ 80/314]  eta: 0:00:07  lr: 0.000107  min_lr: 0.000107  loss: 1.0554 (1.2950)  class_acc: 0.6250 (0.5995)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0303  data: 0.0002  max mem: 2401
Epoch: [1]  [ 90/314]  eta: 0:00:07  lr: 0.000107  min_lr: 0.000107  loss: 0.8995 (1.2643)  class_acc: 0.6875 (0.6099)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0304  data: 0.0002  max mem: 2401
Epoch: [1]  [100/314]  eta: 0:00:06  lr: 0.000143  min_lr: 0.000143  loss: 0.9090 (1.2455)  class_acc: 0.6250 (0.6095)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 93.3956 (93.3956)  time: 0.0302  data: 0.0002  max mem: 2401
Epoch: [1]  [110/314]  eta: 0:00:06  lr: 0.000143  min_lr: 0.000143  loss: 0.7935 (1.1990)  class_acc: 0.6250 (0.6120)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 93.3956 (93.3956)  time: 0.0303  data: 0.0002  max mem: 2404
Epoch: [1]  [120/314]  eta: 0:00:06  lr: 0.000143  min_lr: 0.000143  loss: 0.7845 (1.1706)  class_acc: 0.6250 (0.6100)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 93.3956 (93.3956)  time: 0.0304  data: 0.0002  max mem: 2404
Epoch: [1]  [130/314]  eta: 0:00:05  lr: 0.000143  min_lr: 0.000143  loss: 0.8538 (1.1512)  class_acc: 0.5625 (0.6078)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 93.3956 (93.3956)  time: 0.0308  data: 0.0002  max mem: 2404
Epoch: [1]  [140/314]  eta: 0:00:05  lr: 0.000143  min_lr: 0.000143  loss: 0.8876 (1.1323)  class_acc: 0.5625 (0.6055)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 93.3956 (93.3956)  time: 0.0310  data: 0.0002  max mem: 2404
Epoch: [1]  [150/314]  eta: 0:00:05  lr: 0.000143  min_lr: 0.000143  loss: 0.8956 (1.1196)  class_acc: 0.5625 (0.6026)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 93.3956 (93.3956)  time: 0.0307  data: 0.0002  max mem: 2404
Epoch: [1]  [160/314]  eta: 0:00:04  lr: 0.000143  min_lr: 0.000143  loss: 0.7983 (1.1048)  class_acc: 0.5625 (0.6036)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 93.3956 (93.3956)  time: 0.0306  data: 0.0002  max mem: 2404
Epoch: [1]  [170/314]  eta: 0:00:04  lr: 0.000143  min_lr: 0.000143  loss: 0.7638 (1.0837)  class_acc: 0.6250 (0.6060)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 93.3956 (93.3956)  time: 0.0306  data: 0.0002  max mem: 2404
Epoch: [1]  [180/314]  eta: 0:00:04  lr: 0.000143  min_lr: 0.000143  loss: 0.7535 (1.0703)  class_acc: 0.6250 (0.6046)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 93.3956 (93.3956)  time: 0.0307  data: 0.0002  max mem: 2404
Epoch: [1]  [190/314]  eta: 0:00:03  lr: 0.000143  min_lr: 0.000143  loss: 0.7473 (1.0550)  class_acc: 0.6250 (0.6073)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 93.3956 (93.3956)  time: 0.0308  data: 0.0002  max mem: 2404
Epoch: [1]  [200/314]  eta: 0:00:03  lr: 0.000179  min_lr: 0.000179  loss: 0.8113 (1.0472)  class_acc: 0.6250 (0.6067)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 52.3886 (72.8921)  time: 0.0310  data: 0.0002  max mem: 2404
Epoch: [1]  [210/314]  eta: 0:00:03  lr: 0.000179  min_lr: 0.000179  loss: 0.9461 (1.0511)  class_acc: 0.5000 (0.5989)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 52.3886 (72.8921)  time: 0.0309  data: 0.0002  max mem: 2404
Epoch: [1]  [220/314]  eta: 0:00:02  lr: 0.000179  min_lr: 0.000179  loss: 1.1328 (1.0581)  class_acc: 0.4375 (0.5891)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 52.3886 (72.8921)  time: 0.0307  data: 0.0002  max mem: 2404
Epoch: [1]  [230/314]  eta: 0:00:02  lr: 0.000179  min_lr: 0.000179  loss: 1.0569 (1.0579)  class_acc: 0.4375 (0.5836)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 52.3886 (72.8921)  time: 0.0309  data: 0.0002  max mem: 2404
Epoch: [1]  [240/314]  eta: 0:00:02  lr: 0.000179  min_lr: 0.000179  loss: 1.0403 (1.0624)  class_acc: 0.5000 (0.5762)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 52.3886 (72.8921)  time: 0.0308  data: 0.0002  max mem: 2404
Epoch: [1]  [250/314]  eta: 0:00:02  lr: 0.000179  min_lr: 0.000179  loss: 1.0527 (1.0651)  class_acc: 0.3750 (0.5687)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 52.3886 (72.8921)  time: 0.0308  data: 0.0002  max mem: 2404
Epoch: [1]  [260/314]  eta: 0:00:01  lr: 0.000179  min_lr: 0.000179  loss: 0.9795 (1.0613)  class_acc: 0.4375 (0.5651)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 52.3886 (72.8921)  time: 0.0313  data: 0.0002  max mem: 2404
Epoch: [1]  [270/314]  eta: 0:00:01  lr: 0.000179  min_lr: 0.000179  loss: 1.0648 (1.0700)  class_acc: 0.4375 (0.5579)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 52.3886 (72.8921)  time: 0.0317  data: 0.0002  max mem: 2404
Epoch: [1]  [280/314]  eta: 0:00:01  lr: 0.000179  min_lr: 0.000179  loss: 1.1807 (1.0713)  class_acc: 0.3750 (0.5538)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 52.3886 (72.8921)  time: 0.0316  data: 0.0002  max mem: 2404
Epoch: [1]  [290/314]  eta: 0:00:00  lr: 0.000179  min_lr: 0.000179  loss: 1.0970 (1.0714)  class_acc: 0.4375 (0.5511)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 52.3886 (72.8921)  time: 0.0316  data: 0.0002  max mem: 2404
Epoch: [1]  [300/314]  eta: 0:00:00  lr: 0.000179  min_lr: 0.000179  loss: 1.0181 (1.0744)  class_acc: 0.4375 (0.5473)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 93.3956 (83.6488)  time: 0.0297  data: 0.0001  max mem: 2404
Epoch: [1]  [310/314]  eta: 0:00:00  lr: 0.000179  min_lr: 0.000179  loss: 1.0181 (1.0744)  class_acc: 0.4375 (0.5473)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 93.3956 (83.6488)  time: 0.0139  data: 0.0001  max mem: 2404
Epoch: [1]  [313/314]  eta: 0:00:00  lr: 0.000179  min_lr: 0.000179  loss: 1.0181 (1.0744)  class_acc: 0.4375 (0.5473)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 93.3956 (83.6488)  time: 0.0091  data: 0.0001  max mem: 2404
Epoch: [1] Total time: 0:00:09 (0.0305 s / it)
Averaged stats: lr: 0.000179  min_lr: 0.000179  loss: 1.0181 (1.0744)  class_acc: 0.4375 (0.5473)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 93.3956 (83.6488)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/61]  eta: 0:00:14  loss: 0.4314 (0.4314)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2412  data: 0.2114  max mem: 2404
Val:  [10/61]  eta: 0:00:01  loss: 0.4650 (0.4807)  accuracy: 0.0000 (0.2159)  balanced_accuracy: 0.0000 (0.1298)  pr_auc: 0.0000 (0.2362)  roc_auc: 0.0000 (0.1248)  time: 0.0382  data: 0.0214  max mem: 2404
Val:  [20/61]  eta: 0:00:01  loss: 0.5519 (0.5670)  accuracy: 0.5417 (0.4167)  balanced_accuracy: 0.4444 (0.3117)  pr_auc: 0.6214 (0.4582)  roc_auc: 0.3409 (0.2963)  time: 0.0162  data: 0.0013  max mem: 2404
Val:  [30/61]  eta: 0:00:00  loss: 0.7128 (0.6339)  accuracy: 0.5000 (0.4234)  balanced_accuracy: 0.4857 (0.3701)  pr_auc: 0.6236 (0.4970)  roc_auc: 0.5455 (0.3848)  time: 0.0146  data: 0.0002  max mem: 2404
Val:  [40/61]  eta: 0:00:00  loss: 0.7635 (0.6670)  accuracy: 0.4583 (0.4370)  balanced_accuracy: 0.5000 (0.4016)  pr_auc: 0.5508 (0.5076)  roc_auc: 0.5455 (0.4163)  time: 0.0146  data: 0.0002  max mem: 2404
Val:  [50/61]  eta: 0:00:00  loss: 0.7840 (0.6934)  accuracy: 0.4583 (0.4355)  balanced_accuracy: 0.5000 (0.4191)  pr_auc: 0.4729 (0.5016)  roc_auc: 0.4757 (0.4249)  time: 0.0143  data: 0.0001  max mem: 2404
Val:  [60/61]  eta: 0:00:00  loss: 0.8130 (0.7052)  accuracy: 0.3750 (0.4026)  balanced_accuracy: 0.5000 (0.4089)  pr_auc: 0.4211 (0.4593)  roc_auc: 0.3895 (0.3991)  time: 0.0137  data: 0.0001  max mem: 2404
Val: Total time: 0:00:01 (0.0207 s / it)
* loss 0.705
Accuracy of the network on the 1463 test EEG: 0.57%
Max accuracy val: 0.59%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [2]  [  0/314]  eta: 0:01:19  lr: 0.000214  min_lr: 0.000214  loss: 0.8032 (0.8032)  class_acc: 0.4375 (0.4375)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2548  data: 0.1939  max mem: 2404
Epoch: [2]  [ 10/314]  eta: 0:00:15  lr: 0.000214  min_lr: 0.000214  loss: 0.9541 (0.9318)  class_acc: 0.4375 (0.4091)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0519  data: 0.0180  max mem: 2404
Epoch: [2]  [ 20/314]  eta: 0:00:12  lr: 0.000214  min_lr: 0.000214  loss: 0.9195 (0.8895)  class_acc: 0.4375 (0.4554)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0316  data: 0.0002  max mem: 2404
Epoch: [2]  [ 30/314]  eta: 0:00:11  lr: 0.000214  min_lr: 0.000214  loss: 0.8115 (0.8757)  class_acc: 0.5000 (0.4839)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0315  data: 0.0001  max mem: 2404
Epoch: [2]  [ 40/314]  eta: 0:00:10  lr: 0.000214  min_lr: 0.000214  loss: 0.8603 (0.8822)  class_acc: 0.4375 (0.4695)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0310  data: 0.0001  max mem: 2404
Epoch: [2]  [ 50/314]  eta: 0:00:09  lr: 0.000214  min_lr: 0.000214  loss: 0.8602 (0.8738)  class_acc: 0.4375 (0.4755)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0313  data: 0.0001  max mem: 2404
Epoch: [2]  [ 60/314]  eta: 0:00:08  lr: 0.000214  min_lr: 0.000214  loss: 0.8493 (0.8889)  class_acc: 0.4375 (0.4580)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0323  data: 0.0001  max mem: 2404
Epoch: [2]  [ 70/314]  eta: 0:00:08  lr: 0.000214  min_lr: 0.000214  loss: 0.8587 (0.8866)  class_acc: 0.4375 (0.4577)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0313  data: 0.0002  max mem: 2404
Epoch: [2]  [ 80/314]  eta: 0:00:07  lr: 0.000214  min_lr: 0.000214  loss: 0.8749 (0.8961)  class_acc: 0.4375 (0.4522)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0303  data: 0.0002  max mem: 2404
Epoch: [2]  [ 90/314]  eta: 0:00:07  lr: 0.000214  min_lr: 0.000214  loss: 0.9954 (0.9105)  class_acc: 0.3750 (0.4382)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0307  data: 0.0002  max mem: 2404
Epoch: [2]  [100/314]  eta: 0:00:07  lr: 0.000250  min_lr: 0.000250  loss: 0.8512 (0.8940)  class_acc: 0.4375 (0.4486)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 76.3298 (76.3298)  time: 0.0315  data: 0.0001  max mem: 2404
Epoch: [2]  [110/314]  eta: 0:00:06  lr: 0.000250  min_lr: 0.000250  loss: 0.7792 (0.8901)  class_acc: 0.6250 (0.4690)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 76.3298 (76.3298)  time: 0.0317  data: 0.0002  max mem: 2404
Epoch: [2]  [120/314]  eta: 0:00:06  lr: 0.000250  min_lr: 0.000250  loss: 0.8753 (0.9014)  class_acc: 0.6250 (0.4788)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 76.3298 (76.3298)  time: 0.0315  data: 0.0001  max mem: 2404
Epoch: [2]  [130/314]  eta: 0:00:06  lr: 0.000250  min_lr: 0.000250  loss: 0.9779 (0.9136)  class_acc: 0.5625 (0.4862)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 76.3298 (76.3298)  time: 0.0314  data: 0.0001  max mem: 2404
Epoch: [2]  [140/314]  eta: 0:00:05  lr: 0.000250  min_lr: 0.000250  loss: 0.9271 (0.9116)  class_acc: 0.5625 (0.4960)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 76.3298 (76.3298)  time: 0.0312  data: 0.0001  max mem: 2404
Epoch: [2]  [150/314]  eta: 0:00:05  lr: 0.000250  min_lr: 0.000250  loss: 0.8850 (0.9191)  class_acc: 0.6250 (0.5012)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 76.3298 (76.3298)  time: 0.0318  data: 0.0001  max mem: 2404
Epoch: [2]  [160/314]  eta: 0:00:05  lr: 0.000250  min_lr: 0.000250  loss: 0.8863 (0.9202)  class_acc: 0.6250 (0.5085)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 76.3298 (76.3298)  time: 0.0317  data: 0.0002  max mem: 2404
Epoch: [2]  [170/314]  eta: 0:00:04  lr: 0.000250  min_lr: 0.000250  loss: 0.9344 (0.9173)  class_acc: 0.6250 (0.5168)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 76.3298 (76.3298)  time: 0.0311  data: 0.0002  max mem: 2404
Epoch: [2]  [180/314]  eta: 0:00:04  lr: 0.000250  min_lr: 0.000250  loss: 0.9782 (0.9242)  class_acc: 0.6250 (0.5214)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 76.3298 (76.3298)  time: 0.0313  data: 0.0001  max mem: 2404
Epoch: [2]  [190/314]  eta: 0:00:04  lr: 0.000250  min_lr: 0.000250  loss: 0.8914 (0.9217)  class_acc: 0.6250 (0.5278)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 76.3298 (76.3298)  time: 0.0314  data: 0.0001  max mem: 2404
Epoch: [2]  [200/314]  eta: 0:00:03  lr: 0.000286  min_lr: 0.000286  loss: 0.7920 (0.9233)  class_acc: 0.6250 (0.5323)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 70.9302 (73.6300)  time: 0.0315  data: 0.0001  max mem: 2404
Epoch: [2]  [210/314]  eta: 0:00:03  lr: 0.000286  min_lr: 0.000286  loss: 1.0419 (0.9334)  class_acc: 0.6250 (0.5347)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 70.9302 (73.6300)  time: 0.0315  data: 0.0001  max mem: 2404
Epoch: [2]  [220/314]  eta: 0:00:03  lr: 0.000286  min_lr: 0.000286  loss: 1.0397 (0.9369)  class_acc: 0.5625 (0.5387)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 70.9302 (73.6300)  time: 0.0311  data: 0.0001  max mem: 2404
Epoch: [2]  [230/314]  eta: 0:00:02  lr: 0.000286  min_lr: 0.000286  loss: 1.0461 (0.9526)  class_acc: 0.5625 (0.5390)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 70.9302 (73.6300)  time: 0.0313  data: 0.0001  max mem: 2404
Epoch: [2]  [240/314]  eta: 0:00:02  lr: 0.000286  min_lr: 0.000286  loss: 1.0681 (0.9572)  class_acc: 0.5000 (0.5425)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 70.9302 (73.6300)  time: 0.0311  data: 0.0002  max mem: 2404
Epoch: [2]  [250/314]  eta: 0:00:02  lr: 0.000286  min_lr: 0.000286  loss: 1.0199 (0.9619)  class_acc: 0.6250 (0.5466)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 70.9302 (73.6300)  time: 0.0302  data: 0.0002  max mem: 2404
Epoch: [2]  [260/314]  eta: 0:00:01  lr: 0.000286  min_lr: 0.000286  loss: 1.0749 (0.9721)  class_acc: 0.5625 (0.5457)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 70.9302 (73.6300)  time: 0.0301  data: 0.0001  max mem: 2404
Epoch: [2]  [270/314]  eta: 0:00:01  lr: 0.000286  min_lr: 0.000286  loss: 1.0749 (0.9746)  class_acc: 0.6250 (0.5489)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 70.9302 (73.6300)  time: 0.0303  data: 0.0001  max mem: 2404
Epoch: [2]  [280/314]  eta: 0:00:01  lr: 0.000286  min_lr: 0.000286  loss: 1.0327 (0.9772)  class_acc: 0.6250 (0.5503)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 70.9302 (73.6300)  time: 0.0304  data: 0.0002  max mem: 2404
Epoch: [2]  [290/314]  eta: 0:00:00  lr: 0.000286  min_lr: 0.000286  loss: 0.9419 (0.9824)  class_acc: 0.5625 (0.5520)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 70.9302 (73.6300)  time: 0.0312  data: 0.0002  max mem: 2404
Epoch: [2]  [300/314]  eta: 0:00:00  lr: 0.000286  min_lr: 0.000286  loss: 0.9419 (0.9829)  class_acc: 0.6250 (0.5548)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 76.3298 (78.8056)  time: 0.0296  data: 0.0002  max mem: 2404
Epoch: [2]  [310/314]  eta: 0:00:00  lr: 0.000286  min_lr: 0.000286  loss: 0.9419 (0.9829)  class_acc: 0.6250 (0.5548)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 76.3298 (78.8056)  time: 0.0135  data: 0.0001  max mem: 2404
Epoch: [2]  [313/314]  eta: 0:00:00  lr: 0.000286  min_lr: 0.000286  loss: 0.9419 (0.9829)  class_acc: 0.6250 (0.5548)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 76.3298 (78.8056)  time: 0.0091  data: 0.0001  max mem: 2404
Epoch: [2] Total time: 0:00:09 (0.0309 s / it)
Averaged stats: lr: 0.000286  min_lr: 0.000286  loss: 0.9419 (0.9829)  class_acc: 0.6250 (0.5548)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 76.3298 (78.8056)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/61]  eta: 0:00:15  loss: 0.9425 (0.9425)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2578  data: 0.2172  max mem: 2404
Val:  [10/61]  eta: 0:00:01  loss: 0.9850 (0.9798)  accuracy: 0.0000 (0.0530)  balanced_accuracy: 0.0000 (0.1408)  pr_auc: 0.0000 (0.2385)  roc_auc: 0.0000 (0.1326)  time: 0.0366  data: 0.0199  max mem: 2404
Val:  [20/61]  eta: 0:00:01  loss: 0.8815 (0.9046)  accuracy: 0.2083 (0.1984)  balanced_accuracy: 0.5000 (0.3192)  pr_auc: 0.6460 (0.4683)  roc_auc: 0.3636 (0.3157)  time: 0.0144  data: 0.0002  max mem: 2404
Val:  [30/61]  eta: 0:00:00  loss: 0.7317 (0.8327)  accuracy: 0.4583 (0.3226)  balanced_accuracy: 0.5000 (0.3867)  pr_auc: 0.6019 (0.5056)  roc_auc: 0.5804 (0.4033)  time: 0.0144  data: 0.0001  max mem: 2404
Val:  [40/61]  eta: 0:00:00  loss: 0.7017 (0.8066)  accuracy: 0.5833 (0.3730)  balanced_accuracy: 0.5000 (0.4164)  pr_auc: 0.5579 (0.5153)  roc_auc: 0.5469 (0.4302)  time: 0.0144  data: 0.0001  max mem: 2404
Val:  [50/61]  eta: 0:00:00  loss: 0.7170 (0.7884)  accuracy: 0.5417 (0.4101)  balanced_accuracy: 0.5000 (0.4332)  pr_auc: 0.4730 (0.5054)  roc_auc: 0.4571 (0.4304)  time: 0.0142  data: 0.0001  max mem: 2404
Val:  [60/61]  eta: 0:00:00  loss: 0.7118 (0.7842)  accuracy: 0.5833 (0.4190)  balanced_accuracy: 0.4706 (0.4202)  pr_auc: 0.4006 (0.4607)  roc_auc: 0.3916 (0.4026)  time: 0.0136  data: 0.0001  max mem: 2404
Val: Total time: 0:00:01 (0.0201 s / it)
* loss 0.784
Accuracy of the network on the 1463 test EEG: 0.43%
Max accuracy val: 0.59%
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: [3]  [  0/314]  eta: 0:01:15  lr: 0.000321  min_lr: 0.000321  loss: 0.8384 (0.8384)  class_acc: 0.5000 (0.5000)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.2394  data: 0.1963  max mem: 2404
Epoch: [3]  [ 10/314]  eta: 0:00:15  lr: 0.000321  min_lr: 0.000321  loss: 0.7598 (0.7172)  class_acc: 0.5000 (0.5284)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0512  data: 0.0180  max mem: 2404
Epoch: [3]  [ 20/314]  eta: 0:00:12  lr: 0.000321  min_lr: 0.000321  loss: 0.7598 (0.7478)  class_acc: 0.5000 (0.5238)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0328  data: 0.0002  max mem: 2404
Epoch: [3]  [ 30/314]  eta: 0:00:11  lr: 0.000321  min_lr: 0.000321  loss: 0.7207 (0.7436)  class_acc: 0.5000 (0.5262)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0329  data: 0.0002  max mem: 2404
Epoch: [3]  [ 40/314]  eta: 0:00:10  lr: 0.000321  min_lr: 0.000321  loss: 0.7022 (0.7333)  class_acc: 0.5625 (0.5412)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0329  data: 0.0002  max mem: 2404
Epoch: [3]  [ 50/314]  eta: 0:00:09  lr: 0.000321  min_lr: 0.000321  loss: 0.7267 (0.7298)  class_acc: 0.5625 (0.5466)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0329  data: 0.0002  max mem: 2404
Epoch: [3]  [ 60/314]  eta: 0:00:09  lr: 0.000321  min_lr: 0.000321  loss: 0.7267 (0.7232)  class_acc: 0.5625 (0.5523)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0323  data: 0.0001  max mem: 2404
Epoch: [3]  [ 70/314]  eta: 0:00:08  lr: 0.000321  min_lr: 0.000321  loss: 0.7033 (0.7201)  class_acc: 0.5625 (0.5651)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0323  data: 0.0001  max mem: 2404
Epoch: [3]  [ 80/314]  eta: 0:00:08  lr: 0.000321  min_lr: 0.000321  loss: 0.7198 (0.7208)  class_acc: 0.5625 (0.5648)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0323  data: 0.0002  max mem: 2404
Epoch: [3]  [ 90/314]  eta: 0:00:07  lr: 0.000321  min_lr: 0.000321  loss: 0.7077 (0.7173)  class_acc: 0.5625 (0.5673)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.0319  data: 0.0002  max mem: 2404
Epoch: [3]  [100/314]  eta: 0:00:07  lr: 0.000357  min_lr: 0.000357  loss: 0.7313 (0.7327)  class_acc: 0.5000 (0.5520)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (19.2791)  time: 0.0320  data: 0.0002  max mem: 2404
Epoch: [3]  [110/314]  eta: 0:00:06  lr: 0.000357  min_lr: 0.000357  loss: 1.0315 (0.7984)  class_acc: 0.3750 (0.5327)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (19.2791)  time: 0.0320  data: 0.0002  max mem: 2404
Epoch: [3]  [120/314]  eta: 0:00:06  lr: 0.000357  min_lr: 0.000357  loss: 1.2715 (0.8276)  class_acc: 0.3750 (0.5248)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (19.2791)  time: 0.0315  data: 0.0002  max mem: 2404
Epoch: [3]  [130/314]  eta: 0:00:06  lr: 0.000357  min_lr: 0.000357  loss: 1.0783 (0.8516)  class_acc: 0.4375 (0.5191)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (19.2791)  time: 0.0317  data: 0.0002  max mem: 2404
Epoch: [3]  [140/314]  eta: 0:00:05  lr: 0.000357  min_lr: 0.000357  loss: 1.1056 (0.8730)  class_acc: 0.4375 (0.5111)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (19.2791)  time: 0.0318  data: 0.0001  max mem: 2404
Epoch: [3]  [150/314]  eta: 0:00:05  lr: 0.000357  min_lr: 0.000357  loss: 1.1151 (0.8845)  class_acc: 0.4375 (0.5075)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (19.2791)  time: 0.0324  data: 0.0001  max mem: 2404
Epoch: [3]  [160/314]  eta: 0:00:05  lr: 0.000357  min_lr: 0.000357  loss: 1.1017 (0.9011)  class_acc: 0.4375 (0.5016)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (19.2791)  time: 0.0326  data: 0.0001  max mem: 2404
Epoch: [3]  [170/314]  eta: 0:00:04  lr: 0.000357  min_lr: 0.000357  loss: 1.1247 (0.9193)  class_acc: 0.3750 (0.4923)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (19.2791)  time: 0.0318  data: 0.0001  max mem: 2404
Epoch: [3]  [180/314]  eta: 0:00:04  lr: 0.000357  min_lr: 0.000357  loss: 1.1440 (0.9303)  class_acc: 0.3750 (0.4886)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (19.2791)  time: 0.0321  data: 0.0002  max mem: 2404
Epoch: [3]  [190/314]  eta: 0:00:04  lr: 0.000357  min_lr: 0.000357  loss: 1.1141 (0.9445)  class_acc: 0.3750 (0.4833)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (19.2791)  time: 0.0326  data: 0.0002  max mem: 2404
Epoch: [3]  [200/314]  eta: 0:00:03  lr: 0.000393  min_lr: 0.000393  loss: 1.1978 (0.9555)  class_acc: 0.3750 (0.4792)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (68.6724)  time: 0.0320  data: 0.0002  max mem: 2404
Epoch: [3]  [210/314]  eta: 0:00:03  lr: 0.000393  min_lr: 0.000393  loss: 0.7779 (0.9442)  class_acc: 0.4375 (0.4834)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (68.6724)  time: 0.0321  data: 0.0002  max mem: 2404
Epoch: [3]  [220/314]  eta: 0:00:03  lr: 0.000393  min_lr: 0.000393  loss: 0.7032 (0.9350)  class_acc: 0.5625 (0.4873)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (68.6724)  time: 0.0314  data: 0.0002  max mem: 2404
Epoch: [3]  [230/314]  eta: 0:00:02  lr: 0.000393  min_lr: 0.000393  loss: 0.7394 (0.9290)  class_acc: 0.5625 (0.4881)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (68.6724)  time: 0.0302  data: 0.0001  max mem: 2404
Epoch: [3]  [240/314]  eta: 0:00:02  lr: 0.000393  min_lr: 0.000393  loss: 0.7308 (0.9212)  class_acc: 0.5625 (0.4909)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (68.6724)  time: 0.0302  data: 0.0002  max mem: 2404
Epoch: [3]  [250/314]  eta: 0:00:02  lr: 0.000393  min_lr: 0.000393  loss: 0.7106 (0.9132)  class_acc: 0.5625 (0.4958)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (68.6724)  time: 0.0302  data: 0.0001  max mem: 2404
Epoch: [3]  [260/314]  eta: 0:00:01  lr: 0.000393  min_lr: 0.000393  loss: 0.7070 (0.9058)  class_acc: 0.5625 (0.4995)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (68.6724)  time: 0.0303  data: 0.0001  max mem: 2404
Epoch: [3]  [270/314]  eta: 0:00:01  lr: 0.000393  min_lr: 0.000393  loss: 0.7054 (0.8983)  class_acc: 0.5625 (0.5030)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (68.6724)  time: 0.0304  data: 0.0001  max mem: 2404
Epoch: [3]  [280/314]  eta: 0:00:01  lr: 0.000393  min_lr: 0.000393  loss: 0.7054 (0.8935)  class_acc: 0.5625 (0.5049)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (68.6724)  time: 0.0304  data: 0.0001  max mem: 2404
Epoch: [3]  [290/314]  eta: 0:00:00  lr: 0.000393  min_lr: 0.000393  loss: 0.7337 (0.8864)  class_acc: 0.5625 (0.5064)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (68.6724)  time: 0.0304  data: 0.0001  max mem: 2404
Epoch: [3]  [300/314]  eta: 0:00:00  lr: 0.000393  min_lr: 0.000393  loss: 0.6680 (0.8798)  class_acc: 0.5625 (0.5092)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (47.6244)  time: 0.0287  data: 0.0001  max mem: 2404
Epoch: [3]  [310/314]  eta: 0:00:00  lr: 0.000393  min_lr: 0.000393  loss: 0.6680 (0.8798)  class_acc: 0.5625 (0.5092)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (47.6244)  time: 0.0136  data: 0.0001  max mem: 2404
Epoch: [3]  [313/314]  eta: 0:00:00  lr: 0.000393  min_lr: 0.000393  loss: 0.6680 (0.8798)  class_acc: 0.5625 (0.5092)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (47.6244)  time: 0.0091  data: 0.0001  max mem: 2404
Epoch: [3] Total time: 0:00:09 (0.0315 s / it)
Averaged stats: lr: 0.000393  min_lr: 0.000393  loss: 0.6680 (0.8798)  class_acc: 0.5625 (0.5092)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.2791 (47.6244)
/home/chntzi001/.conda/envs/EEGPT_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Val:  [ 0/61]  eta: 0:00:15  loss: 0.2147 (0.2147)  accuracy: 0.0000 (0.0000)  balanced_accuracy: 0.0000 (0.0000)  pr_auc: 0.0000 (0.0000)  roc_auc: 0.0000 (0.0000)  time: 0.2462  data: 0.2324  max mem: 2404
Val:  [10/61]  eta: 0:00:01  loss: 0.2409 (0.2911)  accuracy: 0.0000 (0.2273)  balanced_accuracy: 0.0000 (0.1364)  pr_auc: 0.0000 (0.2379)  roc_auc: 0.0000 (0.1316)  time: 0.0368  data: 0.0217  max mem: 2404
Val:  [20/61]  eta: 0:00:01  loss: 0.4578 (0.4855)  accuracy: 0.6250 (0.4365)  balanced_accuracy: 0.5000 (0.3095)  pr_auc: 0.6533 (0.4619)  roc_auc: 0.3037 (0.2961)  time: 0.0152  data: 0.0004  max mem: 2404
Val:  [30/61]  eta: 0:00:00  loss: 0.8879 (0.6556)  accuracy: 0.5000 (0.4368)  balanced_accuracy: 0.5000 (0.3710)  pr_auc: 0.6158 (0.4821)  roc_auc: 0.4965 (0.3696)  time: 0.0145  data: 0.0002  max mem: 2404
Val:  [40/61]  eta: 0:00:00  loss: 0.9977 (0.7383)  accuracy: 0.4583 (0.4451)  balanced_accuracy: 0.5000 (0.4014)  pr_auc: 0.4750 (0.4858)  roc_auc: 0.4929 (0.3871)  time: 0.0146  data: 0.0002  max mem: 2404
Val:  [50/61]  eta: 0:00:00  loss: 1.0410 (0.8021)  accuracy: 0.4583 (0.4428)  balanced_accuracy: 0.5000 (0.4208)  pr_auc: 0.4544 (0.4751)  roc_auc: 0.3811 (0.3877)  time: 0.0143  data: 0.0001  max mem: 2404
Val:  [60/61]  eta: 0:00:00  loss: 1.0597 (0.8254)  accuracy: 0.3750 (0.4074)  balanced_accuracy: 0.5000 (0.4094)  pr_auc: 0.3820 (0.4347)  roc_auc: 0.3636 (0.3678)  time: 0.0136  data: 0.0001  max mem: 2404
Val: Total time: 0:00:01 (0.0203 s / it)
* loss 0.825
Accuracy of the network on the 1463 test EEG: 0.59%
Max accuracy val: 0.59%
slurmstepd: error: *** JOB 168598 ON srvrocgpu012 CANCELLED AT 2025-06-25T12:09:03 ***
